name: Validate markdown and stage

on:
  pull_request_target:
    types: [opened, synchronize, reopened, edited, ready_for_review]
    branches: [master, github-actions]

jobs:
  validate_and_stage:
    # Only run for PRs from branches in this repo (not forks) into master
    runs-on: ubuntu-latest
    environment:
      name: staging
    permissions:
      contents: read
      issues: write
      pull-requests: write
      
    steps:
      - name: Check for relevant changes (early exit)
        id: check_relevant
        env:
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          pr_number=${{ github.event.pull_request.number }}
          repo=${{ github.repository }}
          files_json=$(curl -sS -H "Authorization: Bearer $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "https://api.github.com/repos/$repo/pulls/$pr_number/files?per_page=100")

          # Quick check if any files are under site/sfguides/src/ (excluding _* folders)
          relevant_files=$(echo "$files_json" | jq -r '.[]?.filename | select(startswith("site/sfguides/src/")) | select(test("^site/sfguides/src/_") | not)')
          if [ -z "$relevant_files" ]; then
            echo "has_relevant_changes=false" >> $GITHUB_OUTPUT
            echo ":information_source: No changes in site/sfguides/src/, skipping checkout and validation"
            exit 0
          fi
          echo "has_relevant_changes=true" >> $GITHUB_OUTPUT

      - name: Checkout base repository (trusted)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          ref: ${{ github.event.pull_request.base.sha }}
          sparse-checkout: |
            scripts/
          sparse-checkout-cone-mode: true

      - name: Checkout PR head (untrusted) into ./pr
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          repository: ${{ github.event.pull_request.head.repo.full_name }}
          ref: ${{ github.event.pull_request.head.ref }}
          path: pr
          sparse-checkout: |
            site/sfguides/src/
          sparse-checkout-cone-mode: true

      - name: Setup Node.js
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Cache npm dependencies
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/cache@v4
        id: npm-cache
        with:
          path: ~/.npm
          key: ${{ runner.os }}-npm-badwords-graymatter-v1
          restore-keys: |
            ${{ runner.os }}-npm-

      - name: Install validator dependencies
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        run: |
          # Create package.json if it doesn't exist
          if [ ! -f package.json ]; then
            npm init -y >/dev/null 2>&1
          fi
          # Install both packages in a single command (faster than separate installs)
          npm install --no-audit --no-fund badwords-list@1.0.0 gray-matter@4.0.3
          # Verify installation
          node -e "require('badwords-list'); require('gray-matter'); console.log('Dependencies installed successfully')"

      - name: Determine changed files in quickstarts (from PR)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: detect
        env:
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          # Re-fetch files_json since checkout may have overwritten files_json.txt
          pr_number=${{ github.event.pull_request.number }}
          repo=${{ github.repository }}
          files_json=$(curl -sS -H "Authorization: Bearer $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "https://api.github.com/repos/$repo/pulls/$pr_number/files?per_page=100")
          
          md_files=$(echo "$files_json" | jq -r '.[]?.filename | select(startswith("site/sfguides/src/")) | select(endswith(".md"))' | sort -u)
          if [ -z "$md_files" ]; then files_arr='[]'; else files_arr=$(printf '%s\n' "$md_files" | awk '{print "pr/" $0}' | jq -R -s -c 'split("\n") | map(select(length>0))'); fi

          # Build quickstart names and languages like the legacy workflow
          quickstart_names=$(echo "$files_json" | jq -r '.[]?.filename | select(startswith("site/sfguides/src/")) | capture("^site/sfguides/src/(?<name>[^/]+)/").name' | sort -u)
          if [ -z "$quickstart_names" ]; then
            quickstart_names_json='[]'
            quickstart_folders_json='[]'
            quickstart_name=''
          else
            objs='[]'
            names_arr='[]'
            while IFS= read -r name; do
              [ -z "$name" ] && continue
              lang=""
              md_file=""
              # Prefer a markdown file that matches the folder name (from PR checkout)
              candidate="pr/site/sfguides/src/$name/$name.md"
              if [ -f "$candidate" ]; then
                md_file="$candidate"
              else
                # Otherwise, pick the first .md containing a language: field in the first 50 lines
                for f in $(ls -1 "pr/site/sfguides/src/$name"/*.md 2>/dev/null | sort); do
                  if sed -n '1,50p' "$f" | grep -qi -E '^[[:space:]]*language[[:space:]]*:'; then
                    md_file="$f"; break
                  fi
                done
                # As a fallback, choose the first non-README.md file
                if [ -z "$md_file" ]; then
                  for f in $(ls -1 "pr/site/sfguides/src/$name"/*.md 2>/dev/null | sort); do
                    [ "$(basename "$f")" = "README.md" ] && continue
                    md_file="$f"; break
                  done
                fi
                # Final fallback: any .md file if still empty
                if [ -z "$md_file" ]; then
                  md_file=$(ls -1 "pr/site/sfguides/src/$name"/*.md 2>/dev/null | head -n1 || true)
                fi
              fi
              if [ -n "$md_file" ] && [ -f "$md_file" ]; then
                lang=$(sed -n '1,50p' "$md_file" | grep -m1 -E '^[[:space:]]*language:[[:space:]]*' | sed -E 's/^[[:space:]]*language:[[:space:]]*//')
                lang=$(printf '%s' "$lang" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')
                if [[ "$lang" =~ ^\".*\"$ ]]; then lang=${lang:1:${#lang}-2}; fi
                if [[ "$lang" =~ ^\'.*\'$ ]]; then lang=${lang:1:${#lang}-2}; fi
              fi
              objs=$(echo "$objs" | jq -c --arg name "$name" --arg lang "$lang" '. + [{name:$name, language:$lang}]')
              names_arr=$(echo "$names_arr" | jq -c --arg name "$name" '. + [$name]')
            done <<< "$quickstart_names"

            quickstart_names_json="$objs"
            quickstart_folders_json=$(echo "$names_arr" | jq -c .)
            quickstart_name=$(printf '%s\n' "$quickstart_names" | head -n1)
          fi

          # Get all changed files (not just markdown) for file size checking
          # Only include files that will be checked out (site/sfguides/src/)
          all_changed_files=$(echo "$files_json" | jq -r '.[]?.filename | select(startswith("site/sfguides/src/"))' | sort -u)
          if [ -z "$all_changed_files" ]; then 
            all_changed_files_json='[]'
          else
            all_changed_files_json=$(printf '%s\n' "$all_changed_files" | jq -R -s -c 'split("\n") | map(select(length>0))')
          fi

          # Count markdown files
          if [ -z "$md_files" ]; then
            md_file_count=0
          else
            md_file_count=$(echo "$md_files" | wc -l | tr -d ' ')
          fi

          echo "files_json=$files_arr" >> $GITHUB_OUTPUT
          echo "quickstart_names_json=$quickstart_names_json" >> $GITHUB_OUTPUT
          echo "quickstart_folders_json=$quickstart_folders_json" >> $GITHUB_OUTPUT
          echo "quickstart_name=$quickstart_name" >> $GITHUB_OUTPUT
          echo "all_changed_files_json=$all_changed_files_json" >> $GITHUB_OUTPUT
          echo "md_file_count=$md_file_count" >> $GITHUB_OUTPUT

      - name: Check for large files
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: check_large_files
        continue-on-error: true
        env:
          CHANGED_FILES_JSON: ${{ steps.detect.outputs.all_changed_files_json }}
        run: |
          # Check only files that were changed in the PR
          large_files=()
          changed_files=$(echo "$CHANGED_FILES_JSON" | jq -r '.[]')
          
          if [ -z "$changed_files" ]; then
            echo "No files to check"
            exit 0
          fi
          
          while IFS= read -r file; do
            [ -z "$file" ] && continue
            file_path="pr/$file"
            
            if [ -f "$file_path" ]; then
              file_size=$(stat -f%z "$file_path" 2>/dev/null || stat -c%s "$file_path" 2>/dev/null || echo "0")
              if [ "$file_size" -gt 1000000 ]; then
                large_files+=("$file")
              fi
            fi
          done <<< "$changed_files"
          
          if [ ${#large_files[@]} -gt 0 ]; then
            echo "Large files found:"
            printf '%s\n' "${large_files[@]}"
            # Output large files as JSON array for use in next step
            printf '%s\n' "${large_files[@]}" | jq -R -s -c 'split("\n") | map(select(length>0))' > large_files.json
            echo "large_files_json=$(cat large_files.json)" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "No large files found"
            echo "large_files_json=[]" >> $GITHUB_OUTPUT
            exit 0
          fi

      - name: Validate abusive words in markdown
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.files_json }}
          BLOCKLIST_STRING: ${{ vars.PROFANITY_BLOCKLIST }}
        run: |
          node scripts/validate-profanity.js || {
            echo "Abusive words detected. See 'profanity-report.json' for details." >&2
            exit 1
          }

      - name: Validate categories syntax in markdown
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_categories
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.files_json }}
        run: |
          node scripts/validate-categories.js || {
            echo "Category syntax validation failed. See 'categories-error.json' for details." >&2
            exit 1
          }

      - name: Validate frontmatter id and path
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_frontmatter
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.files_json }}
        run: |
          node scripts/validate-frontmatter.js || {
            echo "Frontmatter validation failed. See 'frontmatter-error.json' for details." >&2
            exit 1
          }

      - name: Validate language tags
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_language
        continue-on-error: true
        env:
          QUICKSTART_NAMES_JSON: ${{ steps.detect.outputs.quickstart_names_json }}
        run: |
          # Validate languages are in allowed set
          objs="$QUICKSTART_NAMES_JSON"
          if [ -z "$objs" ] || [ "$objs" = "[]" ]; then
            echo "No quickstarts to validate"
            exit 0
          fi
          
          # Filter to only include objects with invalid languages
          invalid_objs=$(echo "$objs" | jq '[ .[]
            | select(
              if type=="object" and (.language|type)=="string"
              then (.language | ascii_downcase) as $l
              | (["en","es","it","fr","de","ja","ko","pt_br"] | index($l)) == null
              else true
              end
            )
          ]')
          
          invalid_count=$(echo "$invalid_objs" | jq 'length')
          
          if [ "$invalid_count" -ne 0 ]; then
            echo "Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br" >&2
            echo "$invalid_objs" | jq . >&2
            jq -n --argjson objs "$invalid_objs" --arg msg "Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br" '{type:"language", message:$msg, objects:$objs}' > validation-error.json
            exit 1
          else
            echo "Language validation passed"
            exit 0
          fi

      - name: Comment PR with validation findings (on failure)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && (steps.check_large_files.outcome == 'failure' || steps.validate.outcome == 'failure' || steps.validate_categories.outcome == 'failure' || steps.validate_frontmatter.outcome == 'failure' || steps.validate_language.outcome == 'failure') 
        uses: actions/github-script@v7
        env:
          LARGE_FILES_JSON: ${{ steps.check_large_files.outputs.large_files_json }}
        with:
          script: |
            const fs = require('fs');
            const sections = [];
            
            // Collect all validation failures
            
            // 1. Large files
            const largeFiles = JSON.parse(process.env.LARGE_FILES_JSON || '[]');
            if (largeFiles.length > 0) {
              const fileList = largeFiles.map(f => `- \`${f}\``).join('\n');
              sections.push(`### üì¶ Large Files Detected\n\nFiles larger than 1MB found:\n\n${fileList}\n\nPlease reduce the file size(s) before merging.`);
            }
            
            // 2. Language validation
            if (fs.existsSync('validation-error.json')) {
              try {
                const v = JSON.parse(fs.readFileSync('validation-error.json','utf8'));
                if (v && v.type === 'language') {
                  const items = Array.isArray(v.objects) ? v.objects.map(o => `- ${o.name}: "${o.language || ''}"`).join('\n') : '';
                  sections.push(`### üåê Language Validation Failed\n\n${v.message || 'Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br'}\n\n${items}`);
                }
              } catch {}
            }
            
            // 3. Categories validation
            if (fs.existsSync('categories-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('categories-error.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (issues.length) {
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.invalid || []).join(', ')}`);
                  sections.push(`### üè∑Ô∏è Category Syntax Validation Failed\n\nExpected categories like 'snowflake-site:taxonomy/x/y'.\n\n${lines.join('\n')}`);
                } else {
                  sections.push(`### üè∑Ô∏è Category Syntax Validation Failed\n\nCategory syntax is invalid.`);
                }
              } catch {}
            }
            
            // 4. Frontmatter validation
            if (fs.existsSync('frontmatter-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('frontmatter-error.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (issues.length) {
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.errors || []).join('; ')}`);
                  sections.push(`### üìÑ Frontmatter Validation Failed\n\nFrontmatter id must be slugified and match file and folder.\n\n${lines.join('\n')}`);
                } else {
                  sections.push(`### üìÑ Frontmatter Validation Failed\n\nFrontmatter id check failed.`);
                }
              } catch {}
            }
            
            // 5. Profanity validation
            if (fs.existsSync('profanity-report.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('profanity-report.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (data.error) {
                  // Installation error
                  sections.push(`### üö´ Profanity Check Failed\n\n${data.error}`);
                } else if (issues.length) {
                  // Abusive words found
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.words || []).join(', ')}`);
                  sections.push(`### üö´ Profanity Check Failed\n\nAbusive words detected:\n\n${lines.join('\n')}`);
                } else {
                  // Empty issues array (shouldn't happen with refactored script, but handle gracefully)
                  sections.push(`### üö´ Profanity Check Failed\n\nProfanity check failed but no issues were listed.`);
                }
              } catch {
                sections.push(`### üö´ Profanity Check Failed\n\nProfanity check encountered an error.`);
              }
            }
            
            // Build aggregated comment
            if (sections.length > 0) {
              const body = `## ‚ö†Ô∏è Validation Failed\n\n${sections.join('\n\n---\n\n')}\n\nPlease fix the issues above before merging.`;
              
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
                body
              });
            }

      - name: Upload validation artifacts
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-artifacts
          path: |
            profanity-report.json
            validation-error.json
            categories-error.json
            frontmatter-error.json
          if-no-files-found: ignore

      - name: Fail workflow if validation failed
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && always()
        run: |
          # Check if any validation step failed
          if [ "${{ steps.check_large_files.outcome }}" == "failure" ] || \
             [ "${{ steps.validate.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_categories.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_frontmatter.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_language.outcome }}" == "failure" ]; then
            echo "Validation failed. See PR comment for details."
            exit 1
          fi

      - name: Call Workato staging webhook
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && steps.detect.outputs.md_file_count < 4
        env:
          WORKATO_STAGING_WEBHOOK_URL: ${{ secrets.WORKATO_STAGING_WEBHOOK_URL }}
        run: |
          payload=$(jq -n \
            --arg repo "$GITHUB_REPOSITORY" \
            --arg pr '${{ github.event.pull_request.number }}' \
            --arg sha '${{ github.event.pull_request.head.sha }}'' \
            --arg env "staging" \
            --arg checkrunid '${{ github.run_id }}' \ 
            --arg qname '${{ steps.detect.outputs.quickstart_name }}' \
            --argjson qnames '${{ steps.detect.outputs.quickstart_names_json }}' \
            --argjson qfolders '${{ steps.detect.outputs.quickstart_folders_json }}' \
            '{repo:$repo, pr_number: ($pr|tonumber), commit_sha:$sha, environment:$env, check_run_id:$checkrunid, quickstart_name:$qname, quickstart_names:$qnames, quickstart_folders:$qfolders}')
          echo "$payload" | jq .
          echo "Triggering webhook asynchronously..."
          curl -X POST "$WORKATO_STAGING_WEBHOOK_URL" -H "Content-Type: application/json" --data-raw "$payload" --max-time 5 --connect-timeout 2 > /dev/null 2>&1 || true
          echo "Webhook call sent (response will be handled asynchronously by external tool)"

      - name: Wait for tests to succeed
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && steps.detect.outputs.md_file_count < 4
        uses: poseidon/wait-for-status-checks@v0.6.0
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          timeout: 600s
          delay: 60s