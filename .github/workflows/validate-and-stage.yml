name: Validate markdown and stage

on:
  pull_request_target:
    types: [opened, synchronize, reopened, edited, ready_for_review]
    branches: [master, github-actions]

jobs:
  validate_and_stage:
    # Only run for PRs from branches in this repo (not forks) into master
    runs-on: ubuntu-latest
    environment:
      name: staging
    permissions:
      contents: read
      issues: write
      pull-requests: write
    outputs:
      has_relevant_changes: ${{ steps.check_relevant.outputs.has_relevant_changes }}
      quickstart_names_json: ${{ steps.detect.outputs.quickstart_names_json }}
      all_validations_passed: ${{ steps.validation_check.outputs.all_passed }}
      md_file_count: ${{ steps.detect.outputs.md_file_count }}
      
    steps:
      - name: Check for relevant changes (early exit)
        id: check_relevant
        env:
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          pr_number=${{ github.event.pull_request.number }}
          repo=${{ github.repository }}
          files_json=$(curl -sS -H "Authorization: Bearer $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "https://api.github.com/repos/$repo/pulls/$pr_number/files?per_page=100")

          # Quick check if any files are under site/sfguides/src/ (excluding _* folders)
          relevant_files=$(echo "$files_json" | jq -r '.[]?.filename | select(startswith("site/sfguides/src/")) | select(test("/_") | not)')
          if [ -z "$relevant_files" ]; then
            echo "has_relevant_changes=false" >> $GITHUB_OUTPUT
            echo ":information_source: No changes in site/sfguides/src/, skipping checkout and validation"
            exit 0
          fi
          echo "has_relevant_changes=true" >> $GITHUB_OUTPUT

      - name: Checkout base repository (trusted)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          ref: ${{ github.event.pull_request.base.sha }}
          sparse-checkout: |
            scripts/
          sparse-checkout-cone-mode: true

      - name: Download changed files from PR (untrusted)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: download
        env:
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          pr_number=${{ github.event.pull_request.number }}
          repo=${{ github.repository }}
          
          # Fetch changed files from PR (excluding deleted files)
          files_json=$(curl -sS -H "Authorization: Bearer $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "https://api.github.com/repos/$repo/pulls/$pr_number/files?per_page=100")
          
          # Save files_json to a file for reuse in subsequent steps
          echo "$files_json" > pr_files.json
          
          # Filter for:
          # 1. Markdown files (.md) in site/sfguides/src/ (excluding README.md)
          # 2. Image files in assets folders
          # 3. Exclude folders starting with _
          changed_files=$(echo "$files_json" | jq -r '.[] 
            | select(.status != "removed") 
            | .filename 
            | select(startswith("site/sfguides/src/")) 
            | select(test("/_") | not)
            | select(
                (endswith(".md") and (test("/README\\.md$"; "i") | not)) or
                (test("/assets/.*\\.(jpg|jpeg|png|gif|svg|webp|bmp|ico)$"; "i"))
              )')
          
          if [ -z "$changed_files" ]; then
            echo "No changed files to download"
            exit 0
          fi
          
          # Download each changed file
          head_repo="${{ github.event.pull_request.head.repo.full_name }}"
          head_ref="${{ github.event.pull_request.head.ref }}"
          
          echo "Downloading changed files..."
          while IFS= read -r file; do
            [ -z "$file" ] && continue
            
            # Create directory structure
            mkdir -p "pr/$(dirname "$file")"
            
            # Download raw file content
            url="https://raw.githubusercontent.com/$head_repo/$head_ref/$file"
            echo "Downloading: $file"
            
            if ! curl -sS -f -H "Authorization: Bearer $GITHUB_TOKEN" "$url" -o "pr/$file"; then
              echo "Warning: Failed to download $file" >&2
            fi
          done <<< "$changed_files"
          
          echo "Download complete"

      - name: Setup Node.js
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Cache npm dependencies
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/cache@v4
        id: npm-cache
        with:
          path: ~/.npm
          key: ${{ runner.os }}-npm-badwords-graymatter-v1
          restore-keys: |
            ${{ runner.os }}-npm-

      - name: Install validator dependencies
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        run: |
          # Create package.json if it doesn't exist
          if [ ! -f package.json ]; then
            npm init -y >/dev/null 2>&1
          fi
          # Install both packages in a single command (faster than separate installs)
          npm install --no-audit --no-fund badwords-list@1.0.0 gray-matter@4.0.3
          # Verify installation
          node -e "require('badwords-list'); require('gray-matter'); console.log('Dependencies installed successfully')"

      - name: Determine changed files in quickstarts (from PR)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: detect
        run: |
          # Reuse files_json from previous step (already filtered for relevant, non-removed files)
          files_json=$(cat pr_files.json)
          
          # Extract markdown and all changed files (filtering already done in download step)
          md_files=$(echo "$files_json" | jq -r '.[].filename | select(endswith(".md"))' | sort -u)
          all_changed_files=$(echo "$files_json" | jq -r '.[].filename' | sort -u)
          
          # Prepare file arrays for output
          if [ -z "$md_files" ]; then 
            markdown_json='[]'
            md_file_count=0
          else 
            markdown_json=$(printf '%s\n' "$md_files" | awk '{print "pr/" $0}' | jq -R -s -c 'split("\n") | map(select(length>0))')
            md_file_count=$(echo "$md_files" | wc -l | tr -d ' ')
          fi
          
          if [ -z "$all_changed_files" ]; then 
            all_changed_files_json='[]'
          else
            all_changed_files_json=$(printf '%s\n' "$all_changed_files" | jq -R -s -c 'split("\n") | map(select(length>0))')
          fi

          # Extract quickstart names from markdown files
          quickstart_names=$(echo "$md_files" | sed -E 's|^site/sfguides/src/([^/]+)/.*|\1|' | sort -u)
          
          if [ -z "$quickstart_names" ]; then
            quickstart_names_json='[]'
            quickstart_name=''
          else
            objs='[]'
            
            # For each quickstart, extract language from the markdown file
            # Note: 99%+ of quickstarts have exactly 1 markdown file (README.md excluded)
            while IFS= read -r name; do
              [ -z "$name" ] && continue
              lang=""
              md_file=""
              
              # Prefer a markdown file that matches the folder name (e.g., quickstart-name/quickstart-name.md)
              candidate="pr/site/sfguides/src/$name/$name.md"
              if [ -f "$candidate" ]; then
                md_file="$candidate"
              else
                # Fallback: use the first (and likely only) markdown file in this quickstart
                first_changed=$(echo "$md_files" | grep "^site/sfguides/src/$name/" | head -n1)
                [ -n "$first_changed" ] && md_file="pr/$first_changed"
              fi
              
              # Extract language from the markdown file
              if [ -n "$md_file" ] && [ -f "$md_file" ]; then
                lang=$(sed -n '1,50p' "$md_file" | grep -m1 -E '^[[:space:]]*language:[[:space:]]*' | sed -E 's/^[[:space:]]*language:[[:space:]]*//')
                # Only process if language field was found
                if [ -n "$lang" ]; then
                  lang=$(printf '%s' "$lang" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')
                  if [[ "$lang" =~ ^\".*\"$ ]]; then lang=${lang:1:${#lang}-2}; fi
                  if [[ "$lang" =~ ^\'.*\'$ ]]; then lang=${lang:1:${#lang}-2}; fi
                fi
              fi
              
              objs=$(echo "$objs" | jq -c --arg name "$name" --arg lang "$lang" '. + [{name:$name, language:$lang}]')
            done <<< "$quickstart_names"

            quickstart_names_json="$objs"
            quickstart_name=$(echo "$quickstart_names" | head -n1)
          fi

          echo "markdown_json=$markdown_json" >> $GITHUB_OUTPUT
          echo "quickstart_names_json=$quickstart_names_json" >> $GITHUB_OUTPUT
          echo "quickstart_name=$quickstart_name" >> $GITHUB_OUTPUT
          echo "all_changed_files_json=$all_changed_files_json" >> $GITHUB_OUTPUT
          echo "md_file_count=$md_file_count" >> $GITHUB_OUTPUT

      - name: Check for large files
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: check_large_files
        continue-on-error: true
        env:
          CHANGED_FILES_JSON: ${{ steps.detect.outputs.all_changed_files_json }}
        run: |
          # Check only files that were changed in the PR
          large_files=()
          changed_files=$(echo "$CHANGED_FILES_JSON" | jq -r '.[]')
          
          if [ -z "$changed_files" ]; then
            echo "No files to check"
            exit 0
          fi
          
          while IFS= read -r file; do
            [ -z "$file" ] && continue
            
            # Skip files in folders starting with _ (e.g., _archived)
            if [[ "$file" =~ ^site/sfguides/src/_ ]]; then
              echo "Skipping $file (in excluded directory)"
              continue
            fi
            
            file_path="pr/$file"
            
            if [ -f "$file_path" ]; then
              file_size=$(stat -f%z "$file_path" 2>/dev/null || stat -c%s "$file_path" 2>/dev/null || echo "0")
              if [ "$file_size" -gt 1000000 ]; then
                large_files+=("$file")
              fi
            fi
          done <<< "$changed_files"
          
          if [ ${#large_files[@]} -gt 0 ]; then
            echo "Large files found:"
            printf '%s\n' "${large_files[@]}"
            # Output large files as JSON array for use in next step
            printf '%s\n' "${large_files[@]}" | jq -R -s -c 'split("\n") | map(select(length>0))' > large_files.json
            echo "large_files_json=$(cat large_files.json)" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "No large files found"
            echo "large_files_json=[]" >> $GITHUB_OUTPUT
            exit 0
          fi

      - name: Validate abusive words in markdown
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.markdown_json }}
          BLOCKLIST_STRING: ${{ vars.PROFANITY_BLOCKLIST }}
        run: |
          node scripts/validate-profanity.js || {
            echo "Abusive words detected. See 'profanity-report.json' for details." >&2
            exit 1
          }

      - name: Validate categories syntax in markdown
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_categories
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.markdown_json }}
        run: |
          node scripts/validate-categories.js || {
            echo "Category syntax validation failed. See 'categories-error.json' for details." >&2
            exit 1
          }

      - name: Validate frontmatter id and path
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_frontmatter
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.markdown_json }}
        run: |
          node scripts/validate-frontmatter.js || {
            echo "Frontmatter validation failed. See 'frontmatter-error.json' for details." >&2
            exit 1
          }

      - name: Validate language tags
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_language
        continue-on-error: true
        env:
          QUICKSTART_NAMES_JSON: ${{ steps.detect.outputs.quickstart_names_json }}
        run: |
          # Validate languages are in allowed set
          objs="$QUICKSTART_NAMES_JSON"
          if [ -z "$objs" ] || [ "$objs" = "[]" ]; then
            echo "No quickstarts to validate"
            exit 0
          fi
          
          # Filter to only include objects with invalid languages
          invalid_objs=$(echo "$objs" | jq '[ .[]
            | select(
              if type=="object" and (.language|type)=="string"
              then (.language | ascii_downcase) as $l
              | (["en","es","it","fr","de","ja","ko","pt_br"] | index($l)) == null
              else true
              end
            )
          ]')
          
          invalid_count=$(echo "$invalid_objs" | jq 'length')
          
          if [ "$invalid_count" -ne 0 ]; then
            echo "Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br" >&2
            echo "$invalid_objs" | jq . >&2
            jq -n --argjson objs "$invalid_objs" --arg msg "Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br" '{type:"language", message:$msg, objects:$objs}' > validation-error.json
            exit 1
          else
            echo "Language validation passed"
            exit 0
          fi

      - name: Comment PR with validation findings (on failure)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && (steps.check_large_files.outcome == 'failure' || steps.validate.outcome == 'failure' || steps.validate_categories.outcome == 'failure' || steps.validate_frontmatter.outcome == 'failure' || steps.validate_language.outcome == 'failure') 
        uses: actions/github-script@v7
        env:
          LARGE_FILES_JSON: ${{ steps.check_large_files.outputs.large_files_json }}
        with:
          script: |
            const fs = require('fs');
            const sections = [];
            
            // Collect all validation failures
            
            // 1. Large files
            const largeFiles = JSON.parse(process.env.LARGE_FILES_JSON || '[]');
            if (largeFiles.length > 0) {
              const fileList = largeFiles.map(f => `- \`${f}\``).join('\n');
              sections.push(`### üì¶ Large Files Detected\n\nFiles larger than 1MB found:\n\n${fileList}\n\nPlease reduce the file size(s) before merging.`);
            }
            
            // 2. Language validation
            if (fs.existsSync('validation-error.json')) {
              try {
                const v = JSON.parse(fs.readFileSync('validation-error.json','utf8'));
                if (v && v.type === 'language') {
                  const items = Array.isArray(v.objects) ? v.objects.map(o => `- ${o.name}: "${o.language || ''}"`).join('\n') : '';
                  sections.push(`### üåê Language Validation Failed\n\n${v.message || 'Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br'}\n\n${items}`);
                }
              } catch {}
            }
            
            // 3. Categories validation
            if (fs.existsSync('categories-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('categories-error.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (issues.length) {
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.invalid || []).join(', ')}`);
                  sections.push(`### üè∑Ô∏è Category Syntax Validation Failed\n\nExpected categories like 'snowflake-site:taxonomy/x/y'.\n\n${lines.join('\n')}`);
                } else {
                  sections.push(`### üè∑Ô∏è Category Syntax Validation Failed\n\nCategory syntax is invalid.`);
                }
              } catch {}
            }
            
            // 4. Frontmatter validation
            if (fs.existsSync('frontmatter-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('frontmatter-error.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (issues.length) {
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.errors || []).join('; ')}`);
                  sections.push(`### üìÑ Frontmatter Validation Failed\n\nFrontmatter id must be slugified and match file and folder.\n\n${lines.join('\n')}`);
                } else {
                  sections.push(`### üìÑ Frontmatter Validation Failed\n\nFrontmatter id check failed.`);
                }
              } catch {}
            }
            
            // 5. Profanity validation
            if (fs.existsSync('profanity-report.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('profanity-report.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (data.error) {
                  // Installation error
                  sections.push(`### üö´ Profanity Check Failed\n\n${data.error}`);
                } else if (issues.length) {
                  // Abusive words found
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.words || []).join(', ')}`);
                  sections.push(`### üö´ Profanity Check Failed\n\nAbusive words detected:\n\n${lines.join('\n')}`);
                } else {
                  // Empty issues array (shouldn't happen with refactored script, but handle gracefully)
                  sections.push(`### üö´ Profanity Check Failed\n\nProfanity check failed but no issues were listed.`);
                }
              } catch {
                sections.push(`### üö´ Profanity Check Failed\n\nProfanity check encountered an error.`);
              }
            }
            
            // Build aggregated comment
            if (sections.length > 0) {
              const body = `## ‚ö†Ô∏è Validation Failed\n\n${sections.join('\n\n---\n\n')}\n\nPlease fix the issues above before merging.`;
              
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
                body
              });
            }

      - name: Upload validation artifacts
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-artifacts
          path: |
            profanity-report.json
            validation-error.json
            categories-error.json
            frontmatter-error.json
          if-no-files-found: ignore

      - name: Check validation results and fail if needed
        id: validation_check
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && always()
        run: |
          # Check if any validation step failed
          all_passed=true
          if [ "${{ steps.check_large_files.outcome }}" == "failure" ] || \
             [ "${{ steps.validate.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_categories.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_frontmatter.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_language.outcome }}" == "failure" ]; then
            all_passed=false
          fi
          
          # Set output for matrix job to use
          echo "all_passed=$all_passed" >> $GITHUB_OUTPUT
          echo "All validations passed: $all_passed"
          
          # Fail workflow if validations failed
          if [ "$all_passed" == "false" ]; then
            echo "‚ùå Validation failed. See PR comment for details."
            exit 1
          fi

  call_staging_webhooks:
    needs: validate_and_stage
    # Only call webhooks if validations passed, there are relevant changes, and less than 7 markdown files modified
    if: |
      needs.validate_and_stage.outputs.has_relevant_changes == 'true' && 
      needs.validate_and_stage.outputs.all_validations_passed == 'true' &&
      needs.validate_and_stage.outputs.md_file_count < 7
    runs-on: ubuntu-latest
    environment:
      name: staging
    permissions:
      contents: read
      pull-requests: write
    strategy:
      matrix:
        quickstart: ${{ fromJson(needs.validate_and_stage.outputs.quickstart_names_json) }}
      max-parallel: 1  # Process 1 at a time for rate limit purposes
      fail-fast: false  # Continue processing others even if one fails
    
    steps:
      - name: Call Workato staging webhook for ${{ matrix.quickstart.name }}
        id: call_webhook_matrix
        env:
          WORKATO_STAGING_WEBHOOK_URL: ${{ secrets.WORKATO_STAGING_SINGLE_FILE_URL }}
        run: |
          if [ -z "$WORKATO_STAGING_WEBHOOK_URL" ]; then
            echo "‚ùå WORKATO_STAGING_WEBHOOK_URL is not set"
            exit 1
          fi
          
          qname="${{ matrix.quickstart.name }}"
          qlang="${{ matrix.quickstart.language }}"
          
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "üì¶ Processing quickstart: $qname"
          echo "üåê Language: $qlang"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          
          # Create single-item array for backward compatibility with Workato
          qnames_array=$(jq -n --arg name "$qname" --arg lang "$qlang" '[{name:$name, language:$lang}]')
          
          payload=$(jq -n \
            --arg repo "$GITHUB_REPOSITORY" \
            --arg pr '${{ github.event.pull_request.number }}' \
            --arg sha '${{ github.event.pull_request.head.sha }}' \
            --arg env "staging" \
            --arg qname "$qname" \
            --argjson qnames "$qnames_array" \
            '{repo:$repo, pr_number: ($pr|tonumber), commit_sha:$sha, environment:$env, quickstart_name:$qname, quickstart_names:$qnames}')
          
          echo "Payload:"
          echo "$payload" | jq .
          
          # Retry configuration
          max_retries=3
          retry_count=0
          base_delay=10
          
          while [ $retry_count -le $max_retries ]; do
            if [ $retry_count -gt 0 ]; then
              echo "üîÑ Retry attempt $retry_count of $max_retries..."
            else
              echo "üì° Triggering webhook..."
            fi
            
            response=$(curl -s -w "\n%{http_code}" -X POST "$WORKATO_STAGING_WEBHOOK_URL" \
              -H "Content-Type: application/json" \
              --data-raw "$payload" \
              --max-time 10 \
              --connect-timeout 5) || {
              exit_code=$?
              echo "‚ùå Connection failed (curl exit code: $exit_code)"
              
              if [ $retry_count -lt $max_retries ]; then
                retry_count=$((retry_count + 1))
                delay=$((base_delay * retry_count))
                echo "‚è≥ Waiting ${delay}s before retry..."
                sleep $delay
                continue
              else
                echo "‚ùå Max retries reached"
                exit 1
              fi
            }
            
            http_code=$(echo "$response" | tail -n1)
            response_body=$(echo "$response" | sed '$d')
            
            echo "HTTP Status: $http_code"
            echo "Response:"
            echo "$response_body" | jq . 2>/dev/null || echo "$response_body"
            
            if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
              echo "‚úÖ Webhook succeeded for: $qname"
              # Wait 2 minutes after processing to avoid rate limits for next item
              echo "‚è≥ Waiting 2 minutes before next webhook..."
              sleep 120
              exit 0
            fi
            
            # Rate limit handling
            if echo "$response_body" | grep -qi "rate limit"; then
              if [ $retry_count -lt $max_retries ]; then
                retry_count=$((retry_count + 1))
                delay=$((base_delay * retry_count * 2))  # Longer delay for rate limits
                echo "‚ö†Ô∏è Rate limit detected. Waiting ${delay}s..."
                sleep $delay
                continue
              fi
            elif [ $retry_count -lt $max_retries ]; then
              retry_count=$((retry_count + 1))
              delay=$((base_delay * retry_count))
              echo "‚ö†Ô∏è HTTP $http_code. Waiting ${delay}s before retry..."
              sleep $delay
              continue
            fi
            
            echo "‚ùå Failed after $max_retries retries"
            exit 1
          done


      - name: Report webhook failure to PR
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const qname = '${{ matrix.quickstart.name }}';
            const body = `## ‚ö†Ô∏è Staging Webhook Failed for \`${qname}\`
            
            The staging webhook call failed for quickstart **${qname}** after multiple retry attempts.
            
            ### Next Steps
            - Check the [workflow logs](${context.payload.repository.html_url}/actions/runs/${context.runId}) for details
            - If due to rate limiting, wait a few minutes and re-run the failed job
            - Contact the devrel team if the issue persists
            `;
            
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              body
            });
