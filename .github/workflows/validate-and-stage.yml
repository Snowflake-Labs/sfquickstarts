name: Validate markdown and stage

on:
  pull_request_target:
    types: [opened, synchronize, reopened, edited, ready_for_review]
    branches: [master, github-actions]

jobs:
  validate_and_stage:
    # Only run for PRs from branches in this repo (not forks) into master
    runs-on: ubuntu-latest
    environment:
      name: staging
    permissions:
      contents: read
      issues: write
      pull-requests: write
    outputs:
      has_relevant_changes: ${{ steps.check_relevant.outputs.has_relevant_changes }}
      quickstart_names_json: ${{ steps.prepare_webhooks.outputs.quickstart_names_json }}
      all_validations_passed: ${{ steps.validation_check.outputs.all_passed }}
      md_file_count: ${{ steps.detect.outputs.md_file_count }}
      
    steps:
      - name: Check for relevant changes (early exit)
        id: check_relevant
        env:
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          pr_number=${{ github.event.pull_request.number }}
          repo=${{ github.repository }}
          files_json=$(curl -sS -H "Authorization: Bearer $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "https://api.github.com/repos/$repo/pulls/$pr_number/files?per_page=100")

          # Quick check if any files are under site/sfguides/src/ (excluding _* folders)
          relevant_files=$(echo "$files_json" | jq -r '.[]?.filename | select(startswith("site/sfguides/src/")) | select(test("/_") | not)')
          if [ -z "$relevant_files" ]; then
            echo "has_relevant_changes=false" >> $GITHUB_OUTPUT
            echo ":information_source: No changes in site/sfguides/src/, skipping checkout and validation"
            exit 0
          fi
          echo "has_relevant_changes=true" >> $GITHUB_OUTPUT

      - name: Checkout base repository (trusted)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          ref: ${{ github.event.pull_request.base.sha }}
          sparse-checkout: |
            scripts/
          sparse-checkout-cone-mode: true

      - name: Download changed files from PR (untrusted)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: download
        env:
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          pr_number=${{ github.event.pull_request.number }}
          repo=${{ github.repository }}
          
          # Fetch changed files from PR (excluding deleted files)
          files_json=$(curl -sS -H "Authorization: Bearer $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "https://api.github.com/repos/$repo/pulls/$pr_number/files?per_page=100")
          
          # Save files_json to a file for reuse in subsequent steps
          echo "$files_json" > pr_files.json
          
          # Filter for changed files (excluding folders starting with _)
          changed_files=$(echo "$files_json" | jq -r '.[] 
            | select(.status != "removed") 
            | .filename 
            | select(startswith("site/sfguides/src/")) 
            | select(test("/_") | not)')
          
          # Early exit if no files to download
          if [ -z "$changed_files" ]; then
            echo "No relevant files to download for validation"
            echo "" > downloaded_files.txt
            exit 0
          fi
          
          head_repo="${{ github.event.pull_request.head.repo.full_name }}"
          head_ref="${{ github.event.pull_request.head.ref }}"
          
          # Download all changed files
          echo "Downloading changed files..."
          while IFS= read -r file; do
            [ -z "$file" ] && continue
            
            # Create directory structure
            mkdir -p "pr/$(dirname "$file")"
            
            # Download raw file content
            url="https://raw.githubusercontent.com/$head_repo/$head_ref/$file"
            echo "Downloading: $file"
            
            if ! curl -sS -f -H "Authorization: Bearer $GITHUB_TOKEN" "$url" -o "pr/$file"; then
              echo "Warning: Failed to download $file" >&2
            fi
          done <<< "$changed_files"
          
          # Save changed files to separate lists
          echo "$changed_files" > changed_files.txt
          echo "$changed_files" > downloaded_files.txt
          
          # Extract folders that have changes
          modified_folders=$(echo "$changed_files" | sed -E 's|^site/sfguides/src/([^/]+)/.*|\1|' | sort -u)
          
          # Download ALL markdown files from modified folders (even if not changed)
          # This allows us to validate that each folder has only one markdown file
          if [ -n "$modified_folders" ]; then
            echo "Downloading all markdown files from modified folders..."
            
            while IFS= read -r folder; do
              [ -z "$folder" ] && continue
              
              # Use GitHub API to list all files in the folder
              folder_path="site/sfguides/src/$folder"
              tree_url="https://api.github.com/repos/$head_repo/contents/$folder_path?ref=$head_ref"
              
              # Get all .md files in the folder (excluding README.md)
              folder_files=$(curl -sS -H "Authorization: Bearer $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "$tree_url" | jq -r '.[] | select(.type=="file") | select(.name | endswith(".md")) | select(.name | test("^README\\.md$"; "i") | not) | .name')
              
              while IFS= read -r filename; do
                [ -z "$filename" ] && continue
                
                file_path="$folder_path/$filename"
                
                # Skip if already downloaded
                if [ -f "pr/$file_path" ]; then
                  continue
                fi
                
                # Create directory structure
                mkdir -p "pr/$folder_path"
                
                # Download the file
                url="https://raw.githubusercontent.com/$head_repo/$head_ref/$file_path"
                echo "Downloading additional markdown: $file_path"
                
                if curl -sS -f -H "Authorization: Bearer $GITHUB_TOKEN" "$url" -o "pr/$file_path"; then
                  # Add to downloaded files list
                  echo "$file_path" >> downloaded_files.txt
                else
                  echo "Warning: Failed to download $file_path" >&2
                fi
              done <<< "$folder_files"
            done <<< "$modified_folders"
          fi
          
          echo "Download complete"

      - name: Check for renamed folders (blocking)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: check_renamed_folders
        continue-on-error: true
        run: |
          # Check if any top-level folders inside site/sfguides/src (not starting with _) have been renamed
          # Only checks the immediate child directory of /src/, ignoring nested subdirectories
          files_json=$(cat pr_files.json)
          
          renamed_folders='[]'
          seen_renames=()
          all_folder_renames='[]'
          
          # Method 1: Check for explicit file renames (status == "renamed")
          renamed_files=$(echo "$files_json" | jq -r '.[] | select(.status == "renamed") | "\(.previous_filename)|\(.filename)"')
          
          if [ -n "$renamed_files" ]; then
            echo "Checking for renamed files..."
            
            while IFS='|' read -r old_path new_path; do
              [ -z "$old_path" ] || [ -z "$new_path" ] && continue
              
              # Skip if paths are identical (no actual rename)
              [ "$old_path" = "$new_path" ] && continue
              
              # Extract only the first-level directory after site/sfguides/src/
              # Pattern: site/sfguides/src/{folder}/...
              # Must extract old_folder BEFORE checking new_path to avoid BASH_REMATCH being overwritten
              if [[ "$old_path" =~ ^site/sfguides/src/([^/]+)/ ]]; then
                old_folder="${BASH_REMATCH[1]}"
              else
                continue
              fi
              
              if [[ "$new_path" =~ ^site/sfguides/src/([^/]+)/ ]]; then
                new_folder="${BASH_REMATCH[1]}"
              else
                continue
              fi
              
              # Check if the top-level folder name changed
              if [ "$old_folder" != "$new_folder" ]; then
                # Track this rename (we'll check for round-trips later)
                rename_key="${old_folder}â†’${new_folder}"
                if [[ ! " ${seen_renames[@]} " =~ " ${rename_key} " ]]; then
                  all_folder_renames=$(echo "$all_folder_renames" | jq -c --arg old "$old_folder" --arg new "$new_folder" '. + [{from:$old, to:$new}]')
                  seen_renames+=("$rename_key")
                fi
              fi
            done <<< "$renamed_files"
          fi
          
          # Method 2: Check for directory renames via removed + added pattern
          # Git doesn't always detect directory renames, so files may show as removed + added
          echo "Checking for removed/added file patterns..."
          
          # Get all removed files from site/sfguides/src/ (excluding _ folders)
          removed_files=$(echo "$files_json" | jq -r '.[] | select(.status == "removed") | .filename | select(startswith("site/sfguides/src/")) | select(test("^site/sfguides/src/_") | not)')
          
          # Get all added files from site/sfguides/src/ (excluding _ folders)
          added_files=$(echo "$files_json" | jq -r '.[] | select(.status == "added") | .filename | select(startswith("site/sfguides/src/")) | select(test("^site/sfguides/src/_") | not)')
          
          if [ -n "$removed_files" ] && [ -n "$added_files" ]; then
            # Extract unique folder names from removed files
            removed_folders=$(echo "$removed_files" | sed -E 's|^site/sfguides/src/([^/]+)/.*|\1|' | sort -u)
            
            # Extract unique folder names from added files
            added_folders=$(echo "$added_files" | sed -E 's|^site/sfguides/src/([^/]+)/.*|\1|' | sort -u)
            
            # Check if any removed folder has all its files removed AND similar files added to a new folder
            while IFS= read -r removed_folder; do
              [ -z "$removed_folder" ] && continue
              
              # Get all files removed from this folder
              removed_folder_files=$(echo "$removed_files" | grep "^site/sfguides/src/$removed_folder/" | sed "s|^site/sfguides/src/$removed_folder/||" || true)
              [ -z "$removed_folder_files" ] && continue
              
              # For each added folder, check if it has matching file structure
              while IFS= read -r added_folder; do
                [ -z "$added_folder" ] && continue
                
                # Skip if we've already detected this rename
                rename_key="${removed_folder}â†’${added_folder}"
                [[ " ${seen_renames[@]} " =~ " ${rename_key} " ]] && continue
                
                # Get all files added to this folder
                added_folder_files=$(echo "$added_files" | grep "^site/sfguides/src/$added_folder/" | sed "s|^site/sfguides/src/$added_folder/||" || true)
                [ -z "$added_folder_files" ] && continue
                
                # Count matching relative paths
                matching_files=0
                total_removed=0
                while IFS= read -r rel_path; do
                  [ -z "$rel_path" ] && continue
                  total_removed=$((total_removed + 1))
                  if echo "$added_folder_files" | grep -Fxq "$rel_path"; then
                    matching_files=$((matching_files + 1))
                  fi
                done <<< "$removed_folder_files"
                
                # If at least 50% of removed files match added files, consider it a folder rename
                if [ "$total_removed" -gt 0 ] && [ "$matching_files" -gt 0 ]; then
                  match_percentage=$((matching_files * 100 / total_removed))
                  
                  if [ "$match_percentage" -ge 50 ]; then
                    # Track this rename (we'll check for round-trips later)
                    all_folder_renames=$(echo "$all_folder_renames" | jq -c --arg old "$removed_folder" --arg new "$added_folder" '. + [{from:$old, to:$new}]')
                    seen_renames+=("$rename_key")
                  fi
                fi
              done <<< "$added_folders"
            done <<< "$removed_folders"
          fi
          
          # Analyze all detected renames and filter out round-trips
          echo "Analyzing folder renames..."
          
          # Deduplicate renames (same rename may appear multiple times for different files)
          all_renames_json=$(echo "$all_folder_renames" | jq 'unique_by({from: .from, to: .to})')
          
          rename_count=$(echo "$all_renames_json" | jq 'length')
          if [ "$rename_count" -eq 0 ]; then
            echo "âœ… No folder renames detected"
          else
            echo "Detected $rename_count unique folder rename(s):"
            echo "$all_renames_json" | jq -r '.[] | "  \(.from) â†’ \(.to)"'
          fi
          
          while IFS= read -r rename; do
            [ -z "$rename" ] && continue
            
            from=$(echo "$rename" | jq -r '.from')
            to=$(echo "$rename" | jq -r '.to')
            
            # Check if there's a reverse rename (to â†’ from)
            reverse_exists=$(echo "$all_renames_json" | jq --arg from "$to" --arg to "$from" '[.[] | select(.from == $from and .to == $to)] | length')
            
            if [ "$reverse_exists" -gt 0 ]; then
              echo "â„¹ï¸  Round-trip rename detected (allowed): $from â†” $to"
              # This is a round-trip, skip it
              continue
            fi
            
            # Check if original folder started with underscore (allowed)
            if [[ "$from" =~ ^_ ]]; then
              echo "â„¹ï¸  Underscore folder rename (allowed): $from â†’ $to"
              continue
            fi
            
            # This is a real rename that should be blocked
            echo "âŒ Top-level folder rename detected: $from â†’ $to"
            example_file="site/sfguides/src/$from/... â†’ site/sfguides/src/$to/..."
            renamed_folders=$(echo "$renamed_folders" | jq -c --arg old "$from" --arg new "$to" --arg file "$example_file" '. + [{old_folder:$old, new_folder:$new, example_file:$file}]')
            
          done < <(echo "$all_renames_json" | jq -c '.[]')
          
          # Check if any folder renames found
          rename_count=$(echo "$renamed_folders" | jq 'length')
          if [ "$rename_count" -ne 0 ]; then
            echo "" >&2
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" >&2
            echo "âŒ ERROR: Guide rename(s) detected" >&2
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" >&2
            echo "$renamed_folders" | jq -r '.[] | "  - \(.old_folder) â†’ \(.new_folder)"' >&2
            echo "" >&2
            echo "Renaming guide URLs is not supported." >&2
            echo "Please revert the name change." >&2
            echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”" >&2
            
            # Save error details for PR comment
            jq -n --argjson renames "$renamed_folders" '{type:"folder_rename", message:"Renaming guide URLs is not currently supported. Please revert the name change.", renames:$renames}' > folder-rename-error.json
            exit 1
          else
            echo "âœ… No top-level guide folder renames detected"
            exit 0
          fi

      - name: Setup Node.js
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Cache npm dependencies
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/cache@v4
        id: npm-cache
        with:
          path: ~/.npm
          key: ${{ runner.os }}-npm-badwords-graymatter-v1
          restore-keys: |
            ${{ runner.os }}-npm-

      - name: Install validator dependencies
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        run: |
          # Create package.json if it doesn't exist
          if [ ! -f package.json ]; then
            npm init -y >/dev/null 2>&1
          fi
          # Install both packages in a single command (faster than separate installs)
          npm install --no-audit --no-fund badwords-list@1.0.0 gray-matter@4.0.3
          # Verify installation
          node -e "require('badwords-list'); require('gray-matter'); console.log('Dependencies installed successfully')"

      - name: Determine changed files in quickstarts (from PR)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: detect
        run: |
          # Read the list of changed files (modified in PR)
          changed_files=$(cat changed_files.txt)
          
          # Read the list of all downloaded files (changed + additional markdown files)
          downloaded_files=$(cat downloaded_files.txt)
          
          # Filter out non-image files from assets folders for changed files
          # Keep: markdown files outside assets, image files from assets
          # Exclude: non-image files from assets (like .md, .txt, etc.)
          all_changed_files=$(echo "$changed_files" | grep -v '/assets/' || true)
          assets_images=$(echo "$changed_files" | grep '/assets/' | grep -E '\.(jpg|jpeg|png|gif|svg|webp|bmp|ico)$' || true)
          if [ -n "$assets_images" ]; then
            all_changed_files=$(printf '%s\n%s\n' "$all_changed_files" "$assets_images")
          fi
          
          # Extract changed markdown files (for validation)
          changed_md_files=$(echo "$all_changed_files" | grep '\.md$' || true)
          
          # Extract all downloaded markdown files (for multiple markdown check)
          all_md_files=$(echo "$downloaded_files" | grep '\.md$' | grep -v '/assets/' || true)
          
          if [ -z "$all_changed_files" ]; then 
            all_changed_files_json='[]'
          else
            all_changed_files_json=$(printf '%s\n' "$all_changed_files" | jq -R -s -c 'split("\n") | map(select(length>0))')
          fi

          # Prepare markdown files for validation (only changed)
          if [ -z "$changed_md_files" ]; then
            changed_markdown_json='[]'
            md_file_count=0
          else
            # Build changed_markdown_json with only changed markdown files (for validations)
            changed_markdown_json=$(printf '%s\n' "$changed_md_files" | awk '{print "pr/" $0}' | jq -R -s -c 'split("\n") | map(select(length>0))')
            md_file_count=$(echo "$changed_md_files" | wc -l | tr -d ' ')
          fi

          # Print debug information
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "All changed files:"
          echo "$all_changed_files"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Changed markdown files (for validation):"
          echo "$changed_markdown_json" | jq -r '.[]' 2>/dev/null || echo "$changed_markdown_json"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          
          echo "changed_markdown_json=$changed_markdown_json" >> $GITHUB_OUTPUT
          echo "all_changed_files_json=$all_changed_files_json" >> $GITHUB_OUTPUT
          echo "md_file_count=$md_file_count" >> $GITHUB_OUTPUT

      - name: Validate single markdown file per folder
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_single_markdown
        continue-on-error: true
        run: |
          # Define error message once
          ERROR_MESSAGE="Each quickstart folder should contain only one markdown file in the root. Did you mean to add this additional .md file to the /assets folder?"
          
          # Extract all downloaded markdown files (including unchanged ones for thorough validation)
          downloaded_files=$(cat downloaded_files.txt)
          md_files=$(echo "$downloaded_files" | grep '\.md$' | grep -v '/assets/' || true)
          
          if [ -z "$md_files" ]; then
            echo "No markdown files to validate"
            exit 0
          fi
          
          quickstart_folders=$(echo "$md_files" | sed -E 's|^site/sfguides/src/([^/]+)/.*|\1|' | sort -u)
          multiple_files_errors='[]'
          
          # For each folder with markdown changes, check if multiple markdown files exist
          while IFS= read -r folder; do
            [ -z "$folder" ] && continue
            
            # Get ALL markdown files in this quickstart folder
            folder_md_files=$(find "pr/site/sfguides/src/$folder" -maxdepth 1 -type f -name "*.md" 2>/dev/null | sed 's|^pr/||' || echo "")
            file_count=$(echo "$folder_md_files" | grep -c '^' || echo 0)
            
            # Check if more than one markdown file exists
            if [ "$file_count" -gt 1 ]; then
              # Convert files to JSON array
              files_array=$(echo "$folder_md_files" | jq -R -s -c 'split("\n") | map(select(length>0))')
              multiple_files_errors=$(echo "$multiple_files_errors" | jq -c --arg folder "$folder" --argjson files "$files_array" '. + [{folder:$folder, files:$files}]')
            fi
          done <<< "$quickstart_folders"
          
          # Check if any folders have multiple markdown files
          error_count=$(echo "$multiple_files_errors" | jq 'length')
          if [ "$error_count" -ne 0 ]; then
            echo "âŒ Error: Multiple markdown files found in the following folders:" >&2
            echo "$multiple_files_errors" | jq -r '.[] | "  - \(.folder): \(.files | length) files - \(.files | join(", "))"' >&2
            jq -n --argjson errors "$multiple_files_errors" --arg msg "$ERROR_MESSAGE" '{type:"multiple_markdown_files", message:$msg, errors:$errors}' > multiple-md-error.json
            exit 1
          else
            echo "âœ… Single markdown file validation passed"
            exit 0
          fi

      - name: Check for non-markdown files outside assets (informational)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: check_file_types
        run: |
          # Define message once
          INFO_MESSAGE="Only markdown files (.md) are typically placed in the root of quickstart folders. Other file types should be placed in the /assets folder for better organization."
          
          # Read downloaded files
          downloaded_files=$(cat downloaded_files.txt)
          
          if [ -z "$downloaded_files" ]; then
            echo "No files to check"
            exit 0
          fi
          
          # Find non-markdown files outside of assets folder (in root of quickstart folders)
          # Pattern: site/sfguides/src/{folder}/{filename} where filename is not .md and path doesn't contain /assets/
          non_md_files=$(echo "$downloaded_files" | grep '^site/sfguides/src/[^/]\+/[^/]\+$' | grep -v '\.md$' || true)
          
          if [ -n "$non_md_files" ]; then
            echo "ğŸ’¡ Info: Non-markdown files found outside assets folder:"
            echo "$non_md_files"
            
            # Build info JSON
            files_array=$(echo "$non_md_files" | jq -R -s -c 'split("\n") | map(select(length>0))')
            jq -n --argjson files "$files_array" --arg msg "$INFO_MESSAGE" '{type:"non_markdown_root", message:$msg, files:$files}' > non-markdown-root-info.json
            echo "Info file created for PR comment"
          else
            echo "âœ… All files in root are markdown files"
          fi
          
          # Always exit 0 - this is informational, not an error
          exit 0

      - name: Check for non-image files in assets folder (informational)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: check_assets_files
        run: |
          # Define warning message once
          WARNING_MESSAGE="Non-image files found in /assets folders will **NOT** be uploaded to snowflake.com. If you are referencing them in your guide, you should link to them directly (e.g. [using permanent GitHub links](https://docs.github.com/en/repositories/working-with-files/using-files/getting-permanent-links-to-files))."
          
          # Read downloaded files
          downloaded_files=$(cat downloaded_files.txt)
          
          if [ -z "$downloaded_files" ]; then
            echo "No files to check"
            exit 0
          fi
          
          # Find non-image files in assets folders
          non_image_assets=$(echo "$downloaded_files" | grep '/assets/' | grep -v -E '\.(jpg|jpeg|png|gif|svg|webp|bmp|ico)$' || true)
          
          if [ -n "$non_image_assets" ]; then
            echo "ğŸ’¡ Info: Non-image files found in assets folder:"
            echo "$non_image_assets"
            
            # Build warning JSON
            files_array=$(echo "$non_image_assets" | jq -R -s -c 'split("\n") | map(select(length>0))')
            jq -n --argjson files "$files_array" --arg msg "$WARNING_MESSAGE" '{type:"non_image_assets", message:$msg, files:$files}' > non-image-assets-warning.json
            echo "Warning file created for PR comment"
          else
            echo "âœ… No non-image files in assets folders"
          fi
          
          # Always exit 0 - this is a warning, not an error
          exit 0

      - name: Check for large files in assets (blocking)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: check_large_files
        continue-on-error: true
        env:
          CHANGED_FILES_JSON: ${{ steps.detect.outputs.all_changed_files_json }}
        run: |
          # Check only image files in assets folders
          large_files=()
          changed_files=$(echo "$CHANGED_FILES_JSON" | jq -r '.[]')
          
          if [ -z "$changed_files" ]; then
            echo "No files to check"
            exit 0
          fi
          
          # Filter for only image files in /assets/ folders
          assets_images=$(echo "$changed_files" | grep '/assets/' | grep -E '\.(jpg|jpeg|png|gif|svg|webp|bmp|ico)$' || true)
          
          if [ -z "$assets_images" ]; then
            echo "No image files in assets to check"
            echo "large_files_json=[]" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          while IFS= read -r file; do
            [ -z "$file" ] && continue
            
            file_path="pr/$file"
            
            if [ -f "$file_path" ]; then
              file_size=$(stat -f%z "$file_path" 2>/dev/null || stat -c%s "$file_path" 2>/dev/null || echo "0")
              if [ "$file_size" -gt 1000000 ]; then
                large_files+=("$file")
              fi
            fi
          done <<< "$assets_images"
          
          if [ ${#large_files[@]} -gt 0 ]; then
            echo "Large files found:"
            printf '%s\n' "${large_files[@]}"
            # Output large files as JSON array for use in next step
            printf '%s\n' "${large_files[@]}" | jq -R -s -c 'split("\n") | map(select(length>0))' > large_files.json
            echo "large_files_json=$(cat large_files.json)" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "No large files found"
            echo "large_files_json=[]" >> $GITHUB_OUTPUT
            exit 0
          fi

      - name: Validate abusive words in markdown (blocking)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.changed_markdown_json }}
          BLOCKLIST_STRING: ${{ vars.PROFANITY_BLOCKLIST }}
        run: |
          node scripts/validate-profanity.js || {
            # Strip 'pr/' prefix from file paths in error output for cleaner display
            if [ -f profanity-report.json ]; then
              jq '.issues |= map(.file |= sub("^pr/"; ""))' profanity-report.json > profanity-report.tmp.json
              mv profanity-report.tmp.json profanity-report.json
            fi
            echo "Abusive words detected. See 'profanity-report.json' for details." >&2
            exit 1
          }

      - name: Validate categories syntax in markdown (blocking)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_categories
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.changed_markdown_json }}
        run: |
          node scripts/validate-categories.js || {
            # Strip 'pr/' prefix from file paths in error output for cleaner display
            if [ -f categories-error.json ]; then
              jq '.issues |= map(.file |= sub("^pr/"; ""))' categories-error.json > categories-error.tmp.json
              mv categories-error.tmp.json categories-error.json
            fi
            echo "Category syntax validation failed. See 'categories-error.json' for details." >&2
            exit 1
          }

      - name: Validate frontmatter id and path (blocking)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_frontmatter
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.changed_markdown_json }}
        run: |
          node scripts/validate-frontmatter.js || {
            # Strip 'pr/' prefix from file paths in error output for cleaner display
            if [ -f frontmatter-error.json ]; then
              jq '.issues |= map(.file |= sub("^pr/"; ""))' frontmatter-error.json > frontmatter-error.tmp.json
              mv frontmatter-error.tmp.json frontmatter-error.json
            fi
            echo "Frontmatter validation failed. See 'frontmatter-error.json' for details." >&2
            exit 1
          }

      - name: Validate language tags (blocking)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_language
        continue-on-error: true
        env:
          MARKDOWN_JSON: ${{ steps.detect.outputs.changed_markdown_json }}
        run: |
          # Validate languages in individual markdown files
          markdown_files="$MARKDOWN_JSON"
          if [ -z "$markdown_files" ] || [ "$markdown_files" = "[]" ]; then
            echo "No markdown files to validate"
            exit 0
          fi
          
          allowed_languages=("en" "es" "it" "fr" "de" "ja" "ko" "pt_br")
          invalid_files='[]'
          
          # Loop through each markdown file
          while IFS= read -r file_path; do
            [ -z "$file_path" ] && continue
            
            # Extract language from file
            lang=""
            if [ -f "$file_path" ]; then
              lang=$(sed -n '1,50p' "$file_path" | grep -m1 -E '^[[:space:]]*language:[[:space:]]*' | sed -E 's/^[[:space:]]*language:[[:space:]]*//')
              if [ -n "$lang" ]; then
                # Trim and remove quotes
                lang=$(printf '%s' "$lang" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')
                if [[ "$lang" =~ ^\".*\"$ ]]; then lang=${lang:1:${#lang}-2}; fi
                if [[ "$lang" =~ ^\'.*\'$ ]]; then lang=${lang:1:${#lang}-2}; fi
              fi
            fi
            
            # Check if language is valid
            lang_lower=$(echo "$lang" | tr '[:upper:]' '[:lower:]')
            is_valid=false
            
            if [ -n "$lang_lower" ]; then
              for allowed in "${allowed_languages[@]}"; do
                if [ "$lang_lower" = "$allowed" ]; then
                  is_valid=true
                  break
                fi
              done
            fi
            
            # Add to invalid list if not valid
            if [ "$is_valid" = false ]; then
              invalid_files=$(echo "$invalid_files" | jq -c --arg file "$file_path" --arg lang "$lang" '. + [{file:$file, language:$lang}]')
            fi
          done < <(echo "$markdown_files" | jq -r '.[]')
          
          invalid_count=$(echo "$invalid_files" | jq 'length')
          
          if [ "$invalid_count" -ne 0 ]; then
            echo "Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br" >&2
            echo "$invalid_files" | jq . >&2
            jq -n --argjson files "$invalid_files" --arg msg "Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br" '{type:"language", message:$msg, files:$files}' > validation-error.json
            # Strip 'pr/' prefix from file paths in error output for cleaner display
            if [ -f validation-error.json ]; then
              jq '.files |= map(.file |= sub("^pr/"; ""))' validation-error.json > validation-error.tmp.json
              mv validation-error.tmp.json validation-error.json
            fi
            exit 1
          else
            echo "Language validation passed"
            exit 0
          fi

      - name: Comment PR with validation findings (on failure)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && (steps.check_renamed_folders.outcome == 'failure' || steps.validate_single_markdown.outcome == 'failure' || steps.check_large_files.outcome == 'failure' || steps.validate.outcome == 'failure' || steps.validate_categories.outcome == 'failure' || steps.validate_frontmatter.outcome == 'failure' || steps.validate_language.outcome == 'failure') 
        uses: actions/github-script@v7
        env:
          LARGE_FILES_JSON: ${{ steps.check_large_files.outputs.large_files_json }}
        with:
          script: |
            const fs = require('fs');
            const sections = [];
            
            // Collect all validation failures
            
            // 1. Folder renames
            if (fs.existsSync('folder-rename-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('folder-rename-error.json','utf8'));
                if (data && data.type === 'folder_rename') {
                  const renames = Array.isArray(data.renames) ? data.renames : [];
                  if (renames.length) {
                    const renameList = renames.map(r => `- \`${r.old_folder}\` â†’ \`${r.new_folder}\``).join('\n');
                    sections.push(`### ğŸ“ Folder Rename Detected\n\n${data.message}\n\n${renameList}`);
                  }
                }
              } catch {}
            }
            
            // 2. Large files
            const largeFiles = JSON.parse(process.env.LARGE_FILES_JSON || '[]');
            if (largeFiles.length > 0) {
              const fileList = largeFiles.map(f => `- \`${f}\``).join('\n');
              sections.push(`### ğŸ“¦ Large Files Detected\n\nFiles larger than 1MB found:\n\n${fileList}\n\nYou have two options: \n 1. Reduce the file size(s) before merging (best for images); or \n 2. Move your file(s) into a folder beginning with an underscore (e.g., \`_large_files/\`). Note that in this case, your file will NOT be uploaded to snowflake.com, and you will have to [link to it directly](https://docs.github.com/en/repositories/working-with-files/using-files/getting-permanent-links-to-files). `);
            }
            
            // 3. Multiple markdown files
            if (fs.existsSync('multiple-md-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('multiple-md-error.json','utf8'));
                if (data && data.type === 'multiple_markdown_files') {
                  const errors = Array.isArray(data.errors) ? data.errors : [];
                  if (errors.length) {
                    const lines = errors.map(err => `- **${err.folder}** (${err.files.length} files):\n${err.files.map(f => `  - \`${f}\``).join('\n')}`);
                    sections.push(`### ğŸ“„ Multiple Markdown Files Detected\n\n${data.message}\n\n${lines.join('\n\n')}`);
                  }
                }
              } catch {}
            }
            
            // 4. Language validation
            if (fs.existsSync('validation-error.json')) {
              try {
                const v = JSON.parse(fs.readFileSync('validation-error.json','utf8'));
                if (v && v.type === 'language') {
                  const items = Array.isArray(v.files) ? v.files.map(f => `- \`${f.file}\`: "${f.language || ''}"`).join('\n') : '';
                  sections.push(`### ğŸŒ Language Validation Failed\n\n${v.message || 'Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br'}\n\n${items}`);
                }
              } catch {}
            }
            
            // 5. Categories validation
            if (fs.existsSync('categories-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('categories-error.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (issues.length) {
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.invalid || []).join(', ')}`);
                  sections.push(`### ğŸ·ï¸ Category Syntax Validation Failed\n\nExpected categories like 'snowflake-site:taxonomy/x/y'.\n\n${lines.join('\n')}`);
                } else {
                  sections.push(`### ğŸ·ï¸ Category Syntax Validation Failed\n\nCategory syntax is invalid.`);
                }
              } catch {}
            }
            
            // 6. Frontmatter validation
            if (fs.existsSync('frontmatter-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('frontmatter-error.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (issues.length) {
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.errors || []).join('; ')}`);
                  sections.push(`### ğŸ“„ Frontmatter Validation Failed\n\nFrontmatter id must be slugified and match file and folder.\n\n${lines.join('\n')}`);
                } else {
                  sections.push(`### ğŸ“„ Frontmatter Validation Failed\n\nFrontmatter id check failed.`);
                }
              } catch {}
            }
            
            // 7. Profanity validation
            if (fs.existsSync('profanity-report.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('profanity-report.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (data.error) {
                  // Installation error
                  sections.push(`### ğŸš« Profanity Check Failed\n\n${data.error}`);
                } else if (issues.length) {
                  // Abusive words found
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.words || []).join(', ')}`);
                  sections.push(`### ğŸš« Profanity Check Failed\n\nAbusive words detected:\n\n${lines.join('\n')}`);
                } else {
                  // Empty issues array (shouldn't happen with refactored script, but handle gracefully)
                  sections.push(`### ğŸš« Profanity Check Failed\n\nProfanity check failed but no issues were listed.`);
                }
              } catch {
                sections.push(`### ğŸš« Profanity Check Failed\n\nProfanity check encountered an error.`);
              }
            }
            
            // Build aggregated comment
            if (sections.length > 0) {
              const body = `## âš ï¸ Validation Failed\n\n${sections.join('\n\n---\n\n')}\n\nPlease fix the issues above before merging.`;
              
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
                body
              });
            }

      - name: Comment PR with informational messages (when present)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const warnings = [];
            
            // Check for non-image files in assets folders
            if (fs.existsSync('non-image-assets-warning.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('non-image-assets-warning.json','utf8'));
                if (data && data.type === 'non_image_assets') {
                  const files = Array.isArray(data.files) ? data.files : [];
                  if (files.length) {
                    const fileList = files.map(f => `- \`${f}\``).join('\n');
                    warnings.push(`### ğŸ’¡ Non-Image Files in Assets Folder\n\n${data.message}\n\n${fileList}`);
                  }
                }
              } catch {}
            }
            
            // 2. Non-markdown files in root folder
            if (fs.existsSync('non-markdown-root-info.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('non-markdown-root-info.json','utf8'));
                if (data && data.type === 'non_markdown_root') {
                  const files = Array.isArray(data.files) ? data.files : [];
                  if (files.length) {
                    const fileList = files.map(f => `- \`${f}\``).join('\n');
                    warnings.push(`### ğŸ’¡ Non-Markdown Files in Quickstart Root\n\n${data.message}\n\n${fileList}`);
                  }
                }
              } catch {}
            }
            
            // Post warning comment if any warnings exist
            if (warnings.length > 0) {
              const body = `## ğŸ‘‹ Helpful Information\n\n${warnings.join('\n\n---\n\n')}\n\nThese are informational messages and will not block your PR from being merged.`;
              
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
                body
              });
            }

      - name: Upload validation artifacts
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-artifacts
          path: |
            profanity-report.json
            validation-error.json
            categories-error.json
            frontmatter-error.json
            multiple-md-error.json
            non-markdown-root-info.json
            non-image-assets-warning.json
            folder-rename-error.json
          if-no-files-found: ignore

      - name: Check validation results and fail if needed
        id: validation_check
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && always()
        run: |
          # Check if any validation step failed
          all_passed=true
          if [ "${{ steps.check_renamed_folders.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_single_markdown.outcome }}" == "failure" ] || \
             [ "${{ steps.check_large_files.outcome }}" == "failure" ] || \
             [ "${{ steps.validate.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_categories.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_frontmatter.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_language.outcome }}" == "failure" ]; then
            all_passed=false
          fi
          
          # Set output for matrix job to use
          echo "all_passed=$all_passed" >> $GITHUB_OUTPUT
          echo "All validations passed: $all_passed"
          
          # Fail workflow if validations failed
          if [ "$all_passed" == "false" ]; then
            echo "âŒ Validation failed. See PR comment for details."
            exit 1
          fi

      - name: Prepare webhook data
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && steps.validation_check.outputs.all_passed == 'true'
        id: prepare_webhooks
        env:
          CHANGED_MARKDOWN_JSON: ${{ steps.detect.outputs.changed_markdown_json }}
        run: |
          # Use the already-computed changed markdown files from detect step (with pr/ prefix)
          changed_md_files=$(echo "$CHANGED_MARKDOWN_JSON" | jq -r '.[]')
          
          if [ -z "$changed_md_files" ]; then
            echo "No changed markdown files for webhooks"
            echo "quickstart_names_json=[]" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          # Extract quickstart names from paths (handles pr/ prefix)
          quickstart_names=$(echo "$changed_md_files" | sed -E 's|^pr/site/sfguides/src/([^/]+)/.*|\1|' | sort -u)
          objs='[]'
          
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Preparing webhook data for quickstarts:"
          echo "$quickstart_names"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          
          while IFS= read -r name; do
            [ -z "$name" ] && continue
            
            lang=""
            # Prefer a markdown file that matches the folder name
            candidate="pr/site/sfguides/src/$name/$name.md"
            if echo "$changed_md_files" | grep -q "^$candidate$"; then
              md_file="$candidate"
            else
              # Fallback: use the first markdown file in this quickstart
              md_file=$(echo "$changed_md_files" | grep "^pr/site/sfguides/src/$name/" | head -n1)
            fi
            
            # Extract language from the markdown file
            if [ -n "$md_file" ] && [ -f "$md_file" ]; then
              lang=$(sed -n '1,50p' "$md_file" | grep -m1 -E '^[[:space:]]*language:[[:space:]]*' | sed -E 's/^[[:space:]]*language:[[:space:]]*//')
              if [ -n "$lang" ]; then
                lang=$(printf '%s' "$lang" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')
                if [[ "$lang" =~ ^\".*\"$ ]]; then lang=${lang:1:${#lang}-2}; fi
                if [[ "$lang" =~ ^\'.*\'$ ]]; then lang=${lang:1:${#lang}-2}; fi
              fi
            fi
            
            echo "  - $name (language: ${lang:-not specified})"
            objs=$(echo "$objs" | jq -c --arg name "$name" --arg lang "$lang" '. + [{name:$name, language:$lang}]')
          done <<< "$quickstart_names"
          
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "Webhook data prepared:"
          echo "$objs" | jq '.'
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          
          echo "quickstart_names_json=$objs" >> $GITHUB_OUTPUT

  call_staging_webhooks:
    needs: validate_and_stage
    # Only call webhooks if validations passed, there are relevant changes, and less than 7 markdown files modified
    if: |
      needs.validate_and_stage.outputs.has_relevant_changes == 'true' && 
      needs.validate_and_stage.outputs.all_validations_passed == 'true' &&
      needs.validate_and_stage.outputs.md_file_count < 7
    runs-on: ubuntu-latest
    environment:
      name: staging
    permissions:
      contents: read
      pull-requests: write
    strategy:
      matrix:
        quickstart: ${{ fromJson(needs.validate_and_stage.outputs.quickstart_names_json) }}
      max-parallel: 1  # Process 1 at a time for rate limit purposes
      fail-fast: false  # Continue processing others even if one fails
    
    steps:
      - name: Call Workato staging webhook for ${{ matrix.quickstart.name }}
        id: call_webhook_matrix
        env:
          WORKATO_STAGING_WEBHOOK_URL: ${{ secrets.WORKATO_STAGING_SINGLE_FILE_URL }}
        run: |
          if [ -z "$WORKATO_STAGING_WEBHOOK_URL" ]; then
            echo "âŒ WORKATO_STAGING_WEBHOOK_URL is not set"
            exit 1
          fi
          
          qname="${{ matrix.quickstart.name }}"
          qlang="${{ matrix.quickstart.language }}"
          
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          echo "ğŸ“¦ Processing quickstart: $qname"
          echo "ğŸŒ Language: $qlang"
          echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
          
          # Create single-item array for backward compatibility with Workato
          qnames_array=$(jq -n --arg name "$qname" --arg lang "$qlang" '[{name:$name, language:$lang}]')
          
          payload=$(jq -n \
            --arg repo "$GITHUB_REPOSITORY" \
            --arg pr '${{ github.event.pull_request.number }}' \
            --arg sha '${{ github.event.pull_request.head.sha }}' \
            --arg env "staging" \
            --arg qname "$qname" \
            --argjson qnames "$qnames_array" \
            '{repo:$repo, pr_number: ($pr|tonumber), commit_sha:$sha, environment:$env, quickstart_name:$qname, quickstart_names:$qnames}')
          
          echo "Payload:"
          echo "$payload" | jq .
          
          # Retry configuration
          max_retries=3
          retry_count=0
          base_delay=10
          
          while [ $retry_count -le $max_retries ]; do
            if [ $retry_count -gt 0 ]; then
              echo "ğŸ”„ Retry attempt $retry_count of $max_retries..."
            else
              echo "ğŸ“¡ Triggering webhook..."
            fi
            
            response=$(curl -s -w "\n%{http_code}" -X POST "$WORKATO_STAGING_WEBHOOK_URL" \
              -H "Content-Type: application/json" \
              --data-raw "$payload" \
              --max-time 10 \
              --connect-timeout 5) || {
              exit_code=$?
              echo "âŒ Connection failed (curl exit code: $exit_code)"
              
              if [ $retry_count -lt $max_retries ]; then
                retry_count=$((retry_count + 1))
                delay=$((base_delay * retry_count))
                echo "â³ Waiting ${delay}s before retry..."
                sleep $delay
                continue
              else
                echo "âŒ Max retries reached"
                exit 1
              fi
            }
            
            http_code=$(echo "$response" | tail -n1)
            response_body=$(echo "$response" | sed '$d')
            
            echo "HTTP Status: $http_code"
            echo "Response:"
            echo "$response_body" | jq . 2>/dev/null || echo "$response_body"
            
            if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
              echo "âœ… Webhook succeeded for: $qname"
              # Wait 2 minutes after processing to avoid rate limits for next item
              echo "â³ Waiting 2 minutes before next webhook..."
              sleep 120
              exit 0
            fi
            
            # Rate limit handling
            if echo "$response_body" | grep -qi "rate limit"; then
              if [ $retry_count -lt $max_retries ]; then
                retry_count=$((retry_count + 1))
                delay=$((base_delay * retry_count * 2))  # Longer delay for rate limits
                echo "âš ï¸ Rate limit detected. Waiting ${delay}s..."
                sleep $delay
                continue
              fi
            elif [ $retry_count -lt $max_retries ]; then
              retry_count=$((retry_count + 1))
              delay=$((base_delay * retry_count))
              echo "âš ï¸ HTTP $http_code. Waiting ${delay}s before retry..."
              sleep $delay
              continue
            fi
            
            echo "âŒ Failed after $max_retries retries"
            exit 1
          done


      - name: Report webhook failure to PR
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const qname = '${{ matrix.quickstart.name }}';
            const body = `## âš ï¸ Staging Webhook Failed for \`${qname}\`
            
            The staging webhook call failed for quickstart **${qname}** after multiple retry attempts.
            
            ### Next Steps
            - Check the [workflow logs](${context.payload.repository.html_url}/actions/runs/${context.runId}) for details
            - If due to rate limiting, wait a few minutes and re-run the failed job
            - Contact the devrel team if the issue persists
            `;
            
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              body
            });
