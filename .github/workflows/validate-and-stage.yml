name: Validate markdown and stage

on:
  pull_request_target:
    types: [opened, synchronize, reopened, edited, ready_for_review]
    branches: [master, github-actions]

jobs:
  validate_and_stage:
    # Only run for PRs from branches in this repo (not forks) into master
    runs-on: ubuntu-latest
    environment:
      name: staging
    permissions:
      contents: read
      issues: write
      pull-requests: write
    outputs:
      has_relevant_changes: ${{ steps.check_relevant.outputs.has_relevant_changes }}
      quickstart_names_json: ${{ steps.detect.outputs.quickstart_names_json }}
      all_validations_passed: ${{ steps.validation_check.outputs.all_passed }}
      md_file_count: ${{ steps.detect.outputs.md_file_count }}
      
    steps:
      - name: Check for relevant changes (early exit)
        id: check_relevant
        env:
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          pr_number=${{ github.event.pull_request.number }}
          repo=${{ github.repository }}
          files_json=$(curl -sS -H "Authorization: Bearer $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "https://api.github.com/repos/$repo/pulls/$pr_number/files?per_page=100")

          # Quick check if any files are under site/sfguides/src/ (excluding _* folders)
          relevant_files=$(echo "$files_json" | jq -r '.[]?.filename | select(startswith("site/sfguides/src/")) | select(test("/_") | not)')
          if [ -z "$relevant_files" ]; then
            echo "has_relevant_changes=false" >> $GITHUB_OUTPUT
            echo ":information_source: No changes in site/sfguides/src/, skipping checkout and validation"
            exit 0
          fi
          echo "has_relevant_changes=true" >> $GITHUB_OUTPUT

      - name: Checkout base repository (trusted)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          ref: ${{ github.event.pull_request.base.sha }}
          sparse-checkout: |
            scripts/
          sparse-checkout-cone-mode: true

      - name: Download changed files from PR (untrusted)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: download
        env:
          GITHUB_TOKEN: ${{ github.token }}
        run: |
          pr_number=${{ github.event.pull_request.number }}
          repo=${{ github.repository }}
          
          # Fetch changed files from PR (excluding deleted files)
          files_json=$(curl -sS -H "Authorization: Bearer $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "https://api.github.com/repos/$repo/pulls/$pr_number/files?per_page=100")
          
          # Save files_json to a file for reuse in subsequent steps
          echo "$files_json" > pr_files.json
          
          # Filter for changed files (excluding folders starting with _)
          changed_files=$(echo "$files_json" | jq -r '.[] 
            | select(.status != "removed") 
            | .filename 
            | select(startswith("site/sfguides/src/")) 
            | select(test("/_") | not)')
          
          # Early exit if no files to download
          if [ -z "$changed_files" ]; then
            echo "No relevant files to download for validation"
            echo "" > downloaded_files.txt
            exit 0
          fi
          
          head_repo="${{ github.event.pull_request.head.repo.full_name }}"
          head_ref="${{ github.event.pull_request.head.ref }}"
          
          # Download all changed files
          echo "Downloading changed files..."
          while IFS= read -r file; do
            [ -z "$file" ] && continue
            
            # Create directory structure
            mkdir -p "pr/$(dirname "$file")"
            
            # Download raw file content
            url="https://raw.githubusercontent.com/$head_repo/$head_ref/$file"
            echo "Downloading: $file"
            
            if ! curl -sS -f -H "Authorization: Bearer $GITHUB_TOKEN" "$url" -o "pr/$file"; then
              echo "Warning: Failed to download $file" >&2
            fi
          done <<< "$changed_files"
          
          # Save changed files to list
          echo "$changed_files" > downloaded_files.txt
          
          # Extract folders that have changes
          modified_folders=$(echo "$changed_files" | sed -E 's|^site/sfguides/src/([^/]+)/.*|\1|' | sort -u)
          
          # Download ALL markdown files from modified folders (even if not changed)
          # This allows us to validate that each folder has only one markdown file
          if [ -n "$modified_folders" ]; then
            echo "Downloading all markdown files from modified folders..."
            
            while IFS= read -r folder; do
              [ -z "$folder" ] && continue
              
              # Use GitHub API to list all files in the folder
              folder_path="site/sfguides/src/$folder"
              tree_url="https://api.github.com/repos/$head_repo/contents/$folder_path?ref=$head_ref"
              
              # Get all .md files in the folder (excluding README.md)
              folder_files=$(curl -sS -H "Authorization: Bearer $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "$tree_url" | jq -r '.[] | select(.type=="file") | select(.name | endswith(".md")) | select(.name | test("^README\\.md$"; "i") | not) | .name')
              
              while IFS= read -r filename; do
                [ -z "$filename" ] && continue
                
                file_path="$folder_path/$filename"
                
                # Skip if already downloaded
                if [ -f "pr/$file_path" ]; then
                  continue
                fi
                
                # Create directory structure
                mkdir -p "pr/$folder_path"
                
                # Download the file
                url="https://raw.githubusercontent.com/$head_repo/$head_ref/$file_path"
                echo "Downloading additional markdown: $file_path"
                
                if curl -sS -f -H "Authorization: Bearer $GITHUB_TOKEN" "$url" -o "pr/$file_path"; then
                  # Add to downloaded files list
                  echo "$file_path" >> downloaded_files.txt
                else
                  echo "Warning: Failed to download $file_path" >&2
                fi
              done <<< "$folder_files"
            done <<< "$modified_folders"
          fi
          
          echo "Download complete"

      - name: Setup Node.js
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Cache npm dependencies
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        uses: actions/cache@v4
        id: npm-cache
        with:
          path: ~/.npm
          key: ${{ runner.os }}-npm-badwords-graymatter-v1
          restore-keys: |
            ${{ runner.os }}-npm-

      - name: Install validator dependencies
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        run: |
          # Create package.json if it doesn't exist
          if [ ! -f package.json ]; then
            npm init -y >/dev/null 2>&1
          fi
          # Install both packages in a single command (faster than separate installs)
          npm install --no-audit --no-fund badwords-list@1.0.0 gray-matter@4.0.3
          # Verify installation
          node -e "require('badwords-list'); require('gray-matter'); console.log('Dependencies installed successfully')"

      - name: Determine changed files in quickstarts (from PR)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: detect
        run: |
          # Read the list of downloaded files from previous step
          downloaded_files=$(cat downloaded_files.txt)
          
          # Filter out non-image files from assets folders
          # Keep: markdown files outside assets, image files from assets
          # Exclude: non-image files from assets (like .md, .txt, etc.)
          all_changed_files=$(echo "$downloaded_files" | grep -v '/assets/' || true)
          assets_images=$(echo "$downloaded_files" | grep '/assets/' | grep -E '\.(jpg|jpeg|png|gif|svg|webp|bmp|ico)$' || true)
          if [ -n "$assets_images" ]; then
            all_changed_files=$(printf '%s\n%s\n' "$all_changed_files" "$assets_images")
          fi
          
          # Extract markdown files (already excluded from assets by filter above)
          md_files=$(echo "$all_changed_files" | grep '\.md$' || true)
          
          if [ -z "$all_changed_files" ]; then 
            all_changed_files_json='[]'
          else
            all_changed_files_json=$(printf '%s\n' "$all_changed_files" | jq -R -s -c 'split("\n") | map(select(length>0))')
          fi

          # Prepare markdown files for validation
          if [ -z "$md_files" ]; then
            markdown_json='[]'
            md_file_count=0
            quickstart_names_json='[]'
            quickstart_name=''
          else
            # Build markdown_json with all downloaded markdown files
            markdown_json=$(printf '%s\n' "$md_files" | awk '{print "pr/" $0}' | jq -R -s -c 'split("\n") | map(select(length>0))')
            md_file_count=$(echo "$md_files" | wc -l | tr -d ' ')
            
            # Extract quickstart names and languages for webhook
            quickstart_names=$(echo "$md_files" | sed -E 's|^site/sfguides/src/([^/]+)/.*|\1|' | sort -u)
            objs='[]'
            
            while IFS= read -r name; do
              [ -z "$name" ] && continue
              
              lang=""
              # Prefer a markdown file that matches the folder name
              candidate="site/sfguides/src/$name/$name.md"
              if echo "$md_files" | grep -q "^$candidate$"; then
                md_file="$candidate"
              else
                # Fallback: use the first markdown file in this quickstart
                md_file=$(echo "$md_files" | grep "^site/sfguides/src/$name/" | head -n1)
              fi
              
              # Extract language from the markdown file
              if [ -n "$md_file" ] && [ -f "pr/$md_file" ]; then
                lang=$(sed -n '1,50p' "pr/$md_file" | grep -m1 -E '^[[:space:]]*language:[[:space:]]*' | sed -E 's/^[[:space:]]*language:[[:space:]]*//')
                if [ -n "$lang" ]; then
                  lang=$(printf '%s' "$lang" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')
                  if [[ "$lang" =~ ^\".*\"$ ]]; then lang=${lang:1:${#lang}-2}; fi
                  if [[ "$lang" =~ ^\'.*\'$ ]]; then lang=${lang:1:${#lang}-2}; fi
                fi
              fi
              
              objs=$(echo "$objs" | jq -c --arg name "$name" --arg lang "$lang" '. + [{name:$name, language:$lang}]')
            done <<< "$quickstart_names"

            quickstart_names_json="$objs"
            quickstart_name=$(echo "$quickstart_names" | head -n1)
          fi

          # Print debug information
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "All changed files:"
          echo "$all_changed_files"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "Markdown files (for validation):"
          echo "$markdown_json" | jq -r '.[]' 2>/dev/null || echo "$markdown_json"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          
          echo "markdown_json=$markdown_json" >> $GITHUB_OUTPUT
          echo "quickstart_names_json=$quickstart_names_json" >> $GITHUB_OUTPUT
          echo "quickstart_name=$quickstart_name" >> $GITHUB_OUTPUT
          echo "all_changed_files_json=$all_changed_files_json" >> $GITHUB_OUTPUT
          echo "md_file_count=$md_file_count" >> $GITHUB_OUTPUT

      - name: Validate single markdown file per folder
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_single_markdown
        continue-on-error: true
        run: |
          # Define error message once
          ERROR_MESSAGE="Each quickstart folder should contain only one markdown file in the root. Did you mean to add this additional .md file to the /assets folder?"
          
          # Extract quickstart folders that have markdown changes
          downloaded_files=$(cat downloaded_files.txt)
          md_files=$(echo "$downloaded_files" | grep '\.md$' || true)
          
          if [ -z "$md_files" ]; then
            echo "No markdown files to validate"
            exit 0
          fi
          
          quickstart_folders=$(echo "$md_files" | sed -E 's|^site/sfguides/src/([^/]+)/.*|\1|' | sort -u)
          multiple_files_errors='[]'
          
          # For each folder with markdown changes, check if multiple markdown files exist
          while IFS= read -r folder; do
            [ -z "$folder" ] && continue
            
            # Get ALL markdown files in this quickstart folder
            folder_md_files=$(find "pr/site/sfguides/src/$folder" -maxdepth 1 -type f -name "*.md" 2>/dev/null | sed 's|^pr/||' || echo "")
            file_count=$(echo "$folder_md_files" | grep -c '^' || echo 0)
            
            # Check if more than one markdown file exists
            if [ "$file_count" -gt 1 ]; then
              # Convert files to JSON array
              files_array=$(echo "$folder_md_files" | jq -R -s -c 'split("\n") | map(select(length>0))')
              multiple_files_errors=$(echo "$multiple_files_errors" | jq -c --arg folder "$folder" --argjson files "$files_array" '. + [{folder:$folder, files:$files}]')
            fi
          done <<< "$quickstart_folders"
          
          # Check if any folders have multiple markdown files
          error_count=$(echo "$multiple_files_errors" | jq 'length')
          if [ "$error_count" -ne 0 ]; then
            echo "‚ùå Error: Multiple markdown files found in the following folders:" >&2
            echo "$multiple_files_errors" | jq -r '.[] | "  - \(.folder): \(.files | length) files - \(.files | join(", "))"' >&2
            jq -n --argjson errors "$multiple_files_errors" --arg msg "$ERROR_MESSAGE" '{type:"multiple_markdown_files", message:$msg, errors:$errors}' > multiple-md-error.json
            exit 1
          else
            echo "‚úÖ Single markdown file validation passed"
            exit 0
          fi

      - name: Validate no non-markdown files outside assets
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_file_types
        continue-on-error: true
        run: |
          # Define error message once
          ERROR_MESSAGE="Only markdown files (.md) are allowed in the root of quickstart folders. Other file types should be placed in the /assets folder."
          
          # Read downloaded files
          downloaded_files=$(cat downloaded_files.txt)
          
          if [ -z "$downloaded_files" ]; then
            echo "No files to validate"
            exit 0
          fi
          
          # Find non-markdown files outside of assets folder (in root of quickstart folders)
          # Pattern: site/sfguides/src/{folder}/{filename} where filename is not .md and path doesn't contain /assets/
          invalid_files=$(echo "$downloaded_files" | grep '^site/sfguides/src/[^/]\+/[^/]\+$' | grep -v '\.md$' || true)
          
          if [ -n "$invalid_files" ]; then
            echo "‚ùå Error: Non-markdown files found outside assets folder:" >&2
            echo "$invalid_files" >&2
            
            # Build error JSON
            files_array=$(echo "$invalid_files" | jq -R -s -c 'split("\n") | map(select(length>0))')
            jq -n --argjson files "$files_array" --arg msg "$ERROR_MESSAGE" '{type:"invalid_file_types", message:$msg, files:$files}' > invalid-file-types-error.json
            exit 1
          else
            echo "‚úÖ File type validation passed"
            exit 0
          fi

      - name: Check for large files
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: check_large_files
        continue-on-error: true
        env:
          CHANGED_FILES_JSON: ${{ steps.detect.outputs.all_changed_files_json }}
        run: |
          # Check only files that were changed in the PR
          large_files=()
          changed_files=$(echo "$CHANGED_FILES_JSON" | jq -r '.[]')
          
          if [ -z "$changed_files" ]; then
            echo "No files to check"
            exit 0
          fi
          
          while IFS= read -r file; do
            [ -z "$file" ] && continue
            
            # Skip files in folders starting with _ (e.g., _archived)
            if [[ "$file" =~ ^site/sfguides/src/_ ]]; then
              echo "Skipping $file (in excluded directory)"
              continue
            fi
            
            file_path="pr/$file"
            
            if [ -f "$file_path" ]; then
              file_size=$(stat -f%z "$file_path" 2>/dev/null || stat -c%s "$file_path" 2>/dev/null || echo "0")
              if [ "$file_size" -gt 1000000 ]; then
                large_files+=("$file")
              fi
            fi
          done <<< "$changed_files"
          
          if [ ${#large_files[@]} -gt 0 ]; then
            echo "Large files found:"
            printf '%s\n' "${large_files[@]}"
            # Output large files as JSON array for use in next step
            printf '%s\n' "${large_files[@]}" | jq -R -s -c 'split("\n") | map(select(length>0))' > large_files.json
            echo "large_files_json=$(cat large_files.json)" >> $GITHUB_OUTPUT
            exit 1
          else
            echo "No large files found"
            echo "large_files_json=[]" >> $GITHUB_OUTPUT
            exit 0
          fi

      - name: Validate abusive words in markdown
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.markdown_json }}
          BLOCKLIST_STRING: ${{ vars.PROFANITY_BLOCKLIST }}
        run: |
          node scripts/validate-profanity.js || {
            # Strip 'pr/' prefix from file paths in error output for cleaner display
            if [ -f profanity-report.json ]; then
              jq '.issues |= map(.file |= sub("^pr/"; ""))' profanity-report.json > profanity-report.tmp.json
              mv profanity-report.tmp.json profanity-report.json
            fi
            echo "Abusive words detected. See 'profanity-report.json' for details." >&2
            exit 1
          }

      - name: Validate categories syntax in markdown
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_categories
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.markdown_json }}
        run: |
          node scripts/validate-categories.js || {
            # Strip 'pr/' prefix from file paths in error output for cleaner display
            if [ -f categories-error.json ]; then
              jq '.issues |= map(.file |= sub("^pr/"; ""))' categories-error.json > categories-error.tmp.json
              mv categories-error.tmp.json categories-error.json
            fi
            echo "Category syntax validation failed. See 'categories-error.json' for details." >&2
            exit 1
          }

      - name: Validate frontmatter id and path
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_frontmatter
        continue-on-error: true
        env:
          FILE_LIST_JSON: ${{ steps.detect.outputs.markdown_json }}
        run: |
          node scripts/validate-frontmatter.js || {
            # Strip 'pr/' prefix from file paths in error output for cleaner display
            if [ -f frontmatter-error.json ]; then
              jq '.issues |= map(.file |= sub("^pr/"; ""))' frontmatter-error.json > frontmatter-error.tmp.json
              mv frontmatter-error.tmp.json frontmatter-error.json
            fi
            echo "Frontmatter validation failed. See 'frontmatter-error.json' for details." >&2
            exit 1
          }

      - name: Validate language tags
        if: steps.check_relevant.outputs.has_relevant_changes == 'true'
        id: validate_language
        continue-on-error: true
        env:
          MARKDOWN_JSON: ${{ steps.detect.outputs.markdown_json }}
        run: |
          # Validate languages in individual markdown files
          markdown_files="$MARKDOWN_JSON"
          if [ -z "$markdown_files" ] || [ "$markdown_files" = "[]" ]; then
            echo "No markdown files to validate"
            exit 0
          fi
          
          allowed_languages=("en" "es" "it" "fr" "de" "ja" "ko" "pt_br")
          invalid_files='[]'
          
          # Loop through each markdown file
          while IFS= read -r file_path; do
            [ -z "$file_path" ] && continue
            
            # Extract language from file
            lang=""
            if [ -f "$file_path" ]; then
              lang=$(sed -n '1,50p' "$file_path" | grep -m1 -E '^[[:space:]]*language:[[:space:]]*' | sed -E 's/^[[:space:]]*language:[[:space:]]*//')
              if [ -n "$lang" ]; then
                # Trim and remove quotes
                lang=$(printf '%s' "$lang" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')
                if [[ "$lang" =~ ^\".*\"$ ]]; then lang=${lang:1:${#lang}-2}; fi
                if [[ "$lang" =~ ^\'.*\'$ ]]; then lang=${lang:1:${#lang}-2}; fi
              fi
            fi
            
            # Check if language is valid
            lang_lower=$(echo "$lang" | tr '[:upper:]' '[:lower:]')
            is_valid=false
            
            if [ -n "$lang_lower" ]; then
              for allowed in "${allowed_languages[@]}"; do
                if [ "$lang_lower" = "$allowed" ]; then
                  is_valid=true
                  break
                fi
              done
            fi
            
            # Add to invalid list if not valid
            if [ "$is_valid" = false ]; then
              invalid_files=$(echo "$invalid_files" | jq -c --arg file "$file_path" --arg lang "$lang" '. + [{file:$file, language:$lang}]')
            fi
          done < <(echo "$markdown_files" | jq -r '.[]')
          
          invalid_count=$(echo "$invalid_files" | jq 'length')
          
          if [ "$invalid_count" -ne 0 ]; then
            echo "Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br" >&2
            echo "$invalid_files" | jq . >&2
            jq -n --argjson files "$invalid_files" --arg msg "Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br" '{type:"language", message:$msg, files:$files}' > validation-error.json
            # Strip 'pr/' prefix from file paths in error output for cleaner display
            if [ -f validation-error.json ]; then
              jq '.files |= map(.file |= sub("^pr/"; ""))' validation-error.json > validation-error.tmp.json
              mv validation-error.tmp.json validation-error.json
            fi
            exit 1
          else
            echo "Language validation passed"
            exit 0
          fi

      - name: Comment PR with validation findings (on failure)
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && (steps.validate_single_markdown.outcome == 'failure' || steps.validate_file_types.outcome == 'failure' || steps.check_large_files.outcome == 'failure' || steps.validate.outcome == 'failure' || steps.validate_categories.outcome == 'failure' || steps.validate_frontmatter.outcome == 'failure' || steps.validate_language.outcome == 'failure') 
        uses: actions/github-script@v7
        env:
          LARGE_FILES_JSON: ${{ steps.check_large_files.outputs.large_files_json }}
        with:
          script: |
            const fs = require('fs');
            const sections = [];
            
            // Collect all validation failures
            
            // 1. Large files
            const largeFiles = JSON.parse(process.env.LARGE_FILES_JSON || '[]');
            if (largeFiles.length > 0) {
              const fileList = largeFiles.map(f => `- \`${f}\``).join('\n');
              sections.push(`### üì¶ Large Files Detected\n\nFiles larger than 1MB found:\n\n${fileList}\n\nPlease either reduce the file size(s) before merging, or move the file(s) into a folder beginning with an underscore (e.g., _large_files/). Note that your file will NOT be uploaded to snowflake.com if it is larger than 1MB, and you will have to link to it directly. `);
            }
            
            // 2. Multiple markdown files
            if (fs.existsSync('multiple-md-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('multiple-md-error.json','utf8'));
                if (data && data.type === 'multiple_markdown_files') {
                  const errors = Array.isArray(data.errors) ? data.errors : [];
                  if (errors.length) {
                    const lines = errors.map(err => `- **${err.folder}** (${err.files.length} files):\n${err.files.map(f => `  - \`${f}\``).join('\n')}`);
                    sections.push(`### üìÑ Multiple Markdown Files Detected\n\n${data.message}\n\n${lines.join('\n\n')}`);
                  }
                }
              } catch {}
            }
            
            // 3. Invalid file types
            if (fs.existsSync('invalid-file-types-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('invalid-file-types-error.json','utf8'));
                if (data && data.type === 'invalid_file_types') {
                  const files = Array.isArray(data.files) ? data.files : [];
                  if (files.length) {
                    const fileList = files.map(f => `- \`${f}\``).join('\n');
                    sections.push(`### üö´ Invalid File Types Detected\n\n${data.message}\n\n${fileList}`);
                  }
                }
              } catch {}
            }
            
            // 4. Language validation
            if (fs.existsSync('validation-error.json')) {
              try {
                const v = JSON.parse(fs.readFileSync('validation-error.json','utf8'));
                if (v && v.type === 'language') {
                  const items = Array.isArray(v.files) ? v.files.map(f => `- \`${f.file}\`: "${f.language || ''}"`).join('\n') : '';
                  sections.push(`### üåê Language Validation Failed\n\n${v.message || 'Invalid or missing language detected. Allowed: en, es, it, fr, de, ja, ko, pt_br'}\n\n${items}`);
                }
              } catch {}
            }
            
            // 5. Categories validation
            if (fs.existsSync('categories-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('categories-error.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (issues.length) {
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.invalid || []).join(', ')}`);
                  sections.push(`### üè∑Ô∏è Category Syntax Validation Failed\n\nExpected categories like 'snowflake-site:taxonomy/x/y'.\n\n${lines.join('\n')}`);
                } else {
                  sections.push(`### üè∑Ô∏è Category Syntax Validation Failed\n\nCategory syntax is invalid.`);
                }
              } catch {}
            }
            
            // 6. Frontmatter validation
            if (fs.existsSync('frontmatter-error.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('frontmatter-error.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (issues.length) {
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.errors || []).join('; ')}`);
                  sections.push(`### üìÑ Frontmatter Validation Failed\n\nFrontmatter id must be slugified and match file and folder.\n\n${lines.join('\n')}`);
                } else {
                  sections.push(`### üìÑ Frontmatter Validation Failed\n\nFrontmatter id check failed.`);
                }
              } catch {}
            }
            
            // 7. Profanity validation
            if (fs.existsSync('profanity-report.json')) {
              try {
                const data = JSON.parse(fs.readFileSync('profanity-report.json','utf8'));
                const issues = Array.isArray(data.issues) ? data.issues : [];
                if (data.error) {
                  // Installation error
                  sections.push(`### üö´ Profanity Check Failed\n\n${data.error}`);
                } else if (issues.length) {
                  // Abusive words found
                  const lines = issues.map(issue => `- \`${issue.file}\`: ${(issue.words || []).join(', ')}`);
                  sections.push(`### üö´ Profanity Check Failed\n\nAbusive words detected:\n\n${lines.join('\n')}`);
                } else {
                  // Empty issues array (shouldn't happen with refactored script, but handle gracefully)
                  sections.push(`### üö´ Profanity Check Failed\n\nProfanity check failed but no issues were listed.`);
                }
              } catch {
                sections.push(`### üö´ Profanity Check Failed\n\nProfanity check encountered an error.`);
              }
            }
            
            // Build aggregated comment
            if (sections.length > 0) {
              const body = `## ‚ö†Ô∏è Validation Failed\n\n${sections.join('\n\n---\n\n')}\n\nPlease fix the issues above before merging.`;
              
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.payload.pull_request.number,
                body
              });
            }

      - name: Upload validation artifacts
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-artifacts
          path: |
            profanity-report.json
            validation-error.json
            categories-error.json
            frontmatter-error.json
            multiple-md-error.json
            invalid-file-types-error.json
          if-no-files-found: ignore

      - name: Check validation results and fail if needed
        id: validation_check
        if: steps.check_relevant.outputs.has_relevant_changes == 'true' && always()
        run: |
          # Check if any validation step failed
          all_passed=true
          if [ "${{ steps.validate_single_markdown.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_file_types.outcome }}" == "failure" ] || \
             [ "${{ steps.check_large_files.outcome }}" == "failure" ] || \
             [ "${{ steps.validate.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_categories.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_frontmatter.outcome }}" == "failure" ] || \
             [ "${{ steps.validate_language.outcome }}" == "failure" ]; then
            all_passed=false
          fi
          
          # Set output for matrix job to use
          echo "all_passed=$all_passed" >> $GITHUB_OUTPUT
          echo "All validations passed: $all_passed"
          
          # Fail workflow if validations failed
          if [ "$all_passed" == "false" ]; then
            echo "‚ùå Validation failed. See PR comment for details."
            exit 1
          fi

  call_staging_webhooks:
    needs: validate_and_stage
    # Only call webhooks if validations passed, there are relevant changes, and less than 7 markdown files modified
    if: |
      needs.validate_and_stage.outputs.has_relevant_changes == 'true' && 
      needs.validate_and_stage.outputs.all_validations_passed == 'true' &&
      needs.validate_and_stage.outputs.md_file_count < 7
    runs-on: ubuntu-latest
    environment:
      name: staging
    permissions:
      contents: read
      pull-requests: write
    strategy:
      matrix:
        quickstart: ${{ fromJson(needs.validate_and_stage.outputs.quickstart_names_json) }}
      max-parallel: 1  # Process 1 at a time for rate limit purposes
      fail-fast: false  # Continue processing others even if one fails
    
    steps:
      - name: Call Workato staging webhook for ${{ matrix.quickstart.name }}
        id: call_webhook_matrix
        env:
          WORKATO_STAGING_WEBHOOK_URL: ${{ secrets.WORKATO_STAGING_SINGLE_FILE_URL }}
        run: |
          if [ -z "$WORKATO_STAGING_WEBHOOK_URL" ]; then
            echo "‚ùå WORKATO_STAGING_WEBHOOK_URL is not set"
            exit 1
          fi
          
          qname="${{ matrix.quickstart.name }}"
          qlang="${{ matrix.quickstart.language }}"
          
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "üì¶ Processing quickstart: $qname"
          echo "üåê Language: $qlang"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          
          # Create single-item array for backward compatibility with Workato
          qnames_array=$(jq -n --arg name "$qname" --arg lang "$qlang" '[{name:$name, language:$lang}]')
          
          payload=$(jq -n \
            --arg repo "$GITHUB_REPOSITORY" \
            --arg pr '${{ github.event.pull_request.number }}' \
            --arg sha '${{ github.event.pull_request.head.sha }}' \
            --arg env "staging" \
            --arg qname "$qname" \
            --argjson qnames "$qnames_array" \
            '{repo:$repo, pr_number: ($pr|tonumber), commit_sha:$sha, environment:$env, quickstart_name:$qname, quickstart_names:$qnames}')
          
          echo "Payload:"
          echo "$payload" | jq .
          
          # Retry configuration
          max_retries=3
          retry_count=0
          base_delay=10
          
          while [ $retry_count -le $max_retries ]; do
            if [ $retry_count -gt 0 ]; then
              echo "üîÑ Retry attempt $retry_count of $max_retries..."
            else
              echo "üì° Triggering webhook..."
            fi
            
            response=$(curl -s -w "\n%{http_code}" -X POST "$WORKATO_STAGING_WEBHOOK_URL" \
              -H "Content-Type: application/json" \
              --data-raw "$payload" \
              --max-time 10 \
              --connect-timeout 5) || {
              exit_code=$?
              echo "‚ùå Connection failed (curl exit code: $exit_code)"
              
              if [ $retry_count -lt $max_retries ]; then
                retry_count=$((retry_count + 1))
                delay=$((base_delay * retry_count))
                echo "‚è≥ Waiting ${delay}s before retry..."
                sleep $delay
                continue
              else
                echo "‚ùå Max retries reached"
                exit 1
              fi
            }
            
            http_code=$(echo "$response" | tail -n1)
            response_body=$(echo "$response" | sed '$d')
            
            echo "HTTP Status: $http_code"
            echo "Response:"
            echo "$response_body" | jq . 2>/dev/null || echo "$response_body"
            
            if [ "$http_code" -ge 200 ] && [ "$http_code" -lt 300 ]; then
              echo "‚úÖ Webhook succeeded for: $qname"
              # Wait 2 minutes after processing to avoid rate limits for next item
              echo "‚è≥ Waiting 2 minutes before next webhook..."
              sleep 120
              exit 0
            fi
            
            # Rate limit handling
            if echo "$response_body" | grep -qi "rate limit"; then
              if [ $retry_count -lt $max_retries ]; then
                retry_count=$((retry_count + 1))
                delay=$((base_delay * retry_count * 2))  # Longer delay for rate limits
                echo "‚ö†Ô∏è Rate limit detected. Waiting ${delay}s..."
                sleep $delay
                continue
              fi
            elif [ $retry_count -lt $max_retries ]; then
              retry_count=$((retry_count + 1))
              delay=$((base_delay * retry_count))
              echo "‚ö†Ô∏è HTTP $http_code. Waiting ${delay}s before retry..."
              sleep $delay
              continue
            fi
            
            echo "‚ùå Failed after $max_retries retries"
            exit 1
          done


      - name: Report webhook failure to PR
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const qname = '${{ matrix.quickstart.name }}';
            const body = `## ‚ö†Ô∏è Staging Webhook Failed for \`${qname}\`
            
            The staging webhook call failed for quickstart **${qname}** after multiple retry attempts.
            
            ### Next Steps
            - Check the [workflow logs](${context.payload.repository.html_url}/actions/runs/${context.runId}) for details
            - If due to rate limiting, wait a few minutes and re-run the failed job
            - Contact the devrel team if the issue persists
            `;
            
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.payload.pull_request.number,
              body
            });
