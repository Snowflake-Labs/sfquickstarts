
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Apache Airflow, Snowflake 및 dbt를 사용한 데이터 엔지니어링</title>

  
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5Q8R2G');</script>
  

  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-08H27EW14N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-08H27EW14N');
</script>

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5Q8R2G"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  
  <google-codelab-analytics gaid="UA-41491190-9"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="data_engineering_with_apache_airflow_kr"
                  title="Apache Airflow, Snowflake 및 dbt를 사용한 데이터 엔지니어링"
                  environment="web"
                  feedback-link="https://github.com/Snowflake-Labs/sfguides/issues">
    
      <google-codelab-step label="개요" duration="5">
        <p class="image-container"><img alt="architecture" src="img/910e5bbd35f22796.png"></p>
<p>수많은 비즈니스는 민첩성, 성장 및 운영 효율성을 지원할 수 있는 최신 데이터 전략이 탑재된 플랫폼을 고려하고 있습니다. Snowflake는 데이터 클라우드입니다. 이는 인프라 관리와 유지 관리 대신 데이터와 분석에 집중할 수 있도록 모든 비즈니스를 위해 데이터 파이프라인을 간소화할 수 있는 미래 지향적인 솔루션입니다.</p>
<p>Apache Airflow는 데이터 파이프라인을 작성 및 관리하는 데 사용할 수 있는 오픈 소스 워크플로우 관리 플랫폼입니다. Airflow는 작업의 방향성 비순환 그래프(DAG)로 만들어진 워크플로우를 사용합니다.</p>
<p><a href="https://www.getdbt.com/" target="_blank">dbt</a>는 <a href="https://www.getdbt.com/" target="_blank">dbt Labs</a>에서 유지하는 최신 데이터 엔지니어링 프레임워크입니다. Snowflake와 같은 클라우드 데이터 플랫폼을 활용하는 최신 데이터 아키텍처에서 큰 인기를 얻고 있습니다. <a href="https://docs.getdbt.com/dbt-cli/cli-overview" target="_blank">dbt CLI</a>는 dbt 프로젝트를 실행하기 위한 명령줄 인터페이스입니다. CLI는 무료로 사용할 수 있으며 오픈 소스입니다.</p>
<p>이 가상 실습 랩에서는 데이터 변환 작업 스케줄러를 생성하기 위해 Airflow와 dbt를 사용하는 단계별 가이드를 따라 하게 됩니다.</p>
<p>시작하겠습니다.</p>
<h2 is-upgraded>사전 필요 조건 및 지식</h2>
<p>이 가이드는 사용자가 Python 및 dbt에 대한 기본적인 실무 지식을 보유하고 있다고 가정합니다.</p>
<h2 is-upgraded>학습할 내용</h2>
<ul>
<li>데이터 스케줄러를 생성하기 위해 Airflow와 같은 오픈 소스 도구 사용하기</li>
<li>DAG를 작성하고 이를 Airflow에 업로드하기</li>
<li>dbt, Airflow 및 Snowflake를 사용하여 확장 가능한 파이프라인 구축하기</li>
</ul>
<h2 is-upgraded>필요한 것</h2>
<p>시작하기 전에 다음이 필요합니다.</p>
<ol type="1">
<li>Snowflake</li>
<li><strong>Snowflake 계정</strong></li>
<li><strong>적절한 권한을 포함하여 생성된 Snowflake 사용자</strong> 이 사용자에게는 DEMO_DB 데이터베이스에서 객체를 생성하기 위한 권한이 필요합니다.</li>
<li>GitHub</li>
<li><strong>GitHub 계정</strong> GitHub 계정이 아직 없다면 무료로 생성할 수 있습니다. 시작하려면 <a href="https://github.com/join" target="_blank">GitHub 가입</a> 페이지를 방문하십시오.</li>
<li><strong>GitHub 리포지토리</strong> 아직 리포지토리를 생성하지 않았거나 새로운 리포지토리를 생성하고 싶다면, <a href="https://github.com/new" target="_blank">새로운 리포지토리를 생성합니다</a>. 유형으로 <code>Public</code>을 선택합니다(하지만 두 가지 모두 사용할 수 있음). 또한, 지금은 README, .gitignore 및 라이선스 추가를 건너뛸 수 있습니다.</li>
<li>통합 개발 환경(IDE)</li>
<li><strong>Git과 통합되며 선호하는 IDE</strong> Git과 통합되며 선호하는 IDE가 아직 없다면, 훌륭한 무료 오픈 소스 IDE인 <a href="https://code.visualstudio.com/" target="_blank">Visual Studio Code</a>를 추천합니다.</li>
<li><strong>컴퓨터에 프로젝트 리포지토리 복제</strong> Git 리포지토리에 대한 연결 세부 정보를 위해 리포지토리를 열고 페이지 상단 부근에 있는 <code>HTTPS</code> 링크를 복사합니다. 최소 하나의 파일이 리포지토리에 있다면 페이지 상단 부근에 있는 녹색 <code>Code</code> 아이콘을 클릭하고 <code>HTTPS</code> 링크를 복사합니다. 이 링크를 VS Code 또는 선호하는 IDE에서 사용하여 리포지토리를 컴퓨터에 복제합니다.</li>
<li>Docker</li>
<li><strong>노트북에 Docker Desktop 설치</strong>  Airflow를 컨테이너로 실행할 것입니다. <a href="https://docs.docker.com/desktop/" target="_blank">Docker 설정 지침</a>에 따라 운영 체제에 Docker Desktop을 설치하십시오.</li>
</ol>
<h2 is-upgraded>구축할 것</h2>
<ul>
<li>dbt 및 Snowflake와 간단히 작동하는 Airflow 파이프라인</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="환경 설정" duration="2">
        <p>우선, 아래 명령을 실행하여 폴더를 생성하겠습니다.</p>
<pre><code>mkdir dbt_airflow &amp;&amp; cd &#34;$_&#34;
</code></pre>
<p>다음으로 Airflow의 docker-compose 파일을 가져오겠습니다. 이 작업을 수행하기 위해 로컬 노트북에 대해 파일의 curl 명령을 실행하겠습니다.</p>
<pre><code language="language-bash" class="language-bash">curl -LfO &#39;https://airflow.apache.org/docs/apache-airflow/2.3.0/docker-compose.yaml&#39;
</code></pre>
<p>이제 docker-compose 파일을 조정하겠습니다. 2개의 폴더를 볼륨으로 추가합니다. <code>dags</code>는 Airflow가 선택하여 분석할 수 있도록 Airflow DAG가 있는 폴더입니다. <code>dbt</code>는 dbt 모델과 CSV 파일을 구성한 폴더입니다.</p>
<pre><code language="language-bash" class="language-bash">  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./dbt:/dbt # add this in
    - ./dags:/dags # add this in

</code></pre>
<p>이제 추가 docker-compose 매개변수로 추가 파일을 생성해야 합니다. 이렇게 하면 컨테이너가 시작될 때 dbt가 설치됩니다.</p>
<p><code>.env</code></p>
<pre><code language="language-bash" class="language-bash">_PIP_ADDITIONAL_REQUIREMENTS=dbt==0.19.0
</code></pre>
<p>이제 <code>dbt</code> 프로젝트와 <code>dags</code> 폴더를 생성해야 합니다.</p>
<p>dbt 프로젝트에 대해 <code>dbt init dbt</code> 명령을 실행합니다. 추후 4단계에서는 여기에서 dbt를 구성하게 됩니다.</p>
<p>dags 폴더에 대해 다음 작업을 수행해 폴더를 생성합니다.</p>
<pre><code>mkdir dags
</code></pre>
<p>트리 리포지토리는 다음과 같아야 합니다.</p>
<p class="image-container"><img alt="Folderstructure" src="img/526b15af3f7538f.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="dbt 프로젝트 설정" duration="6">
        <p>이제 리포지토리를 설정했으니 dbt 프로젝트를 구성 및 설정하겠습니다.</p>
<p>시작하기 전에 dbt 프로젝트를 위해 어떤 작업을 수행할지 알아보겠습니다.</p>
<p>아래 다이어그램에서 확인할 수 있듯이 <code>bookings_1</code>, <code>bookings_2</code> 및 <code>customers</code> 라는 3개의 CSV 파일이 있습니다. 이러한 CSV 파일을 테이블로 Snowflake에 시드하겠습니다. 이 부분은 추후 4단계에서 자세히 살펴볼 것입니다.</p>
<p>이에 따라 <code>bookings_1</code> 및 <code>bookings_2</code> 테이블을 <code>combined_bookings</code>와 병합하겠습니다. 다음으로 <code>prepped_data</code> 테이블을 만들기 위해 <code>combined_bookings</code> 및 <code>customer</code> 테이블을 customer_id로 결합하겠습니다.</p>
<p>마지막으로 2개의 뷰를 생성하여 <code>prepped_data</code>에 대한 분석 및 변환 작업을 수행하겠습니다.</p>
<ol type="1">
<li><code>hotel_count_by_day.sql</code>: 이렇게 하면 ANALYSIS 스키마에 hotel_count_by_day 뷰가 생성됩니다. 여기에서 일별로 호텔 예약 횟수를 계산하게 됩니다.</li>
<li><code>thirty_day_avg_cost.sql</code>: 이렇게 하면 ANALYSIS 스키마에 thirty_day_avg_cost 뷰가 생성됩니다. 여기에서 지난 30일 동안의 평균 예약 비용을 계산하게 됩니다.</li>
</ol>
<p class="image-container"><img alt="dbt_structure" src="img/d36a140ee3fb4128.png"></p>
<p>우선, Snowflake 콘솔로 이동하여 아래 스크립트를 실행하겠습니다. 이렇게 하면 dbt_user 및 dbt_dev_role이 생성됩니다. 그런 다음 dbt_user를 위한 데이터베이스를 설정합니다.</p>
<pre><code language="language-sql" class="language-sql">USE ROLE SECURITYADMIN;

CREATE OR REPLACE ROLE dbt_DEV_ROLE COMMENT=&#39;dbt_DEV_ROLE&#39;;
GRANT ROLE dbt_DEV_ROLE TO ROLE SYSADMIN;

CREATE OR REPLACE USER dbt_USER PASSWORD=&#39;&lt;PASSWORD&gt;&#39;
	DEFAULT_ROLE=dbt_DEV_ROLE
	DEFAULT_WAREHOUSE=dbt_WH
	COMMENT=&#39;dbt User&#39;;
    
GRANT ROLE dbt_DEV_ROLE TO USER dbt_USER;

-- Grant privileges to role
USE ROLE ACCOUNTADMIN;

GRANT CREATE DATABASE ON ACCOUNT TO ROLE dbt_DEV_ROLE;

/*---------------------------------------------------------------------------
Next we will create a virtual warehouse that will be used
---------------------------------------------------------------------------*/
USE ROLE SYSADMIN;

--Create Warehouse for dbt work
CREATE OR REPLACE WAREHOUSE dbt_DEV_WH
  WITH WAREHOUSE_SIZE = &#39;XSMALL&#39;
  AUTO_SUSPEND = 120
  AUTO_RESUME = true
  INITIALLY_SUSPENDED = TRUE;

GRANT ALL ON WAREHOUSE dbt_DEV_WH TO ROLE dbt_DEV_ROLE;

</code></pre>
<p><code>dbt_user</code>로 로그인하고 명령을 실행하여 <code>DEMO_dbt</code> 데이터베이스를 생성하겠습니다.</p>
<pre><code language="language-sql" class="language-sql">CREATE OR REPLACE DATABASE DEMO_dbt

</code></pre>
<p class="image-container"><img alt="airflow" src="img/bcdcf208927fcb3d.png"></p>
<p>이제 앞서 1단계에서 설정한 <code>dbt_airflow</code> &gt; <code>dbt</code> 프로젝트로 되돌아가겠습니다.</p>
<p>아래에 있는 각 파일의 몇 가지 구성을 설정하겠습니다. <code>dbt_project.yml</code>에 대해서는 모델 섹션만 바꾸면 됩니다.</p>
<p>profiles.yml</p>
<pre><code language="language-yml" class="language-yml">default:
  target: dev
  outputs:
    dev:
      type: snowflake
      ######## Please replace with your Snowflake account name 
      ######## for example sg_demo.ap-southeast-1
      account: &lt;ACCOUNT_URL&gt;.&lt;REGION&gt; 

      user: &#34;&#123;&#123; env_var(&#39;dbt_user&#39;) }}&#34;
      ######## These environment variables dbt_user and dbt_password 
      ######## are read from the variabls in Airflow which we will set later
      password: &#34;&#123;&#123; env_var(&#39;dbt_password&#39;) }}&#34;

      role: dbt_dev_role
      database: demo_dbt
      warehouse: dbt_dev_wh
      schema: public
      threads: 200
</code></pre>
<p>packages.yml</p>
<pre><code language="language-yml" class="language-yml">packages:
  - package: fishtown-analytics/dbt_utils
    version: 0.6.4
</code></pre>
<p>dbt_project.yml</p>
<pre><code language="language-yml" class="language-yml">models:
  my_new_project:
      # Applies to all files under models/example/
      transform:
          schema: transform
          materialized: view
      analysis:
          schema: analysis
          materialized: view
</code></pre>
<p>다음으로 <code>packages.yml</code> 내부에 배치한 <code>fishtown-analytics/dbt_utils</code>를 설치하겠습니다. <code>dbt</code> 폴더에서 <code>dbt deps</code> 명령을 실행하여 설치할 수 있습니다.</p>
<p>이제 <code>macros</code> 폴더에 <code>custom_demo_macros.sql</code>이라는 파일을 생성하고 아래 SQL을 입력하겠습니다.</p>
<pre><code language="language-sql" class="language-sql">{% macro generate_schema_name(custom_schema_name, node) -%}
    {%- set default_schema = target.schema -%}
    {%- if custom_schema_name is none -%}
        &#123;&#123; default_schema }}
    {%- else -%}
        &#123;&#123; custom_schema_name | trim }}
    {%- endif -%}
{%- endmacro %}


{% macro set_query_tag() -%}
  {% set new_query_tag = model.name %} {# always use model name #}
  {% if new_query_tag %}
    {% set original_query_tag = get_current_query_tag() %}
    &#123;&#123; log(&#34;Setting query_tag to &#39;&#34; ~ new_query_tag ~ &#34;&#39;. Will reset to &#39;&#34; ~ original_query_tag ~ &#34;&#39; after materialization.&#34;) }}
    {% do run_query(&#34;alter session set query_tag = &#39;{}&#39;&#34;.format(new_query_tag)) %}
    &#123;&#123; return(original_query_tag)}}
  {% endif %}
  &#123;&#123; return(none)}}
{% endmacro %}
</code></pre>
<p>모든 것이 올바르게 진행되면 폴더는 다음과 같아야 합니다. 주석이 달린 상자는 위에서 진행한 내용입니다.</p>
<p>여기에서 진행할 마지막 단계는 <code>db_utils</code>의 dbt 모듈을 설치하는 것입니다. dbt 디렉터리에서 실행합니다.</p>
<pre><code language="language-\u00a0" class="language-\u00a0">dbt deps
</code></pre>
<p>그러면 <code>dbt_modules</code> 폴더에 관련 모듈이 설치되는 것을 확인할 수 있습니다.</p>
<p>지금쯤 폴더 구조는 다음과 같아야 합니다.</p>
<p class="image-container"><img alt="airflow" src="img/559f995347083ec8.png"></p>
<p>dbt 구성이 완료되었습니다. 다음 섹션에서는 CSV 파일과 DAG를 생성하겠습니다.</p>


      </google-codelab-step>
    
      <google-codelab-step label="dbt에서 CSV 데이터 파일 생성" duration="10">
        <p>이 섹션에서는 샘플 CSV 데이터 파일과 관련 SQL 모델을 준비하겠습니다.</p>
<p>우선 dbt 폴더에 있는 <code>data</code> 폴더에 3개의 Excel 파일을 생성하며 시작하겠습니다.</p>
<p>bookings_1.csv</p>
<pre><code language="language-csv" class="language-csv">id,booking_reference,hotel,booking_date,cost
1,232323231,Pan Pacific,2021-03-19,100
1,232323232,Fullerton,2021-03-20,200
1,232323233,Fullerton,2021-04-20,300
1,232323234,Jackson Square,2021-03-21,400
1,232323235,Mayflower,2021-06-20,500
1,232323236,Suncity,2021-03-19,600
1,232323237,Fullerton,2021-08-20,700
</code></pre>
<p>bookings_2.csv</p>
<pre><code language="language-csv" class="language-csv">id,booking_reference,hotel,booking_date,cost
2,332323231,Fullerton,2021-03-19,100
2,332323232,Jackson Square,2021-03-20,300
2,332323233,Suncity,2021-03-20,300
2,332323234,Jackson Square,2021-03-21,300
2,332323235,Fullerton,2021-06-20,300
2,332323236,Suncity,2021-03-19,300
2,332323237,Berkly,2021-05-20,200
</code></pre>
<p>customers.csv</p>
<pre><code language="language-csv" class="language-csv">id,first_name,last_name,birthdate,membership_no
1,jim,jone,1989-03-19,12334
2,adrian,lee,1990-03-10,12323
</code></pre>
<p>폴더 구조는 다음과 같아야 합니다.</p>
<p class="image-container"><img alt="airflow" src="img/bc601bb6767d64e2.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="models 폴더에서 dbt 모델 생성" duration="2">
        <p>models 폴더에서 <code>analysis</code> 및 <code>transform</code> 폴더 2개를 생성합니다. 분석 및 변환을 위해 아래에 있는 섹션을 각각 따르십시오.</p>
<h2 is-upgraded>transform 폴더를 위한 dbt 모델</h2>
<p><code>transform</code> 폴더에는 3개의 SQL 파일이 있습니다.</p>
<ol type="1">
<li><code>combined_bookings.sql</code>: 이는 위에서 봤던 2개의 예약 CSV 파일을 결합하고 <code>TRANSFORM</code> 스키마에 <code>COMBINED_BOOKINGS</code> 뷰를 생성합니다.</li>
</ol>
<p>combined_bookings.sql</p>
<pre><code language="language-sql" class="language-sql">&#123;&#123; dbt_utils.union_relations(
    relations=[ref(&#39;bookings_1&#39;), ref(&#39;bookings_2&#39;)]
) }}
</code></pre>
<ol type="1" start="2">
<li><code>customer.sql</code>: 이는 <code>TRANSFORM</code> 스키마에 <code>CUSTOMER</code> 뷰를 생성합니다.</li>
</ol>
<p>customer.sql</p>
<pre><code language="language-sql" class="language-sql">SELECT ID 
    , FIRST_NAME
    , LAST_NAME
    , birthdate
FROM &#123;&#123; ref(&#39;customers&#39;) }}
</code></pre>
<ol type="1" start="3">
<li><code>prepped_data.sql</code>: 이는 <code>TRANSFORM</code> 스키마에 <code>PREPPED_DATA</code> 뷰를 생성합니다. 여기에서는 위 단계의 <code>CUSTOMER</code> 및 <code>COMBINED_BOOKINGS</code> 뷰에 대한 내부 조인 작업이 수행됩니다.</li>
</ol>
<p>prepped_data.sql</p>
<pre><code language="language-sql" class="language-sql">SELECT A.ID 
    , FIRST_NAME
    , LAST_NAME
    , birthdate
    , BOOKING_REFERENCE
    , HOTEL
    , BOOKING_DATE
    , COST
FROM &#123;&#123;ref(&#39;customer&#39;)}}  A
JOIN &#123;&#123;ref(&#39;combined_bookings&#39;)}} B
on A.ID = B.ID
</code></pre>
<h2 is-upgraded>analysis 폴더를 위한 dbt 모델</h2>
<p>이제 <code>analysis</code> 폴더로 이동하겠습니다. <code>analysis</code> 폴더로 바꾸고 다음과 같은 2개의 SQL 파일을 생성합니다.</p>
<ol type="1">
<li><code>hotel_count_by_day.sql</code>: 이렇게 하면 <code>ANALYSIS</code> 스키마에 hotel_count_by_day 뷰가 생성됩니다. 여기에서 일별로 호텔 예약 횟수를 계산하게 됩니다.</li>
</ol>
<pre><code language="language-sql" class="language-sql">SELECT
  BOOKING_DATE,
  HOTEL,
  COUNT(ID) as count_bookings
FROM &#123;&#123; ref(&#39;prepped_data&#39;) }}
GROUP BY
  BOOKING_DATE,
  HOTEL
</code></pre>
<ol type="1" start="2">
<li><code>thirty_day_avg_cost.sql</code>: 이렇게 하면 <code>ANALYSIS</code> 스키마에 thirty_day_avg_cost 뷰가 생성됩니다. 여기에서 지난 30일 동안의 평균 예약 비용을 계산하게 됩니다.</li>
</ol>
<pre><code language="language-sql" class="language-sql">SELECT
  BOOKING_DATE,
  HOTEL,
  COST,
  AVG(COST) OVER (
    ORDER BY BOOKING_DATE ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
  ) as &#34;30_DAY_AVG_COST&#34;,
  COST -   AVG(COST) OVER (
    ORDER BY BOOKING_DATE ROWS BETWEEN 29 PRECEDING AND CURRENT ROW
  ) as &#34;DIFF_BTW_ACTUAL_AVG&#34;
FROM &#123;&#123; ref(&#39;prepped_data&#39;) }}
</code></pre>
<p>파일 구조는 다음과 같아야 합니다. 이미 dbt 모델을 완료했으니 Airflow에 대한 작업을 진행할 수 있습니다.</p>
<p class="image-container"><img alt="airflow" src="img/c50fe4445f3c7a09.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Airflow DAG 준비" duration="5">
        <p><code>dags</code> 폴더에서 <code>init.py</code> 및 <code>transform_and_analysis.py</code> 파일 2개를 생성합니다. <code>init.py</code>는 초기화하며 CSV 데이터를 열람합니다. <code>transform_and_analysis.py</code>는 변환 및 분석 작업을 수행합니다.</p>
<p>그런 다음 Airflow를 사용하여 매일 <code>transform_and_analysis</code> DAG를 예약할 수 있습니다. 하지만 이 사례에서는 DAG를 수동으로 트리거하겠습니다.</p>
<p>init.py</p>
<pre><code language="language-python" class="language-python">from datetime import datetime
import os

from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.dummy_operator import DummyOperator

default_args = {
    &#39;owner&#39;: &#39;airflow&#39;,
    &#39;depends_on_past&#39;: False,
    &#39;start_date&#39;: datetime(2020,8,1),
    &#39;retries&#39;: 0
}


with DAG(&#39;1_init_once_seed_data&#39;, default_args=default_args, schedule_interval=&#39;@once&#39;) as dag:
    task_1 = BashOperator(
        task_id=&#39;load_seed_data_once&#39;,
        bash_command=&#39;cd /dbt &amp;&amp; dbt seed --profiles-dir .&#39;,
        env={
            &#39;dbt_user&#39;: &#39;&#123;&#123; var.value.dbt_user }}&#39;,
            &#39;dbt_password&#39;: &#39;&#123;&#123; var.value.dbt_password }}&#39;,
            **os.environ
        },
        dag=dag
    )

task_1  
</code></pre>
<p>transform_and_analysis.py</p>
<pre><code language="language-python" class="language-python">from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.dummy_operator import DummyOperator
from datetime import datetime


default_args = {
    &#39;owner&#39;: &#39;airflow&#39;,
    &#39;depends_on_past&#39;: False,
    &#39;start_date&#39;: datetime(2020,8,1),
    &#39;retries&#39;: 0
}

with DAG(&#39;2_daily_transformation_analysis&#39;, default_args=default_args, schedule_interval=&#39;@once&#39;) as dag:
    task_1 = BashOperator(
        task_id=&#39;daily_transform&#39;,
        bash_command=&#39;cd /dbt &amp;&amp; dbt run --models transform --profiles-dir .&#39;,
        env={
            &#39;dbt_user&#39;: &#39;&#123;&#123; var.value.dbt_user }}&#39;,
            &#39;dbt_password&#39;: &#39;&#123;&#123; var.value.dbt_password }}&#39;,
            **os.environ
        },
        dag=dag
    )

    task_2 = BashOperator(
        task_id=&#39;daily_analysis&#39;,
        bash_command=&#39;cd /dbt &amp;&amp; dbt run --models analysis --profiles-dir .&#39;,
        env={
            &#39;dbt_user&#39;: &#39;&#123;&#123; var.value.dbt_user }}&#39;,
            &#39;dbt_password&#39;: &#39;&#123;&#123; var.value.dbt_password }}&#39;,
            **os.environ
        },
        dag=dag
    )

    task_1 &gt;&gt; task_2 # Define dependencies
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Airflow를 위해 docker-compose 파일 실행" duration="5">
        <p><code>docker-compose up</code>을 실행하고 <a href="http://localhost:8080/" target="_blank">http://localhost:8080/</a>으로 이동하겠습니다. 기본 사용자 이름은 <code>airflow</code>이고 암호는 <code>airflow</code>입니다.</p>
<p class="image-container"><img alt="airflow" src="img/3c5867454e0426d7.png"></p>
<p>이제 2개의 변수를 생성하겠습니다. <code>admin > Variables</code>로 이동하여 <code>+</code> 아이콘을 클릭합니다.</p>
<p class="image-container"><img alt="airflow" src="img/895a8bd6de0ede43.png"></p>
<p>우선 <code>dbt_user</code> 키와 <code>dbt_user</code> 값을 생성하겠습니다.</p>
<p class="image-container"><img alt="airflow" src="img/6ab54f17b7f0c069.png"></p>
<p>이제 두 번째 <code>dbt_password</code> 키와 <code><ADD IN YOUR PASSWORD></code> 값을 생성하겠습니다.</p>
<p class="image-container"><img alt="airflow" src="img/d0b9e76723d5ffd9.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="DAG 활성화 및 실행" duration="0">
        <p>이제 DAG를 활성화하겠습니다. <code>1_init_once_seed_data</code> 및 <code>2_daily_transformation_analysis</code>의 파란색 버튼을 클릭합니다.</p>
<p class="image-container"><img alt="airflow" src="img/f2b16e9ea0323b55.png"></p>
<h2 is-upgraded>1_init_once_seed_data 실행</h2>
<p>이제 데이터를 시드하기 위해 <code>1_init_once_seed_data</code>를 실행하겠습니다. 실행하려면 DAG 오른쪽에 있는 <code>Actions</code> 아래의 재생 아이콘을 클릭합니다.</p>
<p class="image-container"><img alt="airflow" src="img/92a97444078fce7f.png"></p>
<h2 is-upgraded>PUBLIC 스키마에 생성된 테이블에서 시드 데이터 보기</h2>
<p>모든 것이 올바르게 진행되면 Snowflake 인스턴스로 되돌아갔을 때 <code>PUBLIC</code> 스키마에 성공적으로 생성된 트리 테이블이 보여야 합니다.</p>
<p class="image-container"><img alt="airflow" src="img/4d7ba239f2c8ee2b.png"></p>
<h2 is-upgraded>2_daily_transformation_analysis 실행</h2>
<p><code>transform</code> 및 <code>analysis</code> 모델을 실행할 두 번째 <code>2_daily_transformation_analysis</code> DAG를 실행하겠습니다.</p>
<p class="image-container"><img alt="airflow" src="img/c294cb40356b3e3b.png"></p>
<p><code>Transform</code> 및 <code>Analysis</code> 뷰가 성공적으로 생성되었습니다!</p>
<p class="image-container"><img alt="airflow" src="img/549536ac9cffd679.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="결론" duration="1">
        <p>축하합니다! dbt 및 Snowflake로 첫 번째 Apache Airflow를 생성하셨습니다! 자체 샘플이나 프로덕션 데이터를 로드하고 이 랩에서 다루지 않은 더 고급 수준의 Airflow 및 Snowflake 기능을 사용하여 무료 평가판을 계속 사용해 보십시오.</p>
<h2 is-upgraded>추가 리소스:</h2>
<ul>
<li>18,000명 이상의 데이터 실무자가 참여하고 있는 <a href="https://www.getdbt.com/community/" target="_blank">dbt 커뮤니티 Slack</a>에 오늘 참여해 보십시오. Snowflake 관련 콘텐츠 전용 Slack 채널(#db-snowflake)을 운영하고 있습니다.</li>
<li>단순한 <a href="https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html" target="_blank">Airflow DAG</a> 작성 방법에 대한 빠른 자습서</li>
</ul>
<h2 is-upgraded>다룬 내용:</h2>
<ul>
<li>Airflow, dbt 및 Snowflake 설정하기</li>
<li>DAG 생성하기 및 DAG에서 dbt 실행하기</li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/native-shim.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/custom-elements.min.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/prettify.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>
  <script type="text/javascript">
    window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=t.forceSSL||"https:"===document.location.protocol,a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src=(r?"https:":"http:")+"//cdn.heapanalytics.com/js/heap-"+e+".js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(a,n);for(var o=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","removeEventProperty","setEventProperties","track","unsetEventProperty"],c=0;c<p.length;c++)heap[p[c]]=o(p[c])};
    heap.load("2025084205");
  </script>
</body>
</html>
