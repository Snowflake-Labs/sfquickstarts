
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Streamlining Healthcare Claims Change Data Capture Processing Using Amazon DynamoDB and Snowflake OpenFlow</title>

  
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5Q8R2G');</script>
  

  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-08H27EW14N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-08H27EW14N');
</script>

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5Q8R2G"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  
  <google-codelab-analytics gaid="UA-41491190-9"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="Streamline_Healthcare_CDC_DDB_And_Openflow"
                  title="Streamlining Healthcare Claims Change Data Capture Processing Using Amazon DynamoDB and Snowflake OpenFlow"
                  environment="web"
                  feedback-link="https://github.com/Snowflake-Labs/sfguides/issues">
    
      <google-codelab-step label="Overview" duration="10">
        <h2 is-upgraded>Overview</h2>
<p>In a healthcare claims processing system, <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html" target="_blank">Amazon DynamoDB</a> can serve as an effective front-end database due to its high scalability, low-latency performance, and schema flexibility. It allows real-time ingestion and access to diverse, evolving claim event data such as status changes, user interactions, or system-generated events without the constraints of a rigid relational schema. This is especially valuable in front-end applications where speed and adaptability are critical. On the back end, Snowflake complements DynamoDB by handling complex analytics, aggregations, data sharing and reporting. Claim data can be streamed in near real-time from DynamoDB into Snowflake via <a href="https://www.snowflake.com/en/product/features/openflow/" target="_blank">Openflow</a> seamlessly, where advanced SQL queries, joins, and business intelligence tools can be applied to support audits, compliance checks, long-term trends and securely shared with teams across the globe. Together, this architecture balances speed and flexibility at the front with powerful analytics and data integrity at the back.</p>
<p>This guickstart will walk you through how to build CDC pipelines to synchronize the front-end DynamoDB and back-end Snowflake tables for processing real-time insurance claims</p>
<p>Below is a schematic diagram of the demo. The CDC events in DynamoDB are captured in a <a href="https://docs.aws.amazon.com/streams/latest/dev/key-concepts.html" target="_blank">Kinesis data stream</a>, we then use an Openflow connector to consume those events and they are streamed into a CDC table which holds the events yet to be merged with the destination table, while all historical events are loaded into the event history table for logging and tracking purposes. After the CDC events are merged, the source DynamoDB table and the destination Snowflake table are synchronized.</p>
<p class="image-container"><img src="img/74c27fb123d5f746.png"></p>
<h2 is-upgraded>What You Will Learn</h2>
<ul>
<li>How to set up Change Data Capture (CDC) between Amazon DynamoDB and Snowflake</li>
<li>How to configure and use Snowflake Openflow for real-time data synchronization</li>
<li>How to capture and process DynamoDB stream events using Kinesis Data Streams</li>
<li>How to transform semi-structured JSON data for analytics in Openflow</li>
<li>How to implement and monitor real-time CDC operations (INSERT, MODIFY, REMOVE)</li>
<li>Best practices for healthcare claims data processing in a hybrid architecture</li>
</ul>
<h2 is-upgraded>What You will Build</h2>
<ul>
<li>A complete CDC pipeline that synchronizes healthcare insurance claims data between DynamoDB and Snowflake</li>
<li>An Openflow connector that consumes, transforms, and loads CDC events into Snowflake</li>
<li>A set of Snowflake tables for storing and tracking claim events, including: <ul>
<li>A destination table synchronized with the source DynamoDB table</li>
<li>A CDC operations table for staging changes</li>
<li>An event history table for auditing and tracking all data modifications</li>
</ul>
</li>
<li>SQL merge operations that maintain data consistency across both systems</li>
</ul>
<h2 is-upgraded>Prerequisites OR What You Will Need</h2>
<p>Before proceeding with the quickstart, ensure you have:</p>
<ol type="1">
<li><strong>Snowflake and AWS Account Access</strong><ul>
<li>A Snowflake account in one of the AWS commercial <a href="https://docs.snowflake.com/en/user-guide/intro-regions#label-na-general-regions" target="_blank">regions</a>. If you do not have one you can register for a <a href="https://signup.snowflake.com/?utm_cta=quickstart-insurance-claims-dynamodb-openflow-cdc" target="_blank">free trial account</a>.</li>
<li>An AWS account with permissions to create the required resources using <a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html" target="_blank">Cloudformation</a>.</li>
<li>Access to the AWS Management Console and <a href="https://docs.aws.amazon.com/cli/v1/userguide/cli-chap-configure.html" target="_blank">AWS CLI</a> configured with required credentials.</li>
<li>A Snowflake account with permissions to run queries and create <a href="https://www.snowflake.com/en/product/features/openflow/" target="_blank">Openflow</a> objects</li>
</ul>
</li>
<li><strong>Network Infrastructure</strong><ul>
<li>An AWS VPC with at least one subnet that has internet access</li>
</ul>
</li>
<li><strong>Knowledge Requirements</strong><ul>
<li>Basic understanding of AWS services (CloudFormation, DynamoDB, Kinesis Data Streams, EC2)</li>
<li>Familiarity with CloudFormation templates and parameters</li>
<li>Familiarity with Snowflake SQL and Snowsight UI</li>
</ul>
</li>
<li><strong>An Openflow deployment either on AWS BYOC or Snowflake VPC</strong><ul>
<li>Refer to this <a href="https://medium.com/@rahul.reddy.ai/your-step-by-step-practical-guide-to-setting-up-snowflake-openflow-on-aws-byoc-07e5b7be5056" target="_blank">blog</a> to deploy Openflow on AWS BYOC</li>
<li>At the time of writing this quickstart, the managed Openflow deployment running in Snowflake VPC is not yet in public preview. We will update this guide as it becomes available in the near future.</li>
</ul>
</li>
<li><strong>A Snowflake role that can access or create Openflow runtimes</strong><ul>
<li>Refer to this <a href="https://docs.snowflake.com/en/user-guide/data-integration/openflow/setup-openflow" target="_blank">doc</a> for more details.</li>
</ul>
</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Deploy AWS Resources" duration="10">
        <p>Click on this <a href="https://console.aws.amazon.com/cloudformation/home#/stacks/new?stackName=Openflow-DDB-CDC&templateURL=https://snowflake-corp-se-workshop.s3.us-west-1.amazonaws.com/Openflow_Dynamo_CDC/ddb-cft.yaml" target="_blank">link</a> to start a CloudFormation template.</p>
<p>Click <code>Next</code>, in the <code>Specify stack details</code> page, select a <code>SubnetId</code> in the drop-down menu, this subnet will be used to deploy the EC2 instance. Note that this subnet must be a public subnet with egress to the internet. Also specify the <code>VpcId</code> in the drop-down menu, make sure that you select a VPC that includes the subnet from above. Leave the other parameters as default and click <code>Next</code>.</p>
<p class="image-container"><img src="img/a8aa53536887299f.png"></p>
<p>In the <code>Configure stack options</code> page, check the box that says <code>I acknowledge that AWS CloudFormation might create IAM resources.</code> and click <code>Next</code>.</p>
<p>Click <code>Submit</code> button to deploy the resources.</p>
<p>In 5-10 minutes, the Cloudformation template finishes deploying the following AWS resources:</p>
<ol type="1">
<li><strong>DynamoDB Table</strong><ul>
<li>Table name: Configurable (default: <code>InsuranceClaims</code>)</li>
<li>Primary key: <code>claim_id</code> (String)</li>
</ul>
</li>
<li><strong>Kinesis Data Stream</strong><ul>
<li>Stream name: Configurable (default: <code>InsuranceClaimsStream</code>)</li>
<li>Shard count: 1</li>
</ul>
</li>
<li><strong>EC2 Jumphost</strong><ul>
<li>To run a script to stream data into the DynamoDB table</li>
</ul>
</li>
<li><strong>IAM Roles and Policies</strong><ul>
<li>To ensure the created resources have appropriate permissions to carry out necessary tasks</li>
</ul>
</li>
</ol>
<p>Next we are ready to connect to the EC2 instance to create the OpenSSL key pair used for authentication with Snowflake.</p>
<h3 is-upgraded>1. Configure the Linux session for timeout and default shell</h3>
<p>In this step we need to connect to the EC2 instance in order to ingest the real-time data.</p>
<p>Go to the AWS <a href="https://us-west-2.console.aws.amazon.com/systems-manager/home" target="_blank">Systems Manager</a> console in the same region where you set up the EC2 instance, Click <code>Session Manager</code> on the left pane.</p>
<p class="image-container"><img src="img/f1625af9fff73b45.png"></p>
<p>Next, we will set the preferred shell as bash.</p>
<p>Click the <code>Preferences</code> tab. <img src="img/bb9a12f7155b64fc.png"></p>
<p>Click the <code>Edit</code> button. <img src="img/e20b6a8ce135c1d4.png"></p>
<p>Go to <code>General preferences</code> section, type in 60 minutes for idle session timeout value.</p>
<p class="image-container"><img src="img/1b83cece69e1650f.png"></p>
<p>Further scroll down to <code>Linux shell profile</code> section, and type in <code>/bin/bash</code> before clicking <code>Save</code> button.</p>
<p class="image-container"><img src="img/b88863e27392a2de.png"></p>
<h3 is-upgraded>2. Connect to the Linux EC2 instance console</h3>
<p>Now go back to the <code>Session</code> tab and click the <code>Start session</code> button. <img src="img/6a05c2c6089304a0.png"></p>
<p>Now you should see the EC2 instance created by the Cloudformation template under <code>Target instances</code>. Its name should be <code>MyEC2Instance</code>, select it and click <code>Start session</code>.</p>
<p class="image-container"><img src="img/17f73290cfd3f4.png"></p>
<h3 is-upgraded>3. Create a key-pair to be used for authenticating with Snowflake</h3>
<p>Create a key pair in AWS Session Manager console by executing the following commands. You will be prompted to give an encryption password(passphrase), remember this passphrase, you will need it later.</p>
<p>**Note, for better security, it is advised not to use an empty string as the passphrase</p>
<pre><code language="language-commandline" class="language-commandline">cd $HOME
openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8
</code></pre>
<p>See below example screenshot:</p>
<p class="image-container"><img src="img/71780971351526bb.png"></p>
<p>Next we will create a public key by running following commands. You will be prompted to type in the passphrase you used in above step.</p>
<pre><code>openssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub
</code></pre>
<p>see below example screenshot:</p>
<p class="image-container"><img src="img/63a114806cbeec31.png"></p>
<p>Next we will save the public and private key string in a correct format that we can use for configuration later.</p>
<pre><code>grep -v KEY rsa_key.pub | tr -d &#39;\n&#39; | awk &#39;{print $1}&#39; &gt; pub.Key
cat pub.Key
</code></pre>
<p>see below example screenshot:</p>
<p class="image-container"><img src="img/569ba68bc98a143d.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Setup Snowflake" duration="10">
        <h3 is-upgraded>1. Create role, schema and database</h3>
<p>First login to your Snowflake account as a power user with ACCOUNTADMIN role. Then run the following SQL commands in a worksheet to create a database, schema and the role that we will use in the lab.</p>
<pre><code language="language-sql" class="language-sql">-- Type in the username who has the ACCOUNTADMIN role
-- This can be the username you picked when sign up for
-- the Snowflake trial account
SET USER = &lt; username with ACCOUNTADMIN role &gt;
</code></pre>
<pre><code language="language-sql" class="language-sql">-- Set default value for multiple variables
-- For purpose of this workshop, it is recommended to use these defaults during the exercise to avoid errors
-- You should change them after the workshop
SET DB = &#39;CDC_DB&#39;;
SET SCHEMA = &#39;CDC_SCHEMA&#39;;
SET WH = &#39;CDC_WH&#39;;
SET ROLE = &#39;CDC_RL&#39;;

USE ROLE ACCOUNTADMIN;

-- CREATE ROLES
CREATE OR REPLACE ROLE IDENTIFIER($ROLE);

-- CREATE DATABASE AND WAREHOUSE
CREATE DATABASE IF NOT EXISTS IDENTIFIER($DB);
USE IDENTIFIER($DB);
CREATE SCHEMA IF NOT EXISTS IDENTIFIER($SCHEMA);
CREATE OR REPLACE WAREHOUSE IDENTIFIER($WH) WITH WAREHOUSE_SIZE = &#39;SMALL&#39;;

-- GRANTS
GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE IDENTIFIER($ROLE);
GRANT ROLE IDENTIFIER($ROLE) TO USER IDENTIFIER($USER);
GRANT ROLE IDENTIFIER($ROLE) TO ROLE ACCOUNTADMIN;
GRANT OWNERSHIP ON DATABASE IDENTIFIER($DB) TO ROLE IDENTIFIER($ROLE);
GRANT OWNERSHIP ON SCHEMA IDENTIFIER($SCHEMA) TO ROLE IDENTIFIER($ROLE);
GRANT USAGE ON WAREHOUSE IDENTIFIER($WH) TO ROLE IDENTIFIER($ROLE);

-- SET DEFAULTS
ALTER USER IDENTIFIER($USER) SET DEFAULT_ROLE=$ROLE;
ALTER USER IDENTIFIER($USER) SET DEFAULT_WAREHOUSE=$WH;

-- RUN FOLLOWING COMMANDS TO FIND YOUR ACCOUNT IDENTIFIER, WRITE IT DOWN FOR USE LATER
-- IT WILL BE SOMETHING LIKE &lt;organization_name&gt;-&lt;account_name&gt;
-- e.g. ykmxgak-wyb52636

WITH HOSTLIST AS 
(SELECT * FROM TABLE(FLATTEN(INPUT =&gt; PARSE_JSON(SYSTEM$allowlist()))))
SELECT REPLACE(VALUE:host,&#39;.snowflakecomputing.com&#39;,&#39;&#39;) AS ACCOUNT_IDENTIFIER
FROM HOSTLIST
WHERE VALUE:type = &#39;SNOWFLAKE_DEPLOYMENT_REGIONLESS&#39;;
</code></pre>
<p>Please write down the Account Identifier, we will need it later. <img src="img/f5a3bbf6be8a8907.png"></p>
<h3 is-upgraded>2. Set Up Key Pair Authentication</h3>
<p>Next we need to configure the public key for the streaming user to access Snowflake programmatically.</p>
<p>In the Snowflake worksheet, replace <code>< pubKey ></code> with the content of the file <code>/home/ssm-user/pub.Key</code> on the EC2 instance and execute the follow command in Snowflake SQL worksheet.</p>
<pre><code language="language-sql" class="language-sql">USE ROLE ACCOUNTADMIN;

-- Replace the username with the one (with ACCOUNTADMIN role) you used at the beggining in step 1 above
ALTER USER &lt; username &gt; SET RSA_PUBLIC_KEY=&#39;&lt; pubKey &gt;&#39;;
</code></pre>
<h3 is-upgraded>3. Set Up the Snowflake Tables</h3>
<p>Connect to your Snowflake account and create the CDC, event history and destination tables:</p>
<pre><code language="language-sql" class="language-sql">-- Use appropriate role, database, schema, and warehouse
USE ROLE IDENTIFIER($ROLE);
USE DATABASE IDENTIFIER($DB);
USE SCHEMA IDENTIFIER($SCHEMA);
USE WAREHOUSE IDENTIFIER($WH);

-- Create the destination table (to be synchronized with the source DynamoDB table)
CREATE OR REPLACE TABLE openflow_insclaim_dest_tbl (
    eventName STRING,
    eventCreationUnixTime STRING,
    claimId STRING,
    diagnosisCodes STRING,
    dateOfService STRING,
    totalCharge FLOAT,
    procedureDetails STRING,
    memberId STRING,
    insurancePlan STRING,
    patientZip INTEGER,
    patientState STRING,
    patientCity STRING,
    patientStreet STRING,
    patientGender VARCHAR(1),
    patientDOB STRING,
    patientLastName STRING,
    patientPhone STRING,
    patientFirstName STRING,
    patientEmail STRING,
    claimStatus STRING,
    createdTimeStamp timestamp_ntz(9),
    providerName STRING,
    providerNPI STRING,
    providerZip INTEGER,
    providerState STRING,
    providerCity STRING,
    providerStreet STRING,
    billSubmitDate STRING,
    payerName STRING,
    payerId STRING,
    payerContactNumber STRING,
    paymentStatus STRING
);

-- Create a event history table to track the changes being applied at the source
CREATE OR REPLACE TABLE openflow_insClaim_event_hist_tbl LIKE openflow_insClaim_dest_tbl;

-- Create a CDC operation table that holds the changes yet to be merged with the destination table
CREATE OR REPLACE TABLE openflow_insClaim_cdc_tbl like openflow_insClaim_dest_tbl;
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Setup Openflow Connector" duration="0">
        <p>Download the <a href="https://snowflake-corp-se-workshop.s3.us-west-1.amazonaws.com/Openflow_Dynamo_CDC/Openflow-DDB-CDC-connector.json" target="_blank">Openflow connector definition file</a> and save it to your desktop.</p>
<h3 is-upgraded>1. Import the connector definition file</h3>
<p>In Snowsight, switch to the role that has access to Openflow runtimes (described in the <code>Prerequisites OR What You Will Need</code> section above). Go to Openflow by clicking on <code>Openflow</code> in the <code>Data</code> drop-down menu in the Snowsight left pane.</p>
<p class="image-container"><img src="img/acfba5cec9c02d27.png"></p>
<p>Click the <code>Launch Openflow</code> button located at the top-right corner to launch Openflow.</p>
<p>Click <code>View all</code> in the Runtime tile.</p>
<p class="image-container"><img src="img/f73d16c5463e1f8e.png"></p>
<p>Click on the runtime that is assigned to you by your Openflow administrator. <img src="img/387443b240a88493.png"></p>
<p>Drag the processor group icon located at the top to the canvas, release mouse button, then click the browse button to import the json file (Openflow-DDB-CDC-connector.json) you just downloaded. The connector is now imported.</p>
<p class="image-container"><img src="img/2399613c1af1b423.png"></p>
<p class="image-container"><img src="img/7682eef7dbc7ebfb.png"></p>
<h3 is-upgraded>2. Configure parameters for the connector</h3>
<p>Right click anywhere on the connector and select <code>Parameters</code>.</p>
<p class="image-container"><img src="img/73b9e3b33e71b44.png"></p>
<p>There are five parameters you will need to fill in. Leave the others as default. Click on the three dots and <code>Edit</code> to add the values for each of them.</p>
<ul>
<li>AWS KEY ID</li>
<li>AWS SECRET ACCESS KEY</li>
<li>SNOWFLAKE ACCOUNT NAME: This is the Snowflake account identifier you wrote down previously</li>
<li>SNOWFLAKE KEY PASSPHRASE: This is the Snowflake key passphrase you chose when creating the key pair with openssl</li>
<li>SNOWFLAKE PRIVATE KEY: This is the content of the <code>rsa_key.p8</code> file you created in the home directory on the EC2 instance.</li>
<li>SNOWFLAKE USER: This is the username with ACCOUNTADMIN role when you setup Snowflake in previous step</li>
</ul>
<p>Once finished, click <code>Apply</code> and <code>Close</code> to take effect.</p>
<p class="image-container"><img src="img/73f6bce665233aa5.png"></p>
<p>Click <code>Back to process group</code> to return to the main canvas.</p>
<p>Now we are ready to start the Openflow connector. To do so, first right click on the connector and select <code>Enable</code>. Next, right click on the connector again and select <code>Enable all controller services</code>. Finally, right click on the connector and select <code>Start</code>.</p>
<p>Double click anywhere on the connector, you will see all the processors that are used to make up the connector. <img src="img/565da62c123534f8.png"></p>
<p>The pipeline is made up of Extract, Transform, Load and Merge stages:</p>
<ul>
<li>Extract: This is a processor <code>ConsumeKinesisStream</code> that listens to the CDC events in the Kinesis data stream and extract them.</li>
<li>Transform: We use a couple of transformation processors (<a href="https://docs.snowflake.com/en/user-guide/data-integration/openflow/processors/flattenjson" target="_blank"><code>FlattenJson</code></a> and <a href="https://docs.snowflake.com/en/user-guide/data-integration/openflow/processors/jolttransformjson" target="_blank"><code>JoltTransformJSON</code></a>) to transform the raw json into a format that is consumable by the Snowflake tables. The raw json contains multi-level json structure and needs to be flattened and its long key names need to be renamed to shorter, descriptive column names in Snowflake.</li>
<li>Load: The <code>PutSnowpipeStreaming</code> processors in this stage are used to stream the jsons into the CDC and event history tables we created previously.</li>
<li>Merge: At this stage, we use the <code>ExecuteSQLStatement</code> processor to merge the changes held in the CDC table into the destination table to complete the synchronization.</li>
</ul>
<p>Note, we also created the funnels to help us examine contents of the flow files at the Extract, Transformation and Merge stages so you can see the progress and json formats at different stages as the data passing through the pipeline. <img src="img/6dbfb89324772f1d.png"></p>
<p>At this point, we have successfully connected the Openflow connector to the upstream Kinesis data stream.</p>
<p>For a list of all of the available Openflow processors, visit this <a href="https://docs.snowflake.com/en/user-guide/data-integration/openflow/processors/index" target="_blank">doc</a> for more details.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Enable DynamoDB Streams" duration="3">
        <p>Navigate to the <a href="https://console.aws.amazon.com/dynamodbv2/home?#tables" target="_blank">DynamoDB Table Console</a> and click <code>InsuranceClaims</code> table.</p>
<p>Click on the tab that says <code>Exports and streams</code> at the top. <img src="img/8acbbb2d1df25395.png"></p>
<p>Scroll all the way down to the bottom of the page and find the <code>Amazon Kinesis data stream details</code> section, click <code>Turn on</code>. <img src="img/f40f601232a33c7a.png"></p>
<p>Next, you will be prompted to select the destination Kinesis data stream name to capture the CDC events. Go ahead and select <code>KDSInsuranceClaims</code> as the destination stream, click <code>Turn on stream</code>. <img src="img/2d2615fccb5f9758.png"></p>
<p>In a few seconds, the DynamoDB stream is enabled. You will see notification of success at the top of the page.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Change Data Capture (CDC) In Action" duration="15">
        <h2 is-upgraded>Ingest Claims to DynamoDB</h2>
<p>Let&#39;s ingest a few sample insurance claims into the source DynamoDB table. To do this, go back to the EC2 instance console in session manager and issue this command:</p>
<pre><code language="language-command" class="language-command">cd $HOME
python3 ingest_med_claims.py 20
</code></pre>
<p>This will ingest 20 sample claims into the DynamoDB table and because these are new CDC events, so they will be captured by DynamoDB streams and get streamed into the Kinesis data stream followed by the Openflow pipeline.</p>
<p>Let&#39;s navigate to the <a href="https://console.aws.amazon.com/dynamodbv2/home?#tables" target="_blank">DynamoDB Table Console</a> and click on the <code>InsuranceClaims</code> table, then click the <code>Explore table items</code> button located at the top-right corner of the page. You should see 20 items have been inserted to the table. If you don&#39;t see them, just click the <code>Run</code> button to refresh.</p>
<p class="image-container"><img src="img/f8df9ded390c7115.png"></p>
<p>Now, go to the Openflow canvas, you should notice that 20 items are also captured by Openflow and they are merged and landed in the destination table in Snowflake. You can right click anywhere on the canvas and select <code>Refresh</code> to see the number of items in <code>Queue2</code> keeps increasing until it hits 20.</p>
<p class="image-container"><img src="img/96090b77a9d3119b.png"></p>
<p>To verify, go to the worksheet in Snowsight and issue this SQL command:</p>
<pre><code language="language-sql" class="language-sql">-- count and show rows in the destination table
select * from openflow_insclaim_dest_tbl;
select count(*) from openflow_insclaim_dest_tbl;
</code></pre>
<p class="image-container"><img src="img/433ee1e8828e4333.png"></p>
<pre><code language="language-sql" class="language-sql">-- All the Insert events are captured in the event history table
select * from openflow_insclaim_event_hist_tbl where eventname=&#39;INSERT&#39;;
</code></pre>
<h2 is-upgraded>Modify Claim Status in DynamoDB</h2>
<p>In this step, let&#39;s modify the claim status in the source DynamoDB table and see the change synchronized with the destination table in Snowflake thanks to the CDC operations by Openflow.</p>
<p>Go to the DynamoDB console, pick any claim whose status is <code>In Review</code>. <img src="img/378e0f897402f98d.png"></p>
<p>Modify the status from <code>In Review</code> to <code>Rejected</code>, then click <code>Save and close</code>.</p>
<p class="image-container"><img src="img/3561af41e8c8dc08.png"></p>
<p>Because this is a new CDC â€˜MODIFY&#39; event, you will see the number in Queue1 and Queue2 in Openflow increased to 21 now. <img src="img/dfcef3add3f51bd4.png"></p>
<p>To verify, go to the worksheet in Snowsight and issue this SQL command:</p>
<pre><code language="language-sql" class="language-sql">select 
    eventName,
    TO_TIMESTAMP(TO_NUMBER(eventCreationUnixTime) / 1000000) AS eventCreationUTC,
    claimId, 
    claimStatus, 
    paymentStatus, 
    totalCharge, 
    insurancePlan,
    memberId,
    providerName
  from openflow_insclaim_dest_tbl where eventname = &#39;MODIFY&#39;;
</code></pre>
<p>You can see the claim status has been changed from <code>In Review</code> to <code>Rejected</code> in the destination table too, this means the source DynamoDB and destination table are synchronized in real-time! Note that modification time stamp is also captured in the <code>EVENTCREATIONUTC</code> column.</p>
<p>The update is again logged in the event history table.</p>
<pre><code language="language-sql" class="language-sql">select 
    eventName,
    TO_TIMESTAMP(TO_NUMBER(eventCreationUnixTime) / 1000000) AS eventCreationUTC,
    claimId, 
    claimStatus, 
    paymentStatus, 
    totalCharge, 
    insurancePlan,
    memberId,
    providerName
  from openflow_insclaim_event_hist_tbl where eventname = &#39;MODIFY&#39; order by eventCreationUTC;
</code></pre>
<p class="image-container"><img src="img/702239f381b1d220.png"></p>
<h2 is-upgraded>Delete Claims from DynamoDB</h2>
<p>Now let&#39;s delete the claim we just modified by going to the DynamoDB console, check the claim and select <code>Delete items</code> from the <code>Actions</code> drop-down menu and confirm. <img src="img/fbcff0b454e93c00.png"></p>
<p>Similarly, because the claim deletion is a new CDC event, you will see the counters in Queue1 and Queue2 increased to 22 now.</p>
<p class="image-container"><img src="img/d40d8c0146e40353.png"></p>
<p>Now if you issue the following SQL query, the result should be empty because the claim has been deleted from the destination table too.</p>
<pre><code language="language-sql" class="language-sql">SELECT *
FROM openflow_insclaim_dest_tbl
-- get the claim id from the event history table
WHERE claimid IN (
    SELECT claimid
    FROM openflow_insclaim_event_hist_tbl
    WHERE eventname = &#39;MODIFY&#39;
);
</code></pre>
<p>Again, the deletion event is captured in the event history table.</p>
<pre><code language="language-sql" class="language-sql">select 
    eventName,
    TO_TIMESTAMP(TO_NUMBER(eventCreationUnixTime) / 1000000) AS eventCreationUTC,
    claimId, 
    claimStatus, 
    paymentStatus, 
    totalCharge, 
    insurancePlan,
    memberId,
    providerName
  from openflow_insclaim_event_hist_tbl where eventname = &#39;REMOVE&#39; order by eventCreationUTC;
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Transformed Data Across Pipeline" duration="5">
        <p>You can view the transformed json contents in various stages by right-clicking tags in the pipeline, and select <code>List queue</code><img src="img/439f91de83838da9.png"></p>
<p>click on the 3 dots and select <code>Download content</code> and they will be saved to your computer. <img src="img/b34992aa3179331a.png"></p>
<p>Feel free to compare these contents to see how the data is transformed by the <a href="https://docs.snowflake.com/en/user-guide/data-integration/openflow/processors/flattenjson" target="_blank"><code>FlatJson</code></a> and <a href="https://docs.snowflake.com/en/user-guide/data-integration/openflow/processors/jolttransformjson" target="_blank"><code>JoltTransformJson</code></a> processors.</p>


      </google-codelab-step>
    
      <google-codelab-step label="The ExecuSQLCommand Processor" duration="5">
        <p>Let&#39;s take a look at the <code>ExecuteSQLStatement</code> processor and see what SQL query it uses to merge the CDC events.</p>
<p>Right click on the <code>ExecuteSQLStatement</code> processor and select <code>View configuration</code>.</p>
<p class="image-container"><img src="img/709a7098cee852be.png"></p>
<p>Click on the <code>properties</code> tab at the top and click <code>SQL</code> to expand the value, here you will see the SQL queries used to merge the CDC events.</p>
<p class="image-container"><img src="img/3c36e9f2b10e8356.png"></p>
<p>For your reference, below is the SQL command used in the <a href="https://docs.snowflake.com/en/user-guide/data-integration/openflow/processors/executesqlstatement" target="_blank">Openflow ExecuSQLCommand processor</a> to merge the events in the CDC table to destination table in Snowflake.</p>
<pre><code language="language-sql" class="language-sql">BEGIN
-- Create temporary table with all claimIds to be processed
CREATE OR REPLACE TEMPORARY TABLE temp_claim_ids AS
SELECT DISTINCT claimId 
FROM openflow_insclaim_cdc_tbl;

-- Perform the merge operation
MERGE INTO openflow_insclaim_dest_tbl tgt
    USING openflow_insclaim_cdc_tbl src
    ON tgt.claimId = src.claimId

    -- Delete rows
    WHEN MATCHED AND src.eventName = &#39;REMOVE&#39; THEN DELETE

    -- Update existing rows
    WHEN MATCHED AND src.eventName = &#39;MODIFY&#39;
    THEN
        UPDATE SET
            tgt.eventName = src.eventName,
            tgt.eventCreationUnixTime = src.eventCreationUnixTime,
            tgt.claimId = src.claimId,
            tgt.diagnosisCodes = src.diagnosisCodes,
            tgt.dateOfService = src.dateOfService,
            tgt.totalCharge = src.totalCharge,
            tgt.procedureDetails = src.procedureDetails,
            tgt.memberId = src.memberId,
            tgt.insurancePlan = src.insurancePlan,
            tgt.patientZip = src.patientZip,
            tgt.patientState = src.patientState,
            tgt.patientCity = src.patientCity,
            tgt.patientStreet = src.patientStreet,
            tgt.patientGender = src.patientGender,
            tgt.patientDOB = src.patientDOB,
            tgt.patientLastName = src.patientLastName,
            tgt.patientPhone = src.patientPhone,
            tgt.patientFirstName = src.patientFirstName,
            tgt.patientEmail = src.patientEmail,
            tgt.claimStatus = src.claimStatus,
            tgt.createdTimeStamp = src.createdTimeStamp,
            tgt.providerName = src.providerName,
            tgt.providerNPI = src.providerNPI,
            tgt.providerZip = src.providerZip,
            tgt.providerState = src.providerState,
            tgt.providerCity = src.providerCity,
            tgt.providerStreet = src.providerStreet,
            tgt.billSubmitDate = src.billSubmitDate,
            tgt.payerName = src.payerName,
            tgt.payerId = src.payerId,
            tgt.payerContactNumber = src.payerContactNumber,
            tgt.paymentStatus = src.paymentStatus

    -- Insert new rows that don&#39;t exist in target_table
    WHEN NOT MATCHED AND src.eventName = &#39;INSERT&#39; THEN
        INSERT (eventName, eventCreationUnixTime, claimId, diagnosisCodes, dateOfService, totalCharge, procedureDetails, memberId, insurancePlan, patientZip, patientState, patientCity, patientStreet, patientGender, patientDOB, patientLastName, patientPhone, patientFirstName, patientEmail, claimStatus, createdTimeStamp, providerName, providerNPI, providerZip, providerState, providerCity, providerStreet, billSubmitDate, payerName, payerId, payerContactNumber, paymentStatus)
        VALUES (src.eventName, src.eventCreationUnixTime, src.claimId, src.diagnosisCodes, src.dateOfService, src.totalCharge, src.procedureDetails, src.memberId, src.insurancePlan, src.patientZip, src.patientState, src.patientCity, src.patientStreet, src.patientGender, src.patientDOB, src.patientLastName, src.patientPhone, src.patientFirstName, src.patientEmail, src.claimStatus, src.createdTimeStamp, src.providerName, src.providerNPI, src.providerZip, src.providerState, src.providerCity, src.providerStreet, src.billSubmitDate, src.payerName, src.payerId, src.payerContactNumber, src.paymentStatus);

-- Delete processed records in the cdc table from temp table
DELETE FROM openflow_insclaim_cdc_tbl 
WHERE claimId IN (SELECT claimId FROM temp_claim_ids);

END;
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Cleanup" duration="5">
        <p>When you&#39;re done with the demo, you can delete all resources to avoid incurring charges:</p>
<h2 is-upgraded>1. Delete the CloudFormation Stack</h2>
<p>Using the AWS Management Console:</p>
<ol type="1">
<li>Navigate to CloudFormation</li>
<li>Select your stack</li>
<li>Click &#34;Delete&#34;</li>
<li>Confirm the deletion</li>
</ol>
<p>Using the AWS CLI:</p>
<pre><code language="language-bash" class="language-bash">aws cloudformation delete-stack --stack-name Openflow-DDB-CDC
</code></pre>
<h2 is-upgraded>2. Verify Resource Deletion</h2>
<p>Ensure that all resources have been deleted:</p>
<ul>
<li>EC2 instance</li>
<li>DynamoDB table</li>
<li>Kinesis stream</li>
<li>IAM roles and policies</li>
</ul>
<h2 is-upgraded>3. Clean Up Snowflake Resources (Optional)</h2>
<p>If you no longer need the Snowflake tables:</p>
<pre><code language="language-sql" class="language-sql">USE ROLE ACCOUNTADMIN;
-- Drop Snowflake tables
DROP TABLE IF EXISTS openflow_insclaim_dest_tbl;
DROP TABLE IF EXISTS openflow_insclaim_cdc_tbl;
DROP TABLE IF EXISTS openflow_insclaim_event_hist_tbl;
-- Drop schema and database
DROP SCHEMA CDC_SCHEMA;
DROP DATABASE CDC_DB;
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Conclusion And Resources" duration="5">
        <h2 is-upgraded>Overview</h2>
<p>In this quickstart guide, you&#39;ve built a complete Change Data Capture (CDC) pipeline that synchronizes healthcare insurance claims data between Amazon DynamoDB and Snowflake. This architecture leverages DynamoDB&#39;s strengths as a high-performance, scalable front-end database for real-time claims processing, while utilizing Snowflake&#39;s powerful analytics capabilities for complex reporting, data sharing, and long-term trend analysis. The Openflow connector serves as the critical bridge between these systems, ensuring that all changes (inserts, updates, and deletions) are captured and synchronized in near real-time.</p>
<h2 is-upgraded>What You Learned</h2>
<ul>
<li>How to set up and configure DynamoDB streams to capture change events</li>
<li>How to use Kinesis Data Streams as a transport layer for CDC events</li>
<li>How to configure and deploy an Openflow connector for real-time data integration</li>
<li>How to transform complex JSON data structures into analytics-ready formats</li>
<li>How to implement CDC operations (INSERT, MODIFY, REMOVE) with proper tracking</li>
<li>How to use Snowflake&#39;s MERGE operation for efficient data synchronization</li>
<li>How to maintain an audit trail of all data changes using event history tables</li>
<li>Best practices for healthcare data integration across AWS and Snowflake</li>
</ul>
<h2 is-upgraded>Resources</h2>
<ul>
<li><a href="https://docs.snowflake.com/en/user-guide/data-integration/openflow/about" target="_blank">Snowflake Openflow Documentation</a></li>
<li><a href="https://docs.snowflake.com/en/user-guide/data-integration/openflow/processors/index" target="_blank">Snowflake Openflow Processors</a></li>
<li><a href="https://docs.snowflake.com/en/sql-reference/sql/merge" target="_blank">Snowflake MERGE Statement Documentation</a></li>
<li><a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html" target="_blank">Amazon DynamoDB Streams Documentation</a></li>
<li><a href="https://docs.aws.amazon.com/streams/latest/dev/introduction.html" target="_blank">Amazon Kinesis Data Streams Documentation</a></li>
<li><a href="https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/Welcome.html" target="_blank">AWS CloudFormation Documentation</a></li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/native-shim.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/custom-elements.min.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/prettify.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>
  <script type="text/javascript">
    window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=t.forceSSL||"https:"===document.location.protocol,a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src=(r?"https:":"http:")+"//cdn.heapanalytics.com/js/heap-"+e+".js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(a,n);for(var o=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","removeEventProperty","setEventProperties","track","unsetEventProperty"],c=0;c<p.length;c++)heap[p[c]]=o(p[c])};
    heap.load("2025084205");
  </script>
</body>
</html>
