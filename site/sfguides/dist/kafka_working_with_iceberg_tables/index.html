
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Intro to Kafka and Snowflake Managed Iceberg</title>

  
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5Q8R2G');</script>
  

  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-08H27EW14N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-08H27EW14N');
</script>

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5Q8R2G"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  
  <google-codelab-analytics gaid="UA-41491190-9"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="kafka_working_with_iceberg_tables"
                  title="Intro to Kafka and Snowflake Managed Iceberg"
                  environment="web"
                  feedback-link="https://github.com/Snowflake-Labs/sfguides/issues">
    
      <google-codelab-step label="Overview" duration="60">
        <p>This guide provides a comprehensive walkthrough for establishing a local data streaming pipeline from open-source Apache Kafka to Snowflake-managed Iceberg tables. We will configure the Snowflake Kafka Connector to leverage Snowpipe Streaming, enabling efficient, near real-time data ingestion while seamlessly handling schema evolution. This setup facilitates a robust and adaptable data flow, ensuring that changes in your Kafka topics are automatically reflected in your Snowflake-managed Iceberg tables.</p>
<h2 is-upgraded>Prerequisites</h2>
<p>Before proceeding, ensure you have the following:</p>
<ul>
<li>A <a href="https://signup.snowflake.com/?utm_source=google&utm_medium=paidsearch&utm_campaign=ap-in-en-brand-cloud-phrase&utm_content=go-rsa-evg-ss-free-trial&utm_term=c-g-snowflake%20computing-p&_bt=591408722054&_bk=snowflake%20computing&_bm=p&_bn=g&_bg=133380613608&gclsrc=aw.ds&gad_source=1&gclid=EAIaIQobChMI7fjZvc6ljAMVy-gWBR3CORZMEAAYASAAEgLvMvD_BwE" target="_blank">Snowflake account</a></li>
<li>Visual Studio Code (VSCode) installed.</li>
<li>Cloud Provider Amazon S3 or Azure Blob storage to write with the iceberg files.</li>
</ul>
<h2 class="checklist" is-upgraded>What You&#39;ll Learn</h2>
<ul class="checklist">
<li>Local installation and configuration of Apache Kafka on macOS.</li>
<li>Configuration of the Snowflake Kafka Connector for Snowpipe Streaming.</li>
<li>Implementation of data ingestion and schema evolution for Iceberg tables using the Snowflake Kafka Connector.</li>
</ul>
<h2 is-upgraded>What You&#39;ll Build</h2>
<ul>
<li>A streaming pipeline through kafka to Snowflake-managed iceberg table</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Setup" duration="0">
        <ol type="1">
<li>Configure an external volume by following this <a href="https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-external-volume-s3" target="_blank">Document</a> or if you are using Azure Blob you can follow this <a href="https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-external-volume-azure" target="_blank">Document</a></li>
</ol>
<pre><code language="language-commandline" class="language-commandline">Example:

USE ROLE ACCOUNTADMIN;

CREATE OR REPLACE EXTERNAL VOLUME iceberg_external_volume_s3
   STORAGE_LOCATIONS =
      (
         (
            NAME = &#39;my-s3-us-west-2&#39;
            STORAGE_PROVIDER = &#39;S3&#39;
            STORAGE_BASE_URL = &#39;s3://&lt;s3-bucket&gt;/iceberg&#39;
            STORAGE_AWS_ROLE_ARN = &#39;arn:aws:iam::&lt;aws account&gt;:role/icerberg_table_access_role&#39;
            STORAGE_AWS_EXTERNAL_ID = &#39;icerberg_table_access_id&#39;
         )
      );
</code></pre>
<ol type="1" start="2">
<li>Login to snowsight and run the sql statements to create database,schema and Iceberg table:</li>
</ol>
<pre><code language="language-commandline" class="language-commandline">SET PWD = &#39;Test1234567&#39;;
SET USER = &#39;demo_user&#39;;
SET DB = &#39;demo_db&#39;;
SET SCHEMA = &#39;demo_schema&#39;

USE ROLE ACCOUNTADMIN;

-- CREATE USERS
CREATE USER IF NOT EXISTS IDENTIFIER($USER) PASSWORD=$PWD  COMMENT=&#39;STREAMING USER&#39;;

-- GRANTS
GRANT ROLE SYSADMIN TO USER IDENTIFIER($USER);

-- CREATE DATABASE AND SCHEMA
CREATE DATABASE IDENTIFIER($DB);
CREATE SCHEMA IDENTIFIER($SCHEMA);

-- CREATE ICEBERG TABLE
USE DATABASE IDENTIFIER($DB);
USE SCHEMA IDENTIFIER($SCHEMA);

CREATE OR REPLACE ICEBERG TABLE emp_iceberg_tab (
    record_metadata OBJECT()
  )
  EXTERNAL_VOLUME = &#39;iceberg_external_volume_s3&#39;
  CATALOG = &#39;SNOWFLAKE&#39;
  BASE_LOCATION = &#39;emp_iceberg_tab&#39;;
</code></pre>
<ol type="1" start="3">
<li>Enable schema evolution on the table</li>
</ol>
<p>Snowflake enables seamless handling of evolving semi-structured data. As data sources add new columns, Snowflake automatically updates table structures to reflect these changes, including the addition of new columns. This eliminates the need for manual schema adjustments. More Info <a href="https://docs.snowflake.com/en/user-guide/data-load-schema-evolution" target="_blank">Document</a></p>
<pre><code language="language-commandline" class="language-commandline">alter ICEBERG table emp_iceberg_tab set ENABLE_SCHEMA_EVOLUTION  =true;
</code></pre>
<ol type="1" start="4">
<li>Create a key-pair to be used for authenticating(We will not used username and password) with Snowflake user by following <a href="https://docs.snowflake.com/en/user-guide/key-pair-auth" target="_blank">Document</a></li>
</ol>
<p>To generate an encrypted version, use the following command, which omits -nocrypt:</p>
<pre><code language="language-commandline" class="language-commandline">openssl genrsa 2048 | openssl pkcs8 -topk8 -v2 des3 -inform PEM -out rsa_key.p8
</code></pre>
<p>The commands generate a private key in PEM format.</p>
<pre><code language="language-commandline" class="language-commandline">-----BEGIN ENCRYPTED PRIVATE KEY-----
MIIE6T...
-----END ENCRYPTED PRIVATE KEY-----
</code></pre>
<p>Generate a public key</p>
<pre><code language="language-commandline" class="language-commandline">openssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub
</code></pre>
<p>The command generates the public key in PEM format.</p>
<pre><code language="language-commandline" class="language-commandline">-----BEGIN PUBLIC KEY-----
MIIBIj...
-----END PUBLIC KEY-----
</code></pre>
<p>Assign the public key to a Snowflake user</p>
<pre><code language="language-commandline" class="language-commandline">use role accountadmin;
alter user demo_user set rsa_public_key=&#39;&lt; pubKey &gt;&#39;;
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Setup Local Kafka" duration="0">
        <ol type="1">
<li>Download kafka in your local mac from <a href="https://kafka.apache.org/downloads" target="_blank">here</a>. This tutorial used kafka version 2.8.1.</li>
<li>Start zookeeper in new terminal</li>
</ol>
<pre><code language="language-commandline" class="language-commandline">cd kafka_2.13-2.8.1/bin
./zookeeper-server-start.sh ../config/zookeeper.properties
</code></pre>
<p class="image-container"><img src="img/c2fc31e66fee064a.png"></p>
<ol type="1" start="3">
<li>Start Kafka server in new terminal</li>
</ol>
<pre><code language="language-commandline" class="language-commandline">cd kafka_2.13-2.8.1/bin
./kafka-server-start.sh ../config/server.properties
</code></pre>
<p class="image-container"><img src="img/5a0af6fa91d9a80f.png"></p>
<ol type="1" start="4">
<li>Download snowflake-kafka-connector from <a href="https://mvnrepository.com/artifact/com.snowflake/snowflake-kafka-connector/3.1.0" target="_blank">here</a></li>
</ol>
<p class="image-container"><img src="img/5a0af6fa91d9a80f.png"></p>
<ul>
<li>Copy kafka connector jar file into kafka_2.13-2.8.1/libs/ folder</li>
</ul>
<ol type="1" start="5">
<li>Create kafka topic</li>
</ol>
<pre><code language="language-commandline" class="language-commandline">cd kafka_2.13-2.8.1/bin
./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 2 --topic demo_topic
</code></pre>
<ol type="1" start="6">
<li>Run kafka producer in console mode and produce some records</li>
</ol>
<pre><code language="language-commandline" class="language-commandline">cd kafka_2.13-2.8.1/bin
./kafka-console-producer.sh --bootstrap-server localhost:9092 --topic demo_topic
</code></pre>
<ul>
<li>Sample Records</li>
</ul>
<pre><code language="language-commandline" class="language-commandline">Sample records:
{&#34;emp_id&#34;:100,&#34;first_name&#34;:&#34;Keshav&#34;,&#34;last_name&#34;:&#34;Lodhi&#34;,&#34;designation&#34;:&#34;DataEngineer&#34;}
{&#34;emp_id&#34;:101,&#34;first_name&#34;:&#34;Ashish&#34;,&#34;last_name&#34;:&#34;kumar&#34;,&#34;designation&#34;:&#34;Solution Architect&#34;}
{&#34;emp_id&#34;:102,&#34;first_name&#34;:&#34;Anup&#34;,&#34;last_name&#34;:&#34;moncy&#34;,&#34;designation&#34;:&#34;Solution Architect&#34;}
</code></pre>
<ol type="1" start="7">
<li>validate records produced in previous step</li>
</ol>
<pre><code language="language-commandline" class="language-commandline">cd kafka_2.13-2.8.1/bin
./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic demo_topic --from-beginning
</code></pre>
<ul>
<li>You should be able to see all 3 records from above output.</li>
</ul>
<ol type="1" start="8">
<li>Run snowflake kafka connector in distributed mode in new terminal</li>
</ol>
<ul>
<li>Run this command to start kafka connector on your machine</li>
</ul>
<pre><code language="language-commandline" class="language-commandline">cd kafka_2.13-2.8.1/bin
./connect-distributed.sh &lt;full_path&gt;/kafka_2.13-2.8.1/config/connect-distributed.properties
</code></pre>
<ol type="1" start="9">
<li>create configuration for connector</li>
</ol>
<pre><code language="language-commandline" class="language-commandline">cd kafka_2.13-2.8.1/config/

vi SF_connect1.json

{
    &#34;name&#34;:&#34;demoiceberg&#34;,
    &#34;config&#34;:{
    &#34;snowflake.ingestion.method&#34;:&#34;SNOWPIPE_STREAMING&#34;,
    &#34;snowflake.streaming.iceberg.enabled&#34;:true,
    &#34;snowflake.enable.schematization&#34;:true,
    &#34;snowflake.role.name&#34;:&#34;sysadmin&#34;,
    &#34;key.converter&#34;:&#34;org.apache.kafka.connect.json.JsonConverter&#34;,
    &#34;value.converter&#34;:&#34;org.apache.kafka.connect.json.JsonConverter&#34;,
    &#34;key.converter.schemas.enable&#34;:false,
    &#34;value.converter.schemas.enable&#34;:false,
    &#34;connector.class&#34;:&#34;com.snowflake.kafka.connector.SnowflakeSinkConnector&#34;,
    &#34;tasks.max&#34;:&#34;8&#34;,
    &#34;topics&#34;:&#34;demo_topic&#34;,
    &#34;snowflake.topic2table.map&#34;:&#34; demo_topic:emp_iceberg_tab&#34;,
    &#34;buffer.count.records&#34;:&#34;10000&#34;,
    &#34;buffer.flush.time&#34;:&#34;60&#34;,
    &#34;buffer.size.bytes&#34;:&#34;5000000&#34;,
    &#34;snowflake.url.name&#34;:&#34;https://xxxxx-nrb47395.snowflakecomputing.com&#34;,
    &#34;snowflake.user.name&#34;:&#34;demo_user&#34;,
    &#34;snowflake.database.name&#34;:&#34;demo_db&#34;,
    &#34;snowflake.schema.name&#34;:&#34;demo_schema&#34;,
    &#34;snowflake.private.key&#34;:&#34;MIIFDjBABgkqh****&#34;,
    &#34;snowflake.private.key.passphrase&#34;:&#34;***&#34;
    }
}
</code></pre>
<ul>
<li>Update the following configuration values based on your setup:</li>
</ul>
<pre><code>snowflake.role.name → You can set this to SYSADMIN for demo purposes.
topics → This should be the Kafka topic name created in Step 5.
snowflake.topic2table.map → Map your Kafka topic name to the Iceberg table name created earlier.
snowflake.url.name → Enter the URL of your Snowflake account.
snowflake.user.name → Specify your Snowflake username.
snowflake.database.name → Use the database name created in the &#34;Create Snowflake Managed Iceberg Table&#34; step.
snowflake.schema.name → Use the schema name created in the &#34;Create Snowflake Managed Iceberg Table&#34; step.
snowflake.private.key → Copy the content of the private key generated in the &#34;Create Snowflake Managed Iceberg Table&#34; step.
snowflake.private.key.passphrase → Enter the passphrase for the encrypted private key file created in the same step.
</code></pre>
<ol type="1" start="10">
<li>Execute configuration</li>
</ol>
<pre><code language="language-commandline" class="language-commandline">curl -X POST -H &#34;Content-Type: application/json&#34; --data @&lt;full_path&gt;kafka_2.13-2.8.1/config/SF_connect1.json http://localhost:8083/connectors
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Schema evolution and Validate data" duration="0">
        <ol type="1">
<li>Check iceberg table by login into snowsight</li>
</ol>
<pre><code language="language-commandline" class="language-commandline">select * from emp_iceberg_tab;
</code></pre>
<p class="image-container"><img src="img/9518f4323460a8fa.png"></p>
<ul>
<li>Now you will see 4 columns detected by the connector</li>
</ul>
<pre><code>- EMP_ID	NUMBER(19,0)
- DESIGNATION	VARCHAR(16777216)
- LAST_NAME	VARCHAR(16777216)
- FIRST_NAME	VARCHAR(16777216)
</code></pre>
<ol type="1" start="2">
<li>Producer few more record in kafka topic</li>
</ol>
<pre><code>{&#34;emp_id&#34;:102,&#34;first_name&#34;:&#34;Anup&#34;,&#34;last_name&#34;:&#34;moncy&#34;,&#34;designation&#34;:&#34;Solution Architect&#34;,&#34;company&#34;:&#34;Snowflake&#34;}
</code></pre>
<ol type="1" start="3">
<li>Validate the data and schema of Iceberg table</li>
</ol>
<pre><code>select * from emp_iceberg_tab;
</code></pre>
<p class="image-container"><img src="img/9c959586f3eeb2cb.png"></p>
<ul>
<li>Now you will see 5 columns detected by the connector</li>
</ul>
<pre><code>EMP_ID	NUMBER(19,0)
DESIGNATION	VARCHAR(16777216)
LAST_NAME	VARCHAR(16777216)
FIRST_NAME	VARCHAR(16777216)
COMPANY	VARCHAR(16777216)
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Conclusion And Resources" duration="0">
        <h2 is-upgraded>What You Learned</h2>
<ul>
<li>Local installation and configuration of Apache Kafka.</li>
<li>Configuration of the Snowflake Kafka Connector for Snowpipe Streaming in distributed mode.</li>
<li>Implementation of data ingestion and schema evolution for Iceberg tables using the Snowflake Kafka Connector.</li>
</ul>
<h2 is-upgraded>Related Resources</h2>
<ul>
<li><a href="https://docs.snowflake.com/en/user-guide/tables-iceberg" target="_blank">Apache Iceberg™ tables | Snowflake Documentation</a></li>
<li><a href="https://docs.snowflake.com/en/user-guide/key-pair-auth" target="_blank">Configuring key-pair authentication | Snowflake Documentation</a></li>
<li><a href="https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-kafka" target="_blank">Using Snowflake Connector for Kafka with Snowpipe Streaming | Snowflake Documentation</a></li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/native-shim.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/custom-elements.min.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/prettify.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>
  <script type="text/javascript">
    window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=t.forceSSL||"https:"===document.location.protocol,a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src=(r?"https:":"http:")+"//cdn.heapanalytics.com/js/heap-"+e+".js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(a,n);for(var o=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","removeEventProperty","setEventProperties","track","unsetEventProperty"],c=0;c<p.length;c++)heap[p[c]]=o(p[c])};
    heap.load("2025084205");
  </script>
</body>
</html>
