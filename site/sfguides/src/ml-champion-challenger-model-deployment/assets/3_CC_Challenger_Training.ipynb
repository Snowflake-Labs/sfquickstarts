{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "qv7daadcqsp6i6lv67es",
   "authorId": "1282256439461",
   "authorName": "ADMIN",
   "authorEmail": "sheena.nasim@snowflake.com",
   "sessionId": "e585f0c1-4494-495a-b243-38491a17e9ef",
   "lastEditTime": 1758974096293
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Imports"
   },
   "source": "# Import python packages\nfrom sklearn import pipeline, preprocessing, ensemble, metrics\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.registry import Registry\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "Set_up"
   },
   "source": "# Set up the database, schema and model registry\ndatabase_name='DEV_AUTOMATION_DEMO'\nschema_name='CHAMPION_CHALLENGER'\nsession.use_database(database_name)\nsession.use_schema(schema_name)    \n# Initialize registry\nregistry = Registry(session=session, \n                           database_name=database_name, \n                           schema_name=schema_name)  \n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "41bac1d5-0826-48aa-9509-dd807437f925",
   "metadata": {
    "name": "Data_info",
    "collapsed": false
   },
   "source": "# Load the main dataset\nThis dataset was generated in the initial script and data was saved in the table named \"Full_Data\""
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "Load_data"
   },
   "source": "#Load and convert to pandas dataframe\nfull_dataset = session.table('full_data').to_pandas()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "256bf779-abb5-4b01-b035-a6a63096844d",
   "metadata": {
    "name": "Data_split",
    "collapsed": false
   },
   "source": "# Time-Based Data Splits\n```\nWeeks 3-12:  Training Data (Challenger)\nWeeks 13-15: Test Data \nWeeks 16-19:   Evaluation Data (Hold-out for comparison)"
  },
  {
   "cell_type": "code",
   "id": "32e68935-ba07-48cc-a208-401cdfd0666a",
   "metadata": {
    "language": "python",
    "name": "Generate_train_data"
   },
   "outputs": [],
   "source": "def get_data(full_dataset, retrain_week_start = 3):\n    \n    # Use recent training data + some historical data for stability\n    # BUT ensure challenger doesn't train on test data (avoid data leakage)\n    historical_weeks = 10\n    test_start_weeks = historical_weeks + retrain_week_start\n    \n    # Challenger trains from week 3 up to week 12 (before test data starts)\n    challenger_train_mask = ((full_dataset['week_number'] >= retrain_week_start) & \n                       (full_dataset['week_number'] < retrain_week_start + historical_weeks))     \n    challenger_train_data = full_dataset[challenger_train_mask].copy()\n    print(f\"   ğŸ“Š Challenger training on {len(challenger_train_data):,} samples\")\n    print(f\"   ğŸ“… Data from weeks {challenger_train_data['week_number'].min()}-{challenger_train_data['week_number'].max()}\")\n        \n    #Three months of test data from Weeek 13 to 15\n    challenger_test_mask = ((full_dataset['week_number'] >= test_start_weeks) & \n                       (full_dataset['week_number'] < test_start_weeks + 3))\n    challenger_test_data = full_dataset[challenger_test_mask].copy()\n    print(f\"   ğŸ“Š Challenger testing on {len(challenger_test_data):,} samples\")\n    print(f\"   ğŸ“… Data from weeks {challenger_test_data['week_number'].min()}-{challenger_test_data['week_number'].max()}\")\n        \n    return challenger_train_data, challenger_test_data\n       \n    ",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a2c0915d-fa17-433a-82c1-614da739c0ce",
   "metadata": {
    "name": "Train_info"
   },
   "source": "### Train challenger model using recent data (simulates weekly retraining)\n"
  },
  {
   "cell_type": "code",
   "id": "8206bc56-c304-4715-965c-17a1d5b0bad5",
   "metadata": {
    "language": "python",
    "name": "Train_challenger"
   },
   "outputs": [],
   "source": "def train_challenger_model(challenger_train_data, challenger_test_data):\n        \n        print(f\"ğŸ¥Š Training Challenger Model...\")\n    \n        # Feature columns (exclude date, week, and target)\n        feature_cols = [col for col in challenger_train_data.columns \n                       if col not in ['application_date', 'week_number', 'approved']]\n\n        X_challenger = challenger_train_data[feature_cols]\n        y_challenger = challenger_train_data['approved']\n        \n        # Create challenger pipeline (could have different hyperparameters)\n        challenger_pipeline = pipeline.Pipeline([\n            ('scaler', preprocessing.StandardScaler()),\n            ('classifier', ensemble.RandomForestClassifier(\n                n_estimators=120,  # Slightly different config\n                random_state=42,\n                max_depth=12,\n                min_samples_split=3,\n                class_weight='balanced_subsample'  # Better for recent data\n            ))\n        ])\n        \n        # Train challenger\n        challenger_pipeline.fit(X_challenger, y_challenger)\n        \n        # Evaluate challenger on the test set\n        X_test = challenger_test_data[feature_cols]\n        y_test = challenger_test_data['approved']\n        \n        challenger_pred_proba = challenger_pipeline.predict_proba(X_test)[:, 1]\n        challenger_auc = round(metrics.roc_auc_score(y_test, challenger_pred_proba) * 100.0, 2)\n        \n        print(f\"   âœ… Challenger trained successfully\")\n        print(f\"   ğŸ“ˆ Test AUC: {challenger_auc:.2f}\")\n        \n        # Register challenger in model registry\n        sample_input = X_challenger.head(100)\n        model_name=\"CREDIT_APPROVAL\"\n\n        challenger_ref = registry.log_model(\n            model = challenger_pipeline,\n            model_name = model_name,\n            sample_input_data = sample_input,\n            target_platforms = [\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],\n            comment = f\"Challenger model retrained on recent data, AUC: {challenger_auc:.4f}\",\n            metrics = {\n                \"test_auc\": challenger_auc,\n                \"train_weeks\": f\"{challenger_train_data['week_number'].min()}-{challenger_train_data['week_number'].max()}\",\n                \"model_type\": \"challenger\",\n                \"training_samples\": len(X_challenger)\n            }\n        )\n\n        try:\n            challenger_ref.unset_alias(\"CHALLENGER\")\n        except:\n            pass\n        # Set as CHALLENGER alias\n        challenger_ref.set_alias(\"CHALLENGER\")\n        model = registry.get_model(model_name)\n        model.set_tag(\"CHALLENGER_VERSION\", challenger_ref.version_name)\n        \n        print(f\"   ğŸ·ï¸ Challenger registered: {challenger_ref.version_name}\")\n        print(f\"   ğŸ·ï¸ Aliases: CHALLENGER\")\n    \n        return ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9912f9aa-0629-4172-9021-f1c56ab28615",
   "metadata": {
    "language": "python",
    "name": "Train_challenger_func_call"
   },
   "outputs": [],
   "source": "#Get the train and test data split for Challenger\ntrain_data,test_data = get_data(full_dataset)\n#Train the Challenger model and push into Snowflake model registry\ntrain_challenger_model(train_data, test_data)",
   "execution_count": null
  }
 ]
}