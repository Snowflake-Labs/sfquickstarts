{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "z4kscyx6bjnos7rukldl",
   "authorId": "1282256439461",
   "authorName": "ADMIN",
   "authorEmail": "sheena.nasim@snowflake.com",
   "sessionId": "631a7705-dfd9-4e02-be0c-75fc852bc745",
   "lastEditTime": 1758946993625
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd9d5a01-32d2-440e-8ae5-0c81ba9b7948",
   "metadata": {
    "name": "Intro",
    "collapsed": false
   },
   "source": "# Champion-Challenger MLOps Framework for Snowflake\n\nThis repository implements a complete **Champion-Challenger model deployment strategy** using Snowflake ML Registry. It's designed for production-grade machine learning operations with automated model retraining, evaluation, and promotion.\n\n## ðŸŽ¯ What is Champion-Challenger Modeling?\n\nChampion-Challenger is a production MLOps pattern where:\n- **Champion**: The current production model serving predictions\n- **Challenger**: A newly trained model that competes with the champion\n- **Evaluation**: Both models are tested on the same holdout dataset\n- **Promotion**: If the challenger performs significantly better, it becomes the new champion"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Imports"
   },
   "source": "# Import python packages\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom snowflake.snowpark.context import get_active_session\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "Set_up"
   },
   "source": "database_name='DEV_AUTOMATION_DEMO'\nschema_name='CHAMPION_CHALLENGER'\ndef setup_environment(database_name,schema_name):\n    \"\"\"Set up Snowflake database, schema and model registry\"\"\"\n    print(\"ðŸš€ Setting up Champion-Challenger Environment...\")\n    \n    # Create database and schema\n    session.sql(f\"CREATE DATABASE IF NOT EXISTS {database_name}\").collect()\n    session.sql(f\"CREATE SCHEMA IF NOT EXISTS {database_name}.{schema_name}\").collect()\n    session.use_database(database_name)\n    session.use_schema(schema_name)\n\n    # Create two tags for tracking live and challenger version names of models.\n    session.sql(\"CREATE TAG IF NOT EXISTS live_version COMMENT = 'live version identification tag'\")\n    session.sql(\"CREATE TAG IF NOT EXISTS Challenger_version COMMENT = 'Challenger version identification tag'\")\n\n    print(f\"âœ… Environment ready: {database_name}.{schema_name}\")\n    \nsetup_environment(database_name, schema_name)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "063193be-bd57-45b8-abe7-2a95e133f25e",
   "metadata": {
    "name": "Synthetic_dataset",
    "collapsed": false
   },
   "source": "## ðŸ“Š Generate Synthetic Data Timeline (20 weeks total)\nCreate a synthetic credit approval dataset with 8,000 loan applications spanning 20 weeks (400 applications per week). Each application contains 9 realistic financial features: applicant age, annual income, credit score, debt-to-income ratio, employment years, credit cards count, mortgage status, education score, and location risk. The target variable indicates loan approval (approved/denied) based on weighted business logic combining creditworthiness factors.\n\n```\nTimeline: Week 0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Week 19\n\nDataset: [====================FULL DATASET====================]\n         Week 0                                           Week 19\n         Jan 1                                           May 15\n```"
  },
  {
   "cell_type": "code",
   "id": "fd10c34d-c9d4-4d34-a800-77f60eac04b0",
   "metadata": {
    "language": "python",
    "name": "Func_to_generate_data"
   },
   "outputs": [],
   "source": "def create_temporal_dataset(n_weeks=20, samples_per_week=400):\n        \"\"\"\n        Create a time-based dataset simulating real business data arrival\n        This mimics credit applications arriving weekly with concept drift\n        \"\"\"\n        print(f\"ðŸ“Š Creating temporal dataset: {n_weeks} weeks, {samples_per_week} samples/week\")\n        \n        np.random.seed(42)\n        \n        all_data = []\n        base_date = datetime(2024, 1, 1)\n        \n        for week in range(n_weeks):\n            week_date = base_date + timedelta(weeks=week)\n            \n            # Simulate concept drift: fraud patterns change over time\n            drift_factor = 1 + (week * 0.02)  # 2% change per week\n            seasonal_factor = 1 + 0.3 * np.sin(2 * np.pi * week / 52)  # Annual seasonality\n            \n            # Generate weekly data\n            week_data = {\n                'application_date': [week_date + timedelta(days=np.random.randint(0, 7)) \n                                   for _ in range(samples_per_week)],\n                'week_number': [week] * samples_per_week,\n                'applicant_age': np.random.uniform(18, 80, samples_per_week),\n                'annual_income': np.random.uniform(20000, 200000, samples_per_week) * seasonal_factor,\n                'credit_score': np.random.uniform(300, 850, samples_per_week),\n                'debt_to_income': np.random.uniform(0, 1, samples_per_week) * drift_factor,\n                'years_employed': np.random.uniform(0, 40, samples_per_week),\n                'num_credit_cards': np.random.randint(0, 8, samples_per_week),\n                'has_mortgage': np.random.choice([0, 1], samples_per_week, p=[0.6, 0.4]),\n                'education_score': np.random.uniform(1, 5, samples_per_week),\n                'location_risk_score': np.random.uniform(0.1, 1.0, samples_per_week) * drift_factor,\n            }\n            \n            # Create realistic target with temporal dependencies\n            approval_prob = (\n                (week_data['credit_score'] - 300) / 550 * 0.3 +\n                (week_data['annual_income'] - 20000) / 180000 * 0.25 +\n                (1 - week_data['debt_to_income']) * 0.2 +\n                (week_data['years_employed'] / 40) * 0.15 +\n                (1 - week_data['location_risk_score']) * 0.1 +\n                np.random.normal(0, 0.05, samples_per_week)\n            )\n            \n            approval_prob = np.clip(approval_prob, 0, 1)\n            week_data['approved'] = np.random.binomial(1, approval_prob)\n            \n            week_df = pd.DataFrame(week_data)\n            all_data.append(week_df)\n        \n        full_dataset = pd.concat(all_data, ignore_index=True)\n        full_dataset = full_dataset.sort_values('application_date').reset_index(drop=True)\n        \n        print(f\"âœ… Dataset created: {len(full_dataset):,} total applications\")\n        print(f\"   ðŸ“… Date range: {full_dataset['application_date'].min()} to {full_dataset['application_date'].max()}\")\n        print(f\"   ðŸ“ˆ Approval rate: {full_dataset['approved'].mean():.1%}\")\n        \n        return full_dataset",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d77a08b-46e0-49ea-bb37-b07cf7b06a77",
   "metadata": {
    "language": "python",
    "name": "Generate_data"
   },
   "outputs": [],
   "source": "full_data = create_temporal_dataset()\n#Show a sample of data\nfull_data.head(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11357078-ce2c-4b2f-aed5-2df0867dd2cb",
   "metadata": {
    "language": "python",
    "name": "Save_to_table"
   },
   "outputs": [],
   "source": "#Convert pandas DataFrame to Snowpark DataFrame and save as table\nfull_data_snowpark_df = session.create_dataframe(full_data)\nfull_data_snowpark_df.write.mode(\"overwrite\").save_as_table('full_data')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "446f4bbf-abf0-49ed-b1b1-b712f6907811",
   "metadata": {
    "name": "Eval_data_intro",
    "collapsed": false
   },
   "source": "# Evaluation dataset\nReserve the week 16-19 data as hold out test. This will be used later to decide new model (Challenger) performs better than existing Champion model."
  },
  {
   "cell_type": "code",
   "id": "77012223-fc89-4589-af89-47c028869808",
   "metadata": {
    "language": "python",
    "name": "Eval_dataset"
   },
   "outputs": [],
   "source": "# Evaluation data (weeks 16-19) - this is our \"future\" evaluation set\nevaluation_week = 16\nevaluation_mask = full_data['week_number'] >= evaluation_week\nevaluation_data = full_data[evaluation_mask].copy()\n\nprint(f\"âœ… Dataset created: {len(evaluation_data):,} total applications\")\nprint(f\"   ðŸ“… Date range: {evaluation_data['application_date'].min()} to {evaluation_data['application_date'].max()}\")\nprint(f\"   ðŸ“ˆ Approval rate: {evaluation_data['approved'].mean():.1%}\")\n\n#Convert pandas DataFrame to Snowpark DataFrame and save as table\neval_data_snowpark_df = session.create_dataframe(evaluation_data)\neval_data_snowpark_df.write.mode(\"overwrite\").save_as_table('evaluation_data')",
   "execution_count": null
  }
 ]
}