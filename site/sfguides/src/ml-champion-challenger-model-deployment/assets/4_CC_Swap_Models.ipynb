{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "7ipa4gg7qgn4nlu4m45x",
   "authorId": "1282256439461",
   "authorName": "ADMIN",
   "authorEmail": "sheena.nasim@snowflake.com",
   "sessionId": "72ae6c48-7ef2-4260-a280-4cc4e96095f0",
   "lastEditTime": 1758974798726
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5670b2b5-7614-4d62-a65b-74e4ab1e7133",
   "metadata": {
    "name": "cell1",
    "collapsed": false
   },
   "source": "# Evaluation & Promotion Of Models\n\nThis notebook loads the champion and challenger models and run inference on evaluation dataset. Based of the perfomance criteria it decides whether to promote the challenger to champion or not."
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "Imports"
   },
   "source": "# Import python packages\nfrom sklearn.metrics import roc_auc_score, f1_score, accuracy_score\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.ml.registry import Registry\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "Set_up"
   },
   "source": "# Set up the database, schema and model registry\ndatabase_name='DEV_AUTOMATION_DEMO'\nschema_name='CHAMPION_CHALLENGER'\nsession.use_database(database_name)\nsession.use_schema(schema_name)  \n\n# Initialize registry\nregistry = Registry(session=session, \n                           database_name=database_name, \n                           schema_name=schema_name)  \nprint(f\"‚úÖ Environment ready: {database_name}.{schema_name}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bed4f1b4-c5a2-47c9-bb88-2be36c358a9c",
   "metadata": {
    "name": "Eval_data_info",
    "collapsed": false
   },
   "source": "# Evaluation Data\nGet the holdout dataset from week 16-19 saved in the table named evaluation_data."
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "Get_eval_data"
   },
   "source": "eval_dataset = session.table('evaluation_data').to_pandas()\n\n# üìã Prepare Features and Target\nfeature_cols = [col for col in eval_dataset.columns \n               if col not in {'application_date', 'week_number', 'approved'}]\n\n#prepare dataset to evaluate performance\nX_eval = eval_dataset[feature_cols]\ny_true = eval_dataset['approved']",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d5fa3c96-02c9-4bf0-8482-8b64d21e1fd3",
   "metadata": {
    "name": "models",
    "collapsed": false
   },
   "source": "# Model Details\nGet the champion and challenger versions of model from the Snowflake model registry."
  },
  {
   "cell_type": "code",
   "id": "10a059ef-766b-41da-b7be-bc65d0d3a28a",
   "metadata": {
    "language": "python",
    "name": "models_version_names"
   },
   "outputs": [],
   "source": "#Get the model\nmodel = registry.get_model(\"CREDIT_APPROVAL\")\n\n# üèÜ Get Live (Champion) Model Version\nlive_version = model.get_tag(\"live_version\")\nprint(f\"üèÜ Current live model version: {live_version}\")\n\n# ü•ä Get Challenger Model Version (if exists)\ntry:\n    challenger_version = model.get_tag(\"challenger_version\")\n    print(f\"ü•ä Current challenger version: {challenger_version}\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è No challenger model found: {e}\")\n    challenger_version = None",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a21e9c76-8b8e-4325-a23a-abd119e97254",
   "metadata": {
    "language": "python",
    "name": "Metric_func"
   },
   "outputs": [],
   "source": "# Function to evaluate the metric values\ndef compute_metrics(y_true, y_pred, model_name):\n    \"\"\"Compute AUC, F1, and Accuracy scores\"\"\"\n    auc = round(roc_auc_score(y_true, y_pred), 4)\n    f1 = round(f1_score(y_true, y_pred), 4)\n    accuracy = round(accuracy_score(y_true, y_pred), 4)\n    \n    print(f\"\\nüìà {model_name} Performance:\")\n    print(f\"   üéØ AUC Score: {auc:.4f}\")\n    print(f\"   üéØ F1 Score: {f1:.4f}\")\n    print(f\"   üéØ Accuracy: {accuracy:.4f}\")\n    \n    return {'auc': auc, 'f1': f1, 'accuracy': accuracy}\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "14439153-952d-4cb1-b239-84cb0508fa29",
   "metadata": {
    "name": "performance_eval_info",
    "collapsed": false
   },
   "source": "### Calculate the Champion and Challenger performance metric on hold out evaluation data."
  },
  {
   "cell_type": "code",
   "id": "25689dc4-fa70-416d-a1e2-0f23199b9d8e",
   "metadata": {
    "language": "python",
    "name": "Champion_performance"
   },
   "outputs": [],
   "source": "#Run Champion prediction function\ny_pred_champion = model.version(live_version).run(X_eval, function_name=\"predict\")\n# üèÜ Champion Performance\nchampion_metrics = compute_metrics(y_true, y_pred_champion, \"CHAMPION\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "419851e3-1b8d-4be7-baf3-bad9710af4da",
   "metadata": {
    "language": "python",
    "name": "Challenger_performance"
   },
   "outputs": [],
   "source": "#Run Challenger prediction function\ny_pred_challenger = model.version(challenger_version).run(X_eval, function_name=\"predict\")\n\n# ü•ä Challenger Performance\nchallenger_metrics = compute_metrics(y_true, y_pred_challenger, \"CHALLENGER\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36e416a0-e34a-42e5-b394-2aa7a04f38b6",
   "metadata": {
    "name": "swap",
    "collapsed": false
   },
   "source": "# Swap models if Challenger score is higher"
  },
  {
   "cell_type": "code",
   "id": "764456da-e931-4596-81cc-544659123f2e",
   "metadata": {
    "language": "python",
    "name": "Model_promotion"
   },
   "outputs": [],
   "source": "print(\"üîÑ EVALUATING MODEL PROMOTION\")\n\nif challenger_metrics['auc'] > champion_metrics['auc']:\n    \n    #Unset challenger alias\n    model.version('CHALLENGER').unset_alias('CHALLENGER')\n\n    #########  DEMOTE THE CURRENT MODEL ############\n    model.version('CHAMPION').unset_alias('CHAMPION')\n    \n    ########## PROMOTE THE CURRENT CHALLENGER MODEL ############\n    model.version(challenger_version).set_alias('CHAMPION')\n    \n    model.default = challenger_version #optional\n    model.set_tag(\"live_version\", challenger_version)\n    print (\"Challenger is promoted to Live\")\n    \n    # Promotion metadata\n    model.version(challenger_version).set_metric(\"promotion_date\", str(datetime.now()))\n    model.version(challenger_version).set_metric(\"promoted_from\", live_version)\n    model.version(challenger_version).set_metric(\"eval_auc\", f\"{challenger_metrics['auc']}\")\n        \n    print(f\"\\nüéâ MODEL PROMOTION COMPLETED SUCCESSFULLY!\")\n    print(f\"üèÜ New Champion: {live_version}\")\n    \nelse:\n    print(f\"\\n‚ùå PROMOTION CRITERIA NOT MET\")\n    print(f\"üìù Reason: Challenger performance not better than champion\")\n    print(f\"üèÜ Current Champion Remains Active: {live_version}\")\n   ",
   "execution_count": null
  }
 ]
}