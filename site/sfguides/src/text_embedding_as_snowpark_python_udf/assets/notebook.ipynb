{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Prerequisites\n",
    "\n",
    "Let's start by installing some packages we'll need.\n",
    "\n",
    "**NOTE: You may need to restart the notebook after installing these for them to work right!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"snowflake-connector-python[pandas]\" \"transformers~=4.31.0\" \"torch~=2.0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lmerrick/miniconda3/envs/embedding-quickstart/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "from transformers.models.bert.tokenization_bert_fast import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download The Embedding Model For Internet-Free Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs.\n",
    "ARCHIVE_FORMAT = \"tar\"\n",
    "MODEL_NAME = \"intfloat/e5-base-v2\"\n",
    "SAVE_DIR_NAME = \"e5_base_v2_assets\"\n",
    "SAVE_DIR = Path(SAVE_DIR_NAME)\n",
    "ARCHIVE_FILE_NAME = f\"{SAVE_DIR_NAME}.{ARCHIVE_FORMAT}\"\n",
    "TOKENISER_DIR = SAVE_DIR / \"tokenizer\"\n",
    "MODEL_DIR = SAVE_DIR / \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tokenizer and model and save copies to specific local directories.\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Avoids warnings later.\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = BertModel.from_pretrained(MODEL_NAME)\n",
    "assert isinstance(model, BertModel)  # This appeases the typechecker, if you're using one.\n",
    "tokenizer.save_pretrained(TOKENISER_DIR)\n",
    "model.save_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that our saved files work by loading from them.\n",
    "tokenizer = BertTokenizerFast.from_pretrained(TOKENISER_DIR)\n",
    "model = BertModel.from_pretrained(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pack up the assets into an archive for uploading to a Snowflake stage as a single file.\n",
    "_ = shutil.make_archive(\n",
    "    base_name=SAVE_DIR_NAME,\n",
    "    format=ARCHIVE_FORMAT,\n",
    "    root_dir=SAVE_DIR.parent,\n",
    "    base_dir=SAVE_DIR,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload The Model To Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter password: ········\n"
     ]
    }
   ],
   "source": [
    "# Edit these parameters.\n",
    "connection_params = {\n",
    "    \"account\"   : \"<your_account_identifier_goes_here>\",\n",
    "    \"user\"      : \"<your_username_goes_here>\",\n",
    "    \"role\"      : \"ACCOUNTADMIN\",\n",
    "}\n",
    "\n",
    "# Establish and configure connection.\n",
    "connection_params[\"password\"] = getpass.getpass(f\"Enter password:\")\n",
    "connection = snowflake.connector.connect(**connection_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<snowflake.connector.cursor.SnowflakeCursor at 0xffff25f39ac0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # First, let's create a new warehouse, DB, and schema to use for this quickstart.\n",
    "connection.execute_string(\"create or replace warehouse text_embedding_quickstart_wh\")\n",
    "connection.execute_string(\"use warehouse text_embedding_quickstart_wh\")\n",
    "connection.execute_string(\"create or replace database text_embedding_quickstart_db\")\n",
    "connection.execute_string(\"use database text_embedding_quickstart_db\")\n",
    "connection.execute_string(\"create or replace schema text_embedding_quickstart_schema\")\n",
    "connection.execute_string(\"use schema text_embedding_quickstart_schema\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we'll create a new stage and upload our model archive file to it.\n",
    "# NOTE: Be patient, this ~0.5GB upload can take several minutes over some internet connections.\n",
    "stage_name = \"text_embedding_quickstart_stage\"\n",
    "connection.execute_string(f\"create or replace stage {stage_name}\")\n",
    "connection.execute_string(f\"PUT 'file://{ARCHIVE_FILE_NAME}' @{stage_name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Write\" A Python UDF\n",
    "\n",
    "Before we can locally test a Python UDF, we need to write one!\n",
    "\n",
    "Normally this takes a fair amount of effort, but the theme of this guide is blazing through to a working system first, then optionally coming back to discuss how it all works under the hood. Therefore, all we need to do to \"write\" our UDF here is to invoke the next cell and let the `%%writefile` cell magic write our premade UDF implementation to disk as `udf_implementation.py`.\n",
    "\n",
    "Feel free to pause and take as much time as you'd like reading the implementation below, but know that it's equally fine to just run the cell and move on for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting udf_implementation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile udf_implementation.py\n",
    "import fcntl\n",
    "import itertools\n",
    "import shutil\n",
    "import sys\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from transformers.models.bert.modeling_bert import BertModel\n",
    "from transformers.models.bert.tokenization_bert_fast import BertTokenizerFast\n",
    "\n",
    "\n",
    "####\n",
    "#### CONFIG\n",
    "####\n",
    "ARCHIVE_FORMAT = \"tar\"\n",
    "SAVE_DIR_NAME = \"e5_base_v2_assets\"\n",
    "MAX_BATCH_SIZE = 8\n",
    "EMBEDDING_SIZE = 768\n",
    "EMBEDDING_AS_BYTES_DTYPE = np.void(EMBEDDING_SIZE * np.float32().nbytes)\n",
    "\n",
    "####\n",
    "#### BUILDING BLOCKS\n",
    "####\n",
    "\n",
    "\n",
    "# Lock pattern adapted from \"Unzipping a Staged File\" official example.\n",
    "# https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-examples#unzipping-a-staged-file\n",
    "# File lock lets us synchronize access to /tmp in parallelized execution.\n",
    "class FileLock:\n",
    "    def __enter__(self):\n",
    "        self._lock = threading.Lock()\n",
    "        self._lock.acquire()\n",
    "        self._fd = open(\"/tmp/lockfile.LOCK\", \"w+\")\n",
    "        fcntl.lockf(self._fd, fcntl.LOCK_EX)\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self._fd.close()\n",
    "        self._lock.release()\n",
    "\n",
    "\n",
    "def _load_assets(archive_path: Path) -> Tuple[BertTokenizerFast, BertModel]:\n",
    "    # Config.\n",
    "    tmp = Path(\"/tmp\")\n",
    "    extracted_dir = tmp / SAVE_DIR_NAME\n",
    "    tokenizer_dir = extracted_dir / \"tokenizer\"\n",
    "    model_dir = extracted_dir / \"model\"\n",
    "\n",
    "    # Extract and load, with a lock placed for concurrency sanity.\n",
    "    with FileLock():\n",
    "        assert archive_path.exists(), f\"{archive_path} not found!\"\n",
    "        shutil.unpack_archive(archive_path, tmp)\n",
    "        assert tokenizer_dir.exists(), \"failed to extract tokenizer dir\"\n",
    "        assert model_dir.exists(), \"failed to extract model dir\"\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(\n",
    "            tokenizer_dir, local_files_only=True\n",
    "        )\n",
    "        model = BertModel.from_pretrained(model_dir, local_files_only=True)\n",
    "    assert isinstance(model, BertModel)  # Appease typechecker.\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def _average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "\n",
    "\n",
    "def _embed_batch(\n",
    "    tokenizer: BertTokenizerFast, model: BertModel, texts: List[str]\n",
    ") -> np.ndarray:\n",
    "    # Tokenize.\n",
    "    batch_dict = tokenizer(\n",
    "        texts, max_length=512, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Embed.\n",
    "    with torch.no_grad():\n",
    "        model_outputs = model(**batch_dict)\n",
    "        embeddings = F.normalize(\n",
    "            _average_pool(model_outputs.last_hidden_state, batch_dict[\"attention_mask\"]),  # type: ignore\n",
    "            p=2,\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "    # All done!\n",
    "    return embeddings.numpy()\n",
    "\n",
    "\n",
    "def _byte_pack_embedding_matrix_rows(embedding_matrix: np.ndarray) -> np.ndarray:\n",
    "    assert embedding_matrix.dtype == np.float32, \"expect float32 embedding matrix\"\n",
    "    assert embedding_matrix.flags.c_contiguous, \"expect c_contiguous embedding matrix\"\n",
    "    batch_embedding_as_bytes = embedding_matrix.ravel().view(EMBEDDING_AS_BYTES_DTYPE)\n",
    "    assert batch_embedding_as_bytes.shape == (\n",
    "        embedding_matrix.shape[0],\n",
    "    ), \"output shape\"\n",
    "    return batch_embedding_as_bytes\n",
    "\n",
    "\n",
    "####\n",
    "#### LOADING STATE\n",
    "####\n",
    "\n",
    "if \"snowflake_import_directory\" in sys._xoptions:\n",
    "    # In Snowflake, the input path will be given like this.\n",
    "    sf_import_dir = Path(sys._xoptions[\"snowflake_import_directory\"])\n",
    "    sf_archive_path = (sf_import_dir / SAVE_DIR_NAME).with_suffix(f\".{ARCHIVE_FORMAT}\")\n",
    "else:\n",
    "    # Locally, we can mock it here.\n",
    "    sf_archive_path = Path(SAVE_DIR_NAME).with_suffix(f\".{ARCHIVE_FORMAT}\")\n",
    "\n",
    "tokenizer, model = _load_assets(sf_archive_path)\n",
    "\n",
    "\n",
    "####\n",
    "#### DEFINING ACTUAL UDF\n",
    "####\n",
    "def embed(df: pd.DataFrame) -> np.ndarray:\n",
    "    # Unpack and validate our inputs.\n",
    "    assert df.columns == (0,), \"expect single column\"\n",
    "    inputs = df[0].tolist()\n",
    "    assert len(inputs) > 0, \"expect one or more inputs\"\n",
    "    assert all(isinstance(input, str) for input in inputs), \"expect string inputs\"\n",
    "\n",
    "    # Do internal batching according to the `batch_size` constant.\n",
    "    input_iter = iter(inputs)\n",
    "    batched_iter = iter(lambda: list(itertools.islice(input_iter, MAX_BATCH_SIZE)), [])\n",
    "\n",
    "    # Run the embedding.\n",
    "    # Note: We're byte-packing our float32 embedding vectors into binary scalars\n",
    "    # so that we have a scalar output compatible with Snowflake BINARY type.\n",
    "    i = 0\n",
    "    result = np.ndarray(shape=len(inputs), dtype=EMBEDDING_AS_BYTES_DTYPE)\n",
    "    for batch in batched_iter:\n",
    "        n_in_batch = len(batch)\n",
    "        embedding_matrix = _embed_batch(tokenizer=tokenizer, model=model, texts=batch)\n",
    "        result[i : i + n_in_batch] = _byte_pack_embedding_matrix_rows(embedding_matrix)\n",
    "        i = i + n_in_batch\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Make sure the UDF is vectorized.\n",
    "embed._sf_vectorized_input = pd.DataFrame  # type: ignore\n",
    "embed._sf_max_batch_size = 32  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally Test Our UDF\n",
    "\n",
    "Now that we have \"written\" a UDF implemenation, let's take the code for a spin locally before installing it in Snowpark.\n",
    "\n",
    "**NOTE: Importing our udf implementation has the side-effect of loading the tokenizer and model from the tarfile on disk.** If you skipped the model downloading and archiving steps above, you will get an error in the next cell. Normally it is considered bad form to make a Python module load state on import, but in Snowpark this is commonly recommended [as a performance optimization](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-designing#put-expensive-initialization-in-the-module)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The `udf_implementation` module requires the model tarfile to exist at import time.\n",
    "from udf_implementation import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20,),\n",
       " (768,),\n",
       " array([-0.01143286, -0.00579326, -0.02625675,  0.0083553 ,  0.03573489,\n",
       "        -0.01909555,  0.02105159,  0.03357907, -0.00106065, -0.00575643],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mock = pd.DataFrame({0: [\"test text\", \"another test text\"] * 10})\n",
    "result_array = embed(df_mock)\n",
    "first_embedding = np.frombuffer(result_array[0], dtype=np.float32)\n",
    "result_array.shape, first_embedding.shape, first_embedding[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Our UDF To Snowflake\n",
    "\n",
    "Now that we have verified our UDF implementation works locally on the exact same model archive file we have stored in our Snowflake stage, the last step is to push our UDF implementation to Snowpark.\n",
    "\n",
    "To do this, we first upload our UDF code to a Snowflake stage, then we invoke a `create function` SQL statement to tell Snowflake to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<snowflake.connector.cursor.SnowflakeCursor at 0xffff5a7ec130>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload assets and UDF code.\n",
    "udf_implemenation_file = \"udf_implementation.py\"\n",
    "connection.execute_string(f\"PUT 'file://{udf_implemenation_file}' @{stage_name}/ OVERWRITE = true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "create or replace function warehouse_text_embed(s string)\n",
      "returns binary\n",
      "language python\n",
      "runtime_version = '3.8'\n",
      "packages = ('numpy', 'pandas', 'pytorch==2.0.1', 'transformers==4.29.2')\n",
      "handler = 'udf_implementation.embed'\n",
      "imports = ('@text_embedding_quickstart_stage/udf_implementation.py', '@text_embedding_quickstart_stage/e5_base_v2_assets.tar')\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<snowflake.connector.cursor.SnowflakeCursor at 0xffff5a216d30>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the UDF.\n",
    "create_sql = f\"\"\"\n",
    "create or replace function warehouse_text_embed(s string)\n",
    "returns binary\n",
    "language python\n",
    "runtime_version = '3.8'\n",
    "packages = ('numpy', 'pandas', 'pytorch==2.0.1', 'transformers==4.29.2')\n",
    "handler = '{Path(udf_implemenation_file).stem}.embed'\n",
    "imports = ('@{stage_name}/{udf_implemenation_file}', '@{stage_name}/{ARCHIVE_FILE_NAME}')\n",
    "\"\"\"\n",
    "print(create_sql)\n",
    "connection.execute_string(create_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EMBEDING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'\\x07\\xfbS;\\x95\\x9e\\x0c\\xbcG^\\x1a\\xbd\\xaf{Y\\x...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            EMBEDING\n",
       "0  b'\\x07\\xfbS;\\x95\\x9e\\x0c\\xbcG^\\x1a\\xbd\\xaf{Y\\x..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the UDF!\n",
    "query = \"select warehouse_text_embed('hello world!') as embeding\"\n",
    "df_result = connection.cursor().execute(query).fetch_pandas_all()\n",
    "df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((768,),\n",
       " array([ 0.00323457, -0.00858273, -0.03768757, -0.00331853,  0.0448132 ,\n",
       "        -0.03027021,  0.03205844,  0.05281176, -0.00108271, -0.01863021],\n",
       "       dtype=float32),\n",
       " True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate the output is what we expect.\n",
    "result_embedding = np.frombuffer(df_result.iat[0,0], dtype=np.float32)\n",
    "expected_embedding = np.frombuffer(embed(pd.DataFrame({0: [\"hello world!\"]}))[0], dtype=np.float32)\n",
    "result_embedding.shape, result_embedding[:10], np.all(expected_embedding == result_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BINARY vs. ARRAY Vectors\n",
    "\n",
    "For storage and computational efficiency, our text embedding UDF stores embedding vectors as BINARY blobs. If you want to treat them as Snowflake ARRAY type instead, all it takes is one line of Javascript to convert them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<snowflake.connector.cursor.SnowflakeCursor at 0xffff599d5460>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a UDF to unpack arrays.\n",
    "connection.execute_string(\"\"\"\n",
    "create or replace function unpack_array(B binary)\n",
    "returns array\n",
    "language javascript\n",
    "as $$ return Array.from(new Float32Array(B.buffer)); $$;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x07\\xfbS;\\x95\\x9e\\x0c\\xbcG^\\x1a\\xbd\\xaf{Y\\xbb\\x0b\\x8e7=:\\xf9\\xf7\\xbc\\xb6O\\x03=%Q' [0.003234566887840629, -0.008582730777561665, -0.03768756613135338, -0.003318529343232512, 0.04481319710612297, -0.03027020767331123, 0.03205844014883041, 0.05281176045536995, -0.001082708826288581, -0.01863021403551102, -0.02498815208673477, 0.04769045859575272, -0.08714178204536438, 0.018616558983922, -0.03132862970232964, 0.006546470336616039, 0.02404151484370232, -0.008242154493927956, 0.03759677708148956, -0.02006378769874573, -0.04824942350387573, -0.05238137021660805, 0.04970219358801842, -0.00879327766597271, 0.005695960950106382, 0.01114101614803076, -0.005848507396876812, 0.001621721195988357, -0.04690505191683769, -0.03712567314505577]\n"
     ]
    }
   ],
   "source": [
    "# Unpack from binary embeddings to array-type embeddings.\n",
    "import json\n",
    "connection.execute_string(\"\"\"\n",
    "create or replace temporary table tmp_emb (embedding binary) as\n",
    "select warehouse_text_embed('hello world!') as embedding\n",
    "\"\"\")\n",
    "query = \"\"\"\n",
    "select embedding, unpack_array(embedding) as embedding_array from tmp_emb\n",
    "\"\"\"\n",
    "df_result = connection.cursor().execute(query).fetch_pandas_all()\n",
    "print(\n",
    "    df_result[\"EMBEDDING\"].iat[0][:30],  # Vector as binary.\n",
    "    json.loads(df_result[\"EMBEDDING_ARRAY\"].iat[0])[:30]  # Vector as array (json string in Pandas).\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
