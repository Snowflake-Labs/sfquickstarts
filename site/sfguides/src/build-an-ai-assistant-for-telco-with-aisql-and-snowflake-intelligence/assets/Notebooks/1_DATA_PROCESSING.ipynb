{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "intro-overview"
   },
   "source": [
    "# NovaConnect Data Processing Notebook\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook loads and processes all data for the NovaConnect Snowflake Intelligence lab.\n",
    "\n",
    "**What this notebook does:**\n",
    "1. **Load Structured Data:** Import 6 CSV files (81,425 records) from @CSV_STAGE\n",
    "2. **Process Audio Files:** Transcribe 25 customer support calls using AI_TRANSCRIBE from @AUDIO_STAGE\n",
    "3. **Process PDF Documents:** Extract text from 8 NovaConnect help documents using AI_PARSE_DOCUMENT from @PDF_STAGE\n",
    "4. **Apply AI Analysis:** Use Cortex AI functions (AI_SENTIMENT, AI_CLASSIFY, SUMMARIZE, AI_COMPLETE)\n",
    "5. **Populate Tables:** Save all processed data for analytics and Cortex Search\n",
    "\n",
    "**Prerequisites:**\n",
    "- setup.sql has been run successfully\n",
    "- Files uploaded to stages:\n",
    "  - @CSV_STAGE: 6 CSV files (network_performance.csv, customer_details.csv, etc.)\n",
    "  - @AUDIO_STAGE: 25 MP3 audio files (CALL_001.mp3 to CALL_025.mp3)\n",
    "  - @PDF_STAGE: 8 PDF documents (NovaConnect help guides)\n",
    "\n",
    "**Duration:** 15-20 minutes\n",
    "\n",
    "**Output:** Populated tables ready for intelligence_lab.ipynb exploration"
   ],
   "id": "intro-overview"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "setup-session"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "from snowflake.snowpark import Session, Row\n",
    "from snowflake.snowpark.functions import col, lit, call_builtin\n",
    "from snowflake.snowpark.types import StringType, TimestampType\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "session = Session.builder.getOrCreate()\n",
    "print(f\"Connected to: {session.get_current_database()}.{session.get_current_schema()}\")\n"
   ],
   "id": "setup-session"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "part1-csv-intro"
   },
   "source": [
    "## Part 1: Load Structured Data from CSV Files\n",
    "\n",
    "**What happens in this section:**\n",
    "- Loads 6 CSV files from @CSV_STAGE using COPY INTO command\n",
    "- Truncates existing data to ensure clean re-runs\n",
    "- Tables loaded:\n",
    "  - **network_performance:** 49,864 tower performance metrics\n",
    "  - **customer_feedback_summary:** 20,061 daily feedback aggregations\n",
    "  - **customer_details:** 10,000 customer profiles\n",
    "  - **infrastructure_capacity:** 1,500 tower capacity snapshots\n",
    "  - **csat_surveys:** 25 customer satisfaction surveys\n",
    "  - **customer_interaction_history:** 25 customer interaction summaries\n",
    "\n",
    "**Expected output:** All structured data tables populated and ready for analysis\n",
    "\n",
    "**Processing time:** 1-2 minutes"
   ],
   "id": "part1-csv-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "part1-load-csv-data"
   },
   "outputs": [],
   "source": [
    "# Load CSV files from @CSV_STAGE with fully qualified names\n",
    "print(\"Loading structured data from @CSV_STAGE...\\\\n\")\n",
    "\n",
    "# Get current database and use DEFAULT_SCHEMA where tables are located\n",
    "current_db = session.get_current_database()\n",
    "target_schema = \"DEFAULT_SCHEMA\"  # Tables are in DEFAULT_SCHEMA, not NOTEBOOKS\n",
    "print(f\"Loading into: {current_db}.{target_schema}\\\\n\")\n",
    "\n",
    "# Define CSV files in stage and their target tables\n",
    "csv_files = {\n",
    "    'network_performance': 'network_performance.csv',\n",
    "    'customer_feedback_summary': 'customer_feedback_summary.csv',\n",
    "    'customer_details': 'customer_details.csv',\n",
    "    'infrastructure_capacity': 'infrastructure_capacity.csv',\n",
    "    'csat_surveys': 'csat_surveys.csv',\n",
    "    'customer_interaction_history': 'customer_interaction_history.csv'\n",
    "}\n",
    "\n",
    "# Load each CSV from stage using COPY INTO with fully qualified names\n",
    "for table_name, file_name in csv_files.items():\n",
    "    # Build fully qualified table name - tables are in DEFAULT_SCHEMA\n",
    "    fully_qualified_table = f\"{current_db}.{target_schema}.{table_name}\"\n",
    "    \n",
    "    # Clear existing data to avoid duplicates on re-run\n",
    "    session.sql(f\"TRUNCATE TABLE IF EXISTS {fully_qualified_table}\").collect()\n",
    "    \n",
    "    # Load from CSV_STAGE with fully qualified names\n",
    "    session.sql(f\"\"\"\n",
    "    COPY INTO {fully_qualified_table}\n",
    "    FROM @{current_db}.{target_schema}.CSV_STAGE/{file_name}\n",
    "    FILE_FORMAT = (FORMAT_NAME = '{current_db}.{target_schema}.csv_format')\n",
    "    ON_ERROR = 'CONTINUE'\n",
    "    PURGE = FALSE\n",
    "    \"\"\").collect()\n",
    "    \n",
    "    # Count records using fully qualified name\n",
    "    count = session.sql(f\"SELECT COUNT(*) FROM {fully_qualified_table}\").collect()[0][0]\n",
    "    print(f\"Loaded {count:,} records into {table_name.upper()}\")\n",
    "\n",
    "print(\"\\\\nAll structured data loaded from stage!\")"
   ],
   "id": "part1-load-csv-data"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "part2-audio-intro"
   },
   "source": [
    "## Part 2: Process Audio Files with AI_TRANSCRIBE\n",
    "\n",
    "**What happens in this section:**\n",
    "- Lists all audio files (MP3/WAV/M4A) from @AUDIO_STAGE\n",
    "- Transcribes each audio file to text using AI_TRANSCRIBE\n",
    "- Extracts call duration from audio metadata\n",
    "- Applies AI analysis:\n",
    "  - **AI_SENTIMENT:** Analyzes customer emotion (returns VARIANT with sentiment categories)\n",
    "  - **SUMMARIZE:** Creates concise call summaries\n",
    "  - **AI_CLASSIFY:** Categorizes calls (Network Issue, Billing Issue, etc.)\n",
    "  - **AI_COMPLETE:** Extracts agent names and key customer issues\n",
    "- Saves to CUSTOMER_CALL_TRANSCRIPTS table\n",
    "\n",
    "**Expected output:** 25 fully analyzed call transcripts with all fields populated\n",
    "\n",
    "**Processing time:** 5-10 minutes (parallel execution)"
   ],
   "id": "part2-audio-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "part2-transcribe-audio"
   },
   "outputs": [],
   "source": [
    "# Get current database and use DEFAULT_SCHEMA where AUDIO_STAGE is located\n",
    "current_db = session.get_current_database()\n",
    "target_schema = \"DEFAULT_SCHEMA\"  # AUDIO_STAGE is in DEFAULT_SCHEMA\n",
    "\n",
    "# Get audio files from @AUDIO_STAGE with fully qualified name\n",
    "all_files = session.sql(f\"LIST @{current_db}.{target_schema}.AUDIO_STAGE\").collect()\n",
    "\n",
    "# Filter for audio files (.mp3, .wav, .m4a)\n",
    "audio_files = [f for f in all_files if f['name'].lower().endswith(('.mp3', '.wav', '.m4a'))]\n",
    "print(f\"Found {len(audio_files)} audio files in @{current_db}.{target_schema}.AUDIO_STAGE\\n\")\n",
    "\n",
    "if len(audio_files) == 0:\n",
    "    print(\"ERROR: No audio files found in @AUDIO_STAGE\")\n",
    "    print(\"Please upload MP3 files (CALL_001.mp3, etc.) to AUDIO_STAGE/call_recordings/\")\n",
    "    raise Exception(\"No audio files found\")\n",
    "\n",
    "# Build DataFrame with fully qualified stage paths\n",
    "audio_data = []\n",
    "for f in audio_files:\n",
    "    # The LIST result 'name' field contains the full path like 'audio_stage/call_recordings/CALL_001.mp3'\n",
    "    # We need to extract the relative path after the stage name\n",
    "    full_path = f['name']\n",
    "    # Get everything after 'audio_stage/' to get 'call_recordings/CALL_001.mp3'\n",
    "    relative_path = '/'.join(full_path.split('/')[1:])  # Remove stage name prefix\n",
    "    \n",
    "    # Extract just the filename for call_id\n",
    "    file_name = full_path.split('/')[-1]\n",
    "    call_id = file_name.replace('.mp3', '').replace('.wav', '').replace('.m4a', '')\n",
    "    \n",
    "    # Construct fully qualified stage path with the full relative path\n",
    "    fully_qualified_path = f\"@{current_db}.{target_schema}.AUDIO_STAGE/{relative_path}\"\n",
    "    \n",
    "    audio_data.append(Row(\n",
    "        call_id=call_id,\n",
    "        customer_id=f\"CUST_{random.randint(1, 10000):06d}\",\n",
    "        file_path=fully_qualified_path,\n",
    "        call_date=datetime.now()\n",
    "    ))\n",
    "\n",
    "audio_df = session.create_dataframe(audio_data)\n",
    "print(f\"Prepared {len(audio_data)} audio files for processing\")\n",
    "print(f\"Sample path: {audio_data[0]['file_path']}\")\n",
    "\n",
    "# Transcribe with AI_TRANSCRIBE\n",
    "print(\"Transcribing audio...\")\n",
    "\n",
    "# Use TO_FILE to create file reference (per Snowflake docs)\n",
    "transcribed = audio_df.with_column(\n",
    "    \"file_ref\",\n",
    "    call_builtin(\"TO_FILE\", col(\"file_path\"))\n",
    ").with_column(\n",
    "    \"transcription_result\",\n",
    "    call_builtin(\"SNOWFLAKE.CORTEX.AI_TRANSCRIBE\", col(\"file_ref\"))\n",
    ").with_column(\n",
    "    \"transcript_text\",\n",
    "    col(\"transcription_result\")[\"text\"].cast(StringType())\n",
    ").with_column(\n",
    "    \"call_duration_seconds\",\n",
    "    col(\"transcription_result\")[\"audio_duration\"].cast(\"INT\")\n",
    ")\n",
    "\n",
    "# Apply AI analysis\n",
    "transcribed = transcribed.with_column(\n",
    "    \"sentiment_score\",\n",
    "    call_builtin(\"SNOWFLAKE.CORTEX.AI_SENTIMENT\", col(\"transcript_text\"))\n",
    ").with_column(\n",
    "    \"summary\",\n",
    "    call_builtin(\"SNOWFLAKE.CORTEX.SUMMARIZE\", col(\"transcript_text\"))\n",
    ").with_column(\n",
    "    \"classify_result\",\n",
    "    call_builtin(\"SNOWFLAKE.CORTEX.AI_CLASSIFY\", col(\"transcript_text\"),\n",
    "                 lit(['Network Issue', 'Billing Issue', 'Technical Support', 'General']))\n",
    ").with_column(\n",
    "    \"call_reason\",\n",
    "    col(\"classify_result\")[\"labels\"][0].cast(StringType())\n",
    ").drop(\"classify_result\")\n",
    "\n",
    "print(\"Extracting additional insights...\")\n",
    "\n",
    "# Extract agent name using AI_COMPLETE\n",
    "transcribed = transcribed.with_column(\n",
    "    \"agent_name\",\n",
    "    call_builtin(\"SNOWFLAKE.CORTEX.AI_COMPLETE\", lit(\"mistral-large2\"),\n",
    "        call_builtin(\"CONCAT\",\n",
    "            lit(\"Extract only the agent's name from this call transcript. Return just the first name or 'Agent' if none mentioned: \"),\n",
    "            col(\"transcript_text\")))\n",
    ")\n",
    "\n",
    "# Extract key issues as ARRAY using AI_COMPLETE\n",
    "transcribed = transcribed.with_column(\n",
    "    \"issues_text\",\n",
    "    call_builtin(\"SNOWFLAKE.CORTEX.AI_COMPLETE\", lit(\"mistral-large2\"),\n",
    "        call_builtin(\"CONCAT\",\n",
    "            lit(\"Extract main customer issues as comma-separated list: \"),\n",
    "            col(\"transcript_text\")))\n",
    ").with_column(\n",
    "    \"key_issues\",\n",
    "    call_builtin(\"SPLIT\", col(\"issues_text\"), lit(\",\"))\n",
    ").drop(\"issues_text\")\n",
    "\n",
    "print(f\"Processed {transcribed.count()} transcripts with full AI analysis\")\n",
    "\n",
    "# Clear existing data before saving (for re-runs) using fully qualified name\n",
    "fully_qualified_table = f\"{current_db}.{target_schema}.customer_call_transcripts\"\n",
    "session.sql(f\"TRUNCATE TABLE IF EXISTS {fully_qualified_table}\").collect()\n",
    "\n",
    "# Save to table with fully qualified name\n",
    "transcribed.select(\n",
    "    \"call_id\",\n",
    "    \"customer_id\",\n",
    "    \"call_date\",\n",
    "    \"call_duration_seconds\",\n",
    "    col(\"file_path\").alias(\"audio_file_path\"),\n",
    "    \"transcript_text\",\n",
    "    \"sentiment_score\",\n",
    "    \"key_issues\",\n",
    "    \"summary\",\n",
    "    lit(\"Processed\").alias(\"resolution_status\"),\n",
    "    \"agent_name\",\n",
    "    \"call_reason\",\n",
    "    lit(None).alias(\"csat_score\")\n",
    ").write.mode(\"append\").save_as_table(fully_qualified_table)\n",
    "\n",
    "print(f\"Saved {transcribed.count()} call transcripts with all fields populated\")\n",
    ""
   ],
   "id": "part2-transcribe-audio"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "part3-pdf-intro"
   },
   "source": [
    "## Part 3: Process PDF Documents with AI_PARSE_DOCUMENT\n",
    "\n",
    "**What happens in this section:**\n",
    "- Uses bulk processing (single INSERT SELECT) for parallel execution\n",
    "- Parses all PDF files from @PDF_STAGE using AI_PARSE_DOCUMENT with LAYOUT mode\n",
    "- Extracts full text content for Cortex Search indexing\n",
    "- PDFs are NovaConnect help documentation:\n",
    "  - 5G plans information\n",
    "  - Help Centre guides (prepaid, roaming, internet issues, etc.)\n",
    "  - One Plan benefits\n",
    "- No sentiment analysis (these are reference documents, not customer feedback)\n",
    "- Saves to CUSTOMER_COMPLAINT_DOCUMENTS table\n",
    "\n",
    "**Expected output:** 8 help documents parsed and ready for Cortex Search\n",
    "\n",
    "**Processing time:** 2-3 minutes (all PDFs processed in parallel)\n",
    "\n",
    "**Use case:** Cortex Search will index this content so the Intelligence Agent can answer questions about NovaConnect plans and policies"
   ],
   "id": "part3-pdf-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "part3-parse-pdfs"
   },
   "outputs": [],
   "source": [
    "# Bulk process PDF files from @PDF_STAGE with fully qualified names\n",
    "try:\n",
    "    # Get current database and use DEFAULT_SCHEMA where tables/stages are located\n",
    "    current_db = session.get_current_database()\n",
    "    target_schema = \"DEFAULT_SCHEMA\"  # Tables and PDF_STAGE are in DEFAULT_SCHEMA\n",
    "    fully_qualified_table = f\"{current_db}.{target_schema}.customer_complaint_documents\"\n",
    "    fully_qualified_stage = f\"{current_db}.{target_schema}.PDF_STAGE\"\n",
    "    \n",
    "    # Get PDF file count\n",
    "    all_files = session.sql(f\"LIST @{fully_qualified_stage}\").collect()\n",
    "    pdf_count = len([f for f in all_files if f['name'].lower().endswith('.pdf')])\n",
    "    \n",
    "    print(f\"Found {pdf_count} PDF files in @PDF_STAGE\")\n",
    "    \n",
    "    if pdf_count > 0:\n",
    "        # Clear existing data\n",
    "        session.sql(f\"TRUNCATE TABLE IF EXISTS {fully_qualified_table}\").collect()\n",
    "        \n",
    "        # Bulk process all PDFs in single query (parallel processing)\n",
    "        print(\"Processing all PDFs in parallel...\")\n",
    "        \n",
    "        session.sql(f\"\"\"\n",
    "        INSERT INTO {fully_qualified_table} (\n",
    "            document_id, customer_id, document_date, document_type,\n",
    "            file_path, extracted_text, sentiment_score,\n",
    "            complaint_category, priority_level, summary\n",
    "        )\n",
    "        SELECT \n",
    "            REPLACE(relative_path, '.pdf', '') as document_id,\n",
    "            'SYSTEM' as customer_id,\n",
    "            CURRENT_DATE() as document_date,\n",
    "            'Help Document' as document_type,\n",
    "            '@{fully_qualified_stage}/' || relative_path as file_path,\n",
    "            SNOWFLAKE.CORTEX.AI_PARSE_DOCUMENT(\n",
    "                TO_FILE('@{fully_qualified_stage}', relative_path), \n",
    "                {{'mode': 'LAYOUT'}}\n",
    "            ):content::VARCHAR as extracted_text,\n",
    "            NULL::VARIANT as sentiment_score,\n",
    "            'Documentation' as complaint_category,\n",
    "            'Reference' as priority_level,\n",
    "            relative_path as summary\n",
    "        FROM DIRECTORY('@{fully_qualified_stage}')\n",
    "        WHERE LOWER(relative_path) LIKE '%.pdf'\n",
    "        \"\"\").collect()\n",
    "        \n",
    "        count = session.sql(f\"SELECT COUNT(*) FROM {fully_qualified_table}\").collect()[0][0]\n",
    "        print(f\"\\\\nProcessed and saved {count} help documents in parallel\")\n",
    "    else:\n",
    "        print(\"No PDFs found - skipping\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"PDF processing error: {e}\")"
   ],
   "id": "part3-parse-pdfs"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "name": "summary-verification-intro"
   },
   "source": [
    "## Summary: Data Processing Complete\n",
    "\n",
    "**Verification:**\n",
    "This cell verifies all data has been successfully loaded and processed.\n",
    "\n",
    "**What to check:**\n",
    "- All 8 tables should show record counts\n",
    "- customer_call_transcripts: 25 records\n",
    "- customer_complaint_documents: 8 records\n",
    "- Other tables: Thousands of records\n",
    "\n",
    "**If any table shows \"Not loaded\":**\n",
    "- Check that files were uploaded to @raw_files stage\n",
    "- Verify file names match expected names\n",
    "- Re-run the specific Part (1, 2, or 3) above\n",
    "\n",
    "**Next step:** Run intelligence_lab.ipynb for hands-on exploration and visualization\n"
   ],
   "id": "summary-verification-intro"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "name": "summary-verify-data-loaded"
   },
   "outputs": [],
   "source": [
    "# Verify all data loaded with fully qualified names\n",
    "print(\"Data Loading Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get current database and use DEFAULT_SCHEMA where tables are located\n",
    "current_db = session.get_current_database()\n",
    "target_schema = \"DEFAULT_SCHEMA\"  # Tables are in DEFAULT_SCHEMA\n",
    "print(f\"Schema: {current_db}.{target_schema}\\\\n\")\n",
    "\n",
    "tables = ['network_performance', 'customer_details', 'customer_feedback_summary',\n",
    "          'infrastructure_capacity', 'csat_surveys', 'customer_interaction_history',\n",
    "          'customer_call_transcripts', 'customer_complaint_documents']\n",
    "\n",
    "for table in tables:\n",
    "    try:\n",
    "        fully_qualified_table = f\"{current_db}.{target_schema}.{table}\"\n",
    "        count = session.sql(f\"SELECT COUNT(*) FROM {fully_qualified_table}\").collect()[0][0]\n",
    "        print(f\"{table:40s}: {count:>6,} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"{table:40s}: Not loaded ({str(e)[:50]})\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 60)\n",
    "print(\"Data processing complete!\")\n",
    "print(\"Next: Run intelligence_lab.ipynb for hands-on exploration\")\n"
   ],
   "id": "summary-verify-data-loaded"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}