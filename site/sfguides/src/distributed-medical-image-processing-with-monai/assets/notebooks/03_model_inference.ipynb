{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e783bba",
   "metadata": {
    "name": "title"
   },
   "source": [
    "# MONAI Lung CT Registration - Distributed Inference\n",
    "\n",
    "This notebook demonstrates **distributed inference** using the trained registration model from Notebook 3.\n",
    "\n",
    "## Overview\n",
    "- **Task**: Apply trained registration model to medical images at scale\n",
    "- **Architecture**: Same LocalNet model, now deployed for inference\n",
    "- **Framework**: Ray for parallel processing across multiple GPUs\n",
    "- **Data Source**: Trained model from Snowflake Model Registry + NIfTI images from stages\n",
    "\n",
    "## Workflow\n",
    "1. **Setup & Dependencies** - Configure Ray cluster and install libraries\n",
    "2. **Load Trained Model** - Retrieve model from Snowflake Model Registry\n",
    "3. **Define Inference Pipeline** - Create distributed inference class\n",
    "4. **Run Parallel Inference** - Process multiple cases simultaneously\n",
    "5. **Save Results** - Store registered images and metrics to Snowflake\n",
    "\n",
    "## Key Features\n",
    "- **Model Registry Integration**: Load versioned models directly from Snowflake\n",
    "- **GPU Acceleration**: Parallel inference across multiple GPUs\n",
    "- **Automatic Scaling**: Ray distributes workload across available compute\n",
    "- **Result Persistence**: Save registered images back to Snowflake stages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4907fc4a",
   "metadata": {
    "name": "step1_header"
   },
   "source": [
    "## Step 1: Initialize Snowflake Session\n",
    "\n",
    "Set up connection to Snowflake for accessing:\n",
    "- Model Registry (to load trained models)\n",
    "- Stages (for reading input images and saving results)\n",
    "- Compute resources (Ray cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13130c4-a902-4638-9b9e-a6cb67fcec40",
   "metadata": {
    "language": "python",
    "name": "init_session"
   },
   "outputs": [],
   "source": [
    "# Core Python libraries\n",
    "import streamlit as st  # Interactive UI for Snowflake notebooks\n",
    "import pandas as pd     # Data manipulation\n",
    "\n",
    "# Snowflake integration\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "\n",
    "# Establish Snowflake session connection\n",
    "# Provides access to Model Registry, stages, and compute resources\n",
    "session = get_active_session()\n",
    "\n",
    "# Set query tag for consumption tracking\n",
    "session.query_tag = '{\"origin\":\"sf_sit-is\",\"name\":\"distributed_medical_image_processing_with_monai\",\"version\":{\"major\":1,\"minor\":0},\"attributes\":{\"is_quickstart\":1,\"source\":\"notebook\"}}'\n",
    "\n",
    "# Database name - matches setup.sql\n",
    "DATABASE_NAME = \"MONAI_DB\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7fd8c0",
   "metadata": {
    "name": "step2_header"
   },
   "source": [
    "## Step 2: Configure Distributed Inference Environment\n",
    "\n",
    "Set up the distributed computing environment for parallel inference:\n",
    "\n",
    "1. **Ray Cluster** - Initialize connection to distributed compute cluster\n",
    "2. **Cluster Scaling** - Scale to 4 nodes for parallel GPU inference\n",
    "3. **Dependencies** - Install MONAI and medical imaging libraries on all workers\n",
    "\n",
    "**Why distributed inference?** \n",
    "- Process multiple medical images simultaneously across GPUs\n",
    "- Dramatically reduce total inference time (e.g., 10 cases in parallel vs sequential)\n",
    "- Efficient utilization of available compute resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd5db5-8a00-47d6-bbb7-981ccb192409",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "ray_cluster_setup"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DISTRIBUTED INFERENCE SETUP WITH RAY\n",
    "# ============================================================================\n",
    "\n",
    "# Standard Python libraries\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import logging\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Deep learning and distributed computing\n",
    "import ray                          # Distributed computing framework\n",
    "import torch                        # PyTorch deep learning\n",
    "import torch.nn as nn              # Neural network modules\n",
    "import torch.optim as optim        # Optimization algorithms (not used in inference)\n",
    "import numpy as np                 # Numerical operations\n",
    "\n",
    "# Snowflake integrations\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.files import SnowflakeFile      # Read files from stages\n",
    "from snowflake.ml.runtime_cluster import scale_cluster  # Scale Ray cluster\n",
    "\n",
    "\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# RAY CLUSTER INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Connect to Ray cluster in Snowflake container runtime\n",
    "# 'auto' automatically discovers the cluster head node\n",
    "ray.init(address='auto', ignore_reinit_error=True)\n",
    "\n",
    "def configure_ray_logger() -> None:\n",
    "    \"\"\"\n",
    "    Configure logging to reduce Ray's verbose output during inference.\n",
    "    \n",
    "    Suppresses Ray internals while keeping application logs for tracking\n",
    "    inference progress and results.\n",
    "    \"\"\"\n",
    "    # Suppress Ray core logging (only critical errors)\n",
    "    ray_logger = logging.getLogger(\"ray\")\n",
    "    ray_logger.setLevel(logging.CRITICAL)\n",
    "    \n",
    "    # Suppress Ray Data logging (dataset operations)\n",
    "    data_logger = logging.getLogger(\"ray.data\")\n",
    "    data_logger.setLevel(logging.CRITICAL)\n",
    "    \n",
    "    # Keep INFO level for our inference logs (progress, metrics)\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Disable progress bars for cleaner output\n",
    "    context = ray.data.DataContext.get_current()\n",
    "    context.execution_options.verbose_progress = False\n",
    "    context.enable_operator_progress_bars = False\n",
    "\n",
    "# Apply logging configuration\n",
    "configure_ray_logger()\n",
    "\n",
    "# Scale cluster to 4 worker nodes (one GPU per node for parallel inference)\n",
    "scale_cluster(4)\n",
    "\n",
    "# ============================================================================\n",
    "# DISTRIBUTED DEPENDENCY INSTALLATION\n",
    "# ============================================================================\n",
    "\n",
    "@ray.remote(num_cpus=0)  # Minimal CPU usage for pip install\n",
    "def install_deps():\n",
    "    \"\"\"\n",
    "    Install required Python packages on a single Ray worker node.\n",
    "    \n",
    "    This runs on each node in the cluster to ensure all workers have\n",
    "    the necessary libraries for medical image inference.\n",
    "    \n",
    "    Returns:\n",
    "        str: Success or failure message with node IP address\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import subprocess\n",
    "        \n",
    "        # Required packages for medical image registration inference\n",
    "        packages = [\n",
    "            \"monai\",            # Medical imaging framework\n",
    "            \"pytorch-ignite\",   # Training utilities (dependencies)\n",
    "            \"itk\",             # Medical image I/O\n",
    "            \"gdown\",           # File downloader (if needed)\n",
    "            \"torchvision\",     # Vision utilities\n",
    "            \"lmdb\",            # Database library\n",
    "            \"transformers\",    # Tensor utilities\n",
    "            \"einops\",          # Tensor operations\n",
    "            \"nibabel\"          # NIfTI file format support\n",
    "        ]\n",
    "        \n",
    "        # Install packages silently (suppress output)\n",
    "        subprocess.run(\n",
    "            [\"pip\", \"install\"] + packages, \n",
    "            check=True, \n",
    "            stdout=subprocess.PIPE, \n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        \n",
    "        # Verify MONAI installation\n",
    "        result = subprocess.run(\n",
    "            [\"pip\", \"show\", \"monai\"], \n",
    "            capture_output=True, \n",
    "            text=True, \n",
    "            check=True\n",
    "        )\n",
    "        \n",
    "        # Return success message with node identifier\n",
    "        return f\"\u2705 Dependencies installed on {ray.util.get_node_ip_address()}:\\n{result.stdout.splitlines()[0]}\"\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        # Return error message if installation fails\n",
    "        error_msg = e.stderr if e.stderr else e.stdout\n",
    "        return f\"\u274c Failed on {ray.util.get_node_ip_address()}: {error_msg}\"\n",
    "\n",
    "# Get all alive nodes in the Ray cluster\n",
    "nodes = {node[\"NodeManagerAddress\"] for node in ray.nodes() if node[\"Alive\"]}\n",
    "\n",
    "# Create installation task for each node (runs in parallel)\n",
    "tasks = [install_deps.options(resources={f\"node:{node}\": 0.01}).remote() for node in nodes]\n",
    "\n",
    "# Wait for all installations to complete\n",
    "results = ray.get(tasks)\n",
    "\n",
    "# Print installation status for each node\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040a69f",
   "metadata": {
    "name": "step3_header"
   },
   "source": [
    "## Step 3: Define Distributed Inference Class\n",
    "\n",
    "This cell creates a `RegistryBasedInferencer` class that:\n",
    "\n",
    "1. **Loads Model from Registry** - Retrieves trained model from Snowflake Model Registry\n",
    "2. **Preprocesses Images** - Applies same transforms as training (CT windowing, resizing)\n",
    "3. **Runs Inference** - Predicts deformation fields and applies warping\n",
    "4. **Calculates Metrics** - Computes Dice score to evaluate registration quality\n",
    "5. **Saves Results** - Writes registered images back to Snowflake stages\n",
    "\n",
    "**Key Design**: This class is designed to run on Ray workers, with each worker processing a batch of cases independently. Ray handles distribution across GPUs automatically.\n",
    "\n",
    "**Model Registry vs Direct Loading**: The class supports both loading from Model Registry and fallback to stage paths for flexibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1ad925-c673-46cd-b21e-f2780da43158",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "inference_class"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DISTRIBUTED INFERENCE IMPLEMENTATION\n",
    "# ============================================================================\n",
    "# This cell implements a complete inference pipeline using:\n",
    "# - Snowflake Model Registry for model loading\n",
    "# - Ray for distributed parallel processing\n",
    "# - MONAI for medical image transformations\n",
    "\n",
    "# Core libraries\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import torch                    # PyTorch for model inference\n",
    "import ray                      # Distributed computing\n",
    "import tempfile                 # Temporary file management\n",
    "import os\n",
    "import io                       # Binary I/O operations\n",
    "import numpy as np\n",
    "\n",
    "# Snowflake integrations\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.files import SnowflakeFile\n",
    "from snowflake.ml.registry import Registry        # Model Registry access\n",
    "\n",
    "# MONAI medical imaging components\n",
    "from monai.networks.nets import LocalNet          # Registration network architecture\n",
    "from monai.networks.blocks import Warp            # Spatial warping layer\n",
    "from monai.transforms import (\n",
    "    Compose,                    # Chain transformations\n",
    "    LoadImage,                  # Load medical images\n",
    "    EnsureChannelFirst,         # Ensure channel-first format\n",
    "    ScaleIntensityRange,        # CT windowing (HU normalization)\n",
    "    Resize                      # Resize 3D volumes\n",
    ")\n",
    "\n",
    "session = get_active_session()\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# INFERENCE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# CT windowing parameters (must match training)\n",
    "CT_MIN_HU = -1000  # Air (minimum Hounsfield Unit)\n",
    "CT_MAX_HU = 500    # Soft tissue/bone (maximum HU)\n",
    "\n",
    "# Inference configuration parameters\n",
    "INFERENCE_CONFIG = {\n",
    "    \"spatial_size\": (96, 96, 104),              # Volume dimensions (H, W, D)\n",
    "    \"num_channel_initial\": 32,                  # Model architecture parameter\n",
    "    \"database\": DATABASE_NAME,               # Snowflake database\n",
    "    \"schema\": \"UTILS\",                          # Schema containing model\n",
    "    \"model_name\": \"LUNG_CT_REGISTRATION\",       # Model name in registry\n",
    "    \"model_version\": \"v1\"                       # Model version to use\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# DISTRIBUTED INFERENCE CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class RegistryBasedInferencer:\n",
    "    \"\"\"\n",
    "    Ray-compatible inference class for medical image registration.\n",
    "    \n",
    "    This class is designed to run on Ray workers for distributed inference:\n",
    "    - Each worker loads the model from Snowflake Model Registry\n",
    "    - Processes batches of medical image pairs independently\n",
    "    - Applies deformation fields to register images\n",
    "    - Calculates registration quality metrics (Dice score)\n",
    "    - Saves results back to Snowflake stages\n",
    "    \n",
    "    **Distributed Execution**:\n",
    "    Ray instantiates this class on each worker, and the __call__ method\n",
    "    processes batches of data in parallel across multiple GPUs.\n",
    "    \n",
    "    **Model Loading**:\n",
    "    Supports two modes:\n",
    "    1. Model Registry: Load versioned model from Snowflake registry\n",
    "    2. Fallback: Load directly from stage path if registry fails\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"LUNG_CT_REGISTRATION\",\n",
    "        model_version: str = \"v1\",\n",
    "        database: str = None,\n",
    "        schema: str = \"UTILS\",\n",
    "        spatial_size: tuple = (96, 96, 104),\n",
    "        num_channel_initial: int = 32,\n",
    "        save_to_stage: bool = True,\n",
    "        fallback_stage_path: str = None  # Fallback if registry loading fails\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the inference worker with model and configuration.\n",
    "        \n",
    "        This method is called once per Ray worker when the class is instantiated.\n",
    "        It loads the model, sets up preprocessing transforms, and prepares for inference.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): Model name in Snowflake Model Registry\n",
    "            model_version (str): Version identifier (e.g., \"v1\", \"v2\")\n",
    "            database (str): Snowflake database containing the model\n",
    "            schema (str): Schema containing the model\n",
    "            spatial_size (tuple): Target image size (H, W, D) for preprocessing\n",
    "            num_channel_initial (int): Model architecture parameter (must match training)\n",
    "            save_to_stage (bool): Whether to save registered images to Snowflake stages\n",
    "            fallback_stage_path (str): Direct path to model weights if registry fails\n",
    "                                       (e.g., f\"@{DATABASE_NAME}.UTILS.RESULTS_STG/best_model.pth\")\n",
    "        \"\"\"\n",
    "        # Store configuration\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.database = database\n",
    "        self.schema = schema\n",
    "        self.spatial_size = spatial_size\n",
    "        self.num_channel_initial = int(num_channel_initial)\n",
    "        self.save_to_stage = save_to_stage\n",
    "        self.fallback_stage_path = fallback_stage_path\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 1: Establish Snowflake session\n",
    "        # ====================================================================\n",
    "        try:\n",
    "            self.session = get_active_session()\n",
    "            print(f\"\u2705 Worker connected to Snowflake\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f Worker session failed: {e}\")\n",
    "            self.session = None\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 2: Detect compute device (GPU preferred)\n",
    "        # ====================================================================\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"\ud83d\udccd Using device: {self.device}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 3: Load trained model from registry or fallback\n",
    "        # ====================================================================\n",
    "        self._load_model_from_registry()\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 4: Initialize warping layer for applying deformations\n",
    "        # ====================================================================\n",
    "        self.warp_layer = Warp().to(self.device)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # STEP 5: Setup preprocessing transforms (must match training)\n",
    "        # ====================================================================\n",
    "        self._setup_preprocessing()\n",
    "    \n",
    "    def _load_model_from_registry(self):\n",
    "        \"\"\"\n",
    "        Load trained model from Snowflake Model Registry with fallback support.\n",
    "        \n",
    "        Loading Strategy:\n",
    "        1. Try to connect to Model Registry and retrieve model reference\n",
    "        2. Create model architecture (LocalNet) matching training configuration\n",
    "        3. Load trained weights from fallback stage path\n",
    "        4. Set model to evaluation mode\n",
    "        \n",
    "        **Note**: Currently uses fallback_stage_path for weight loading because\n",
    "        direct model.run() from registry has compatibility issues with Ray workers.\n",
    "        For production, ensure fallback_stage_path points to latest trained model.\n",
    "        \"\"\"\n",
    "        print(f\"\ud83d\udce5 Loading model from registry: {self.model_name} {self.model_version}\")\n",
    "        \n",
    "        try:\n",
    "            # ================================================================\n",
    "            # STEP 1: Connect to Snowflake Model Registry\n",
    "            # ================================================================\n",
    "            registry = Registry(\n",
    "                session=self.session,\n",
    "                database_name=self.database,\n",
    "                schema_name=self.schema\n",
    "            )\n",
    "            \n",
    "            # Get model reference (metadata and versioning info)\n",
    "            model_ref = registry.get_model(self.model_name).version(self.model_version)\n",
    "            print(f\"\u2705 Model reference obtained: {model_ref.fully_qualified_model_name}\")\n",
    "            \n",
    "            # ================================================================\n",
    "            # STEP 2: Reconstruct model architecture\n",
    "            # ================================================================\n",
    "            # Must match the architecture used during training!\n",
    "            self.model = LocalNet(\n",
    "                spatial_dims=3,              # 3D medical images\n",
    "                in_channels=2,               # Concatenated fixed + moving images\n",
    "                out_channels=3,              # 3D deformation field (x, y, z)\n",
    "                num_channel_initial=self.num_channel_initial,  # Feature channels\n",
    "                extract_levels=[3],          # U-Net depth\n",
    "                out_activation=None,         # No activation (regression task)\n",
    "                out_kernel_initializer=\"zeros\"  # Identity transform initialization\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # ================================================================\n",
    "            # STEP 3: Load trained weights\n",
    "            # ================================================================\n",
    "            # Currently using fallback path for weight loading\n",
    "            # Registry.run() has known issues with Ray distributed workers\n",
    "            \n",
    "            if self.fallback_stage_path:\n",
    "                print(f\"\ud83d\udce5 Loading weights from fallback path: {self.fallback_stage_path}\")\n",
    "                \n",
    "                # Download model weights from Snowflake stage\n",
    "                raw_stream = self.session.file.get_stream(self.fallback_stage_path)\n",
    "                model_buffer = io.BytesIO(raw_stream.read())\n",
    "                \n",
    "                # Load state dict (trained parameters)\n",
    "                state_dict = torch.load(model_buffer, map_location=self.device)\n",
    "                self.model.load_state_dict(state_dict)\n",
    "            else:\n",
    "                # No fallback provided - model will have random weights!\n",
    "                print(\"\u26a0\ufe0f No fallback path provided. Model may not have trained weights.\")\n",
    "                print(\"   Provide fallback_stage_path parameter for production use.\")\n",
    "            \n",
    "            # ================================================================\n",
    "            # STEP 4: Set to evaluation mode\n",
    "            # ================================================================\n",
    "            # Disables dropout, batch norm tracking, etc.\n",
    "            self.model.eval()\n",
    "            print(f\"\u2705 Model loaded successfully on {self.device}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Registry loading failed, try direct stage loading\n",
    "            print(f\"\u274c Failed to load from registry: {e}\")\n",
    "            print(f\"   Attempting fallback to stage path...\")\n",
    "            \n",
    "            if self.fallback_stage_path:\n",
    "                self._load_from_stage_fallback()\n",
    "            else:\n",
    "                raise RuntimeError(f\"Model loading failed and no fallback provided: {e}\")\n",
    "    \n",
    "    def _load_from_stage_fallback(self):\n",
    "        \"\"\"Fallback: Load directly from stage\"\"\"\n",
    "        print(f\"\ud83d\udce5 Loading from stage: {self.fallback_stage_path}\")\n",
    "        \n",
    "        raw_stream = self.session.file.get_stream(self.fallback_stage_path)\n",
    "        model_buffer = io.BytesIO(raw_stream.read())\n",
    "        state_dict = torch.load(model_buffer, map_location=self.device)\n",
    "        \n",
    "        self.model = LocalNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=2,\n",
    "            out_channels=3,\n",
    "            num_channel_initial=self.num_channel_initial,\n",
    "            extract_levels=[3],\n",
    "            out_activation=None,\n",
    "            out_kernel_initializer=\"zeros\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.model.load_state_dict(state_dict)\n",
    "        self.model.eval()\n",
    "        print(f\"\u2705 Model loaded from stage on {self.device}\")\n",
    "    \n",
    "    def _setup_preprocessing(self):\n",
    "        \"\"\"\n",
    "        Setup MONAI preprocessing transforms for images and labels.\n",
    "        \n",
    "        **CRITICAL**: These transforms must EXACTLY match training preprocessing!\n",
    "        Any mismatch will cause poor inference results.\n",
    "        \n",
    "        Image Pipeline:\n",
    "        1. Load NIfTI file\n",
    "        2. Ensure channel-first format\n",
    "        3. Apply CT windowing (normalize HU to [0,1])\n",
    "        4. Resize to model input size\n",
    "        \n",
    "        Label Pipeline:\n",
    "        1. Load NIfTI file\n",
    "        2. Ensure channel-first format\n",
    "        3. Resize with nearest neighbor (preserve binary values)\n",
    "        \"\"\"\n",
    "        # ====================================================================\n",
    "        # IMAGE PREPROCESSING (with CT windowing)\n",
    "        # ====================================================================\n",
    "        self.preprocess_image = Compose([\n",
    "            LoadImage(image_only=True),           # Load NIfTI file\n",
    "            EnsureChannelFirst(),                 # Add channel dimension\n",
    "            ScaleIntensityRange(                  # CT windowing (CRITICAL!)\n",
    "                a_min=CT_MIN_HU,                  # Input min: -1000 HU (air)\n",
    "                a_max=CT_MAX_HU,                  # Input max: 500 HU (tissue)\n",
    "                b_min=0.0,                        # Output min: 0\n",
    "                b_max=1.0,                        # Output max: 1\n",
    "                clip=True                         # Clip outliers\n",
    "            ),\n",
    "            Resize(                               # Resize to model dimensions\n",
    "                spatial_size=self.spatial_size, \n",
    "                mode=\"trilinear\"                  # Smooth interpolation\n",
    "            ),\n",
    "        ])\n",
    "        \n",
    "        # ====================================================================\n",
    "        # LABEL (SEGMENTATION MASK) PREPROCESSING\n",
    "        # ====================================================================\n",
    "        # NO intensity normalization - labels are binary (0 or 1)\n",
    "        self.preprocess_label = Compose([\n",
    "            LoadImage(image_only=True),           # Load NIfTI file\n",
    "            EnsureChannelFirst(),                 # Add channel dimension\n",
    "            Resize(                               # Resize to model dimensions\n",
    "                spatial_size=self.spatial_size, \n",
    "                mode=\"nearest\"                    # Nearest neighbor (preserve binary)\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def _read_file_from_stage(self, stage_path: str) -> bytes:\n",
    "        \"\"\"Read file from Snowflake stage\"\"\"\n",
    "        clean_path = stage_path.strip()\n",
    "        if not clean_path.startswith(\"@\"):\n",
    "            clean_path = f\"@{clean_path}\"\n",
    "        with SnowflakeFile.open(clean_path, 'rb') as f:\n",
    "            return f.read()\n",
    "    \n",
    "    def _load_and_preprocess(self, file_binary: bytes, is_label: bool = False) -> torch.Tensor:\n",
    "        \"\"\"Load and preprocess a file\"\"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix=\".nii.gz\", delete=False) as tmp:\n",
    "            tmp.write(file_binary)\n",
    "            tmp_path = tmp.name\n",
    "        \n",
    "        try:\n",
    "            if is_label:\n",
    "                image = self.preprocess_label(tmp_path)\n",
    "            else:\n",
    "                image = self.preprocess_image(tmp_path)\n",
    "            return image.unsqueeze(0).to(self.device)\n",
    "        finally:\n",
    "            if os.path.exists(tmp_path):\n",
    "                os.unlink(tmp_path)\n",
    "    \n",
    "    def _save_result(self, tensor: torch.Tensor, case_id: str, suffix: str = \"\") -> str:\n",
    "        \"\"\"Save result to stage\"\"\"\n",
    "        if not self.save_to_stage or self.session is None:\n",
    "            return None\n",
    "        \n",
    "        import nibabel as nib\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(suffix=\".nii.gz\", delete=False) as tmp:\n",
    "            array = tensor.cpu().numpy()[0, 0]\n",
    "            nib.save(nib.Nifti1Image(array, affine=np.eye(4)), tmp.name)\n",
    "            \n",
    "            try:\n",
    "                output_filename = f\"registered_{case_id}{suffix}.nii.gz\"\n",
    "                stage_path = f\"@{self.database}.{self.schema}.RESULTS_STG/{output_filename}\"\n",
    "                self.session.file.put(tmp.name, stage_path, overwrite=True, auto_compress=False)\n",
    "                return stage_path\n",
    "            except Exception as e:\n",
    "                print(f\"\u26a0\ufe0f Failed to save: {e}\")\n",
    "                return None\n",
    "            finally:\n",
    "                os.unlink(tmp.name)\n",
    "    \n",
    "    def __call__(self, batch: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process a batch of medical image pairs (main inference entry point).\n",
    "        \n",
    "        This method is called by Ray for each batch of data in distributed processing.\n",
    "        Each worker processes its assigned batch independently in parallel.\n",
    "        \n",
    "        For each case:\n",
    "        1. Load images from Snowflake stages\n",
    "        2. Apply preprocessing transforms\n",
    "        3. Run inference (predict deformation field)\n",
    "        4. Apply warping to register images\n",
    "        5. Calculate Dice score (registration quality metric)\n",
    "        6. Save results back to Snowflake\n",
    "        \n",
    "        Args:\n",
    "            batch (pd.DataFrame): Batch of cases with columns:\n",
    "                - case_id: Unique case identifier\n",
    "                - fixed_path: Path to expiration CT scan\n",
    "                - moving_path: Path to inspiration CT scan\n",
    "                - fixed_label_path: Path to expiration lung mask\n",
    "                - moving_label_path: Path to inspiration lung mask\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Results with columns:\n",
    "                - case_id, status, dice_score, output_image, output_label,\n",
    "                  model_name, model_version (and error if failed)\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Process each case in the batch sequentially\n",
    "        for idx, row in batch.iterrows():\n",
    "            case_id = row.get('case_id', 'unknown')\n",
    "            \n",
    "            try:\n",
    "                print(f\"\ud83d\udd04 Processing case: {case_id}\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 1: Load images from Snowflake stages\n",
    "                # ============================================================\n",
    "                fixed_bin = self._read_file_from_stage(row['fixed_path'])\n",
    "                moving_bin = self._read_file_from_stage(row['moving_path'])\n",
    "                fixed_lbl_bin = self._read_file_from_stage(row['fixed_label_path'])\n",
    "                moving_lbl_bin = self._read_file_from_stage(row['moving_label_path'])\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 2: Preprocess images (CT windowing, resizing)\n",
    "                # ============================================================\n",
    "                fixed_img = self._load_and_preprocess(fixed_bin, is_label=False)\n",
    "                moving_img = self._load_and_preprocess(moving_bin, is_label=False)\n",
    "                fixed_lbl = self._load_and_preprocess(fixed_lbl_bin, is_label=True)\n",
    "                moving_lbl = self._load_and_preprocess(moving_lbl_bin, is_label=True)\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 3: Run inference\n",
    "                # ============================================================\n",
    "                with torch.no_grad():  # Disable gradient computation (inference only)\n",
    "                    # Concatenate images for model input (channel dimension)\n",
    "                    # Shape: (1, 2, H, W, D) where 2 = [moving, fixed]\n",
    "                    input_tensor = torch.cat((moving_img, fixed_img), dim=1)\n",
    "                    \n",
    "                    # Predict 3D deformation field\n",
    "                    # Output shape: (1, 3, H, W, D) where 3 = [dx, dy, dz]\n",
    "                    ddf = self.model(input_tensor)\n",
    "                    \n",
    "                    # Apply warping to register moving image/label to fixed space\n",
    "                    reg_label = self.warp_layer(moving_lbl, ddf)\n",
    "                    reg_image = self.warp_layer(moving_img, ddf)\n",
    "                    \n",
    "                    # Calculate Dice score (overlap between registered and fixed masks)\n",
    "                    # Range: [0, 1] where 1 = perfect overlap\n",
    "                    intersection = (reg_label * fixed_lbl).sum()\n",
    "                    dice = (2.0 * intersection + 1e-5) / (reg_label.sum() + fixed_lbl.sum() + 1e-5)\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 4: Save results to Snowflake stages\n",
    "                # ============================================================\n",
    "                img_path = self._save_result(reg_image, case_id, \"_img\")\n",
    "                lbl_path = self._save_result(reg_label, case_id, \"_label\")\n",
    "                \n",
    "                # ============================================================\n",
    "                # STEP 5: Record results\n",
    "                # ============================================================\n",
    "                results.append({\n",
    "                    \"case_id\": case_id,\n",
    "                    \"status\": \"success\",\n",
    "                    \"dice_score\": float(dice.item()),\n",
    "                    \"output_image\": img_path,\n",
    "                    \"output_label\": lbl_path,\n",
    "                    \"model_name\": self.model_name,\n",
    "                    \"model_version\": self.model_version\n",
    "                })\n",
    "                \n",
    "                print(f\"\u2705 {case_id}: Dice = {dice.item():.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Log error and continue with next case\n",
    "                print(f\"\u274c Failed on {case_id}: {e}\")\n",
    "                results.append({\n",
    "                    \"case_id\": case_id,\n",
    "                    \"status\": \"failed\",\n",
    "                    \"error\": str(e),\n",
    "                    \"dice_score\": 0.0,\n",
    "                    \"model_name\": self.model_name,\n",
    "                    \"model_version\": self.model_version\n",
    "                })\n",
    "        \n",
    "        # Return results as DataFrame for Ray to aggregate\n",
    "        return pd.DataFrame(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742c13a",
   "metadata": {
    "name": "step4_header"
   },
   "source": [
    "## Step 4: Define Distributed Inference Orchestrator\n",
    "\n",
    "This function orchestrates the distributed inference process:\n",
    "\n",
    "1. **Configures Ray Workers** - Sets up inference parameters for all workers\n",
    "2. **Partitions Dataset** - Divides data across multiple workers\n",
    "3. **Launches Parallel Inference** - Executes inference across GPUs simultaneously\n",
    "4. **Aggregates Results** - Collects and combines results from all workers\n",
    "\n",
    "**Key Parameter**: `num_workers` controls parallelism (typically matches number of available GPUs).\n",
    "\n",
    "**Result**: A DataFrame containing registration results (Dice scores, output paths) for all processed cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521d759-2060-458f-95f0-fdb02ca35344",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "inference_orchestrator"
   },
   "outputs": [],
   "source": [
    "def run_distributed_inference_with_registry(\n",
    "    ray_dataset,\n",
    "    model_name: str = \"LUNG_CT_REGISTRATION\",\n",
    "    model_version: str = \"V1\",\n",
    "    fallback_stage_path: str = f'@{DATABASE_NAME}.UTILS.RESULTS_STG/best_model.pth',\n",
    "    num_workers: int = 4,\n",
    "    batch_size: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrate distributed inference across multiple GPU workers.\n",
    "    \n",
    "    This function:\n",
    "    1. Configures inference parameters (model, preprocessing, etc.)\n",
    "    2. Partitions the dataset across workers\n",
    "    3. Launches Ray workers with GPU allocation\n",
    "    4. Aggregates results from all workers\n",
    "    \n",
    "    **Distributed Execution**:\n",
    "    - Ray creates `num_workers` instances of RegistryBasedInferencer\n",
    "    - Each worker gets 1 GPU and processes batches independently\n",
    "    - Results are automatically aggregated by Ray\n",
    "    \n",
    "    Args:\n",
    "        ray_dataset (ray.data.Dataset): Dataset with case information\n",
    "            Required columns: case_id, fixed_path, moving_path, \n",
    "                            fixed_label_path, moving_label_path\n",
    "        model_name (str): Model name in Snowflake Model Registry\n",
    "        model_version (str): Model version to use\n",
    "        fallback_stage_path (str): Direct path to model weights file\n",
    "            Example: f\"@{DATABASE_NAME}.UTILS.RESULTS_STG/best_model.pth\"\n",
    "        num_workers (int): Number of parallel workers (typically = number of GPUs)\n",
    "        batch_size (int): Number of cases each worker processes per batch\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Results with columns:\n",
    "            - case_id: Case identifier\n",
    "            - status: \"success\" or \"failed\"\n",
    "            - dice_score: Registration quality metric [0, 1]\n",
    "            - output_image: Path to registered image\n",
    "            - output_label: Path to registered label\n",
    "            - model_name, model_version: Model identifiers\n",
    "    \"\"\"\n",
    "    \n",
    "    st.subheader(f\"\ud83d\ude80 Distributed Inference: {model_name} v{model_version}\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: Configure inference parameters for all workers\n",
    "    # ========================================================================\n",
    "    inference_args = {\n",
    "        \"model_name\": model_name,\n",
    "        \"model_version\": model_version,\n",
    "        \"database\": INFERENCE_CONFIG[\"database\"],\n",
    "        \"schema\": INFERENCE_CONFIG[\"schema\"],\n",
    "        \"spatial_size\": INFERENCE_CONFIG[\"spatial_size\"],\n",
    "        \"num_channel_initial\": INFERENCE_CONFIG[\"num_channel_initial\"],\n",
    "        \"save_to_stage\": True,\n",
    "        \"fallback_stage_path\": fallback_stage_path  # Critical for loading weights!\n",
    "    }\n",
    "    \n",
    "    st.info(f\"\ud83d\udd27 Using {num_workers} workers with {batch_size} case(s) per batch\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Partition dataset for parallel processing\n",
    "    # ========================================================================\n",
    "    # Repartition into 20 blocks for fine-grained parallelism\n",
    "    # Ray will distribute these blocks across the workers\n",
    "    ds_partitioned = ray_dataset.repartition(20)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Launch distributed inference\n",
    "    # ========================================================================\n",
    "    with st.spinner(f\"Running inference on {num_workers} GPUs...\"):\n",
    "        results = ds_partitioned.map_batches(\n",
    "            RegistryBasedInferencer,            # Inference class to instantiate\n",
    "            fn_constructor_kwargs=inference_args,  # Arguments for __init__\n",
    "            batch_size=batch_size,              # Cases per batch\n",
    "            concurrency=num_workers,            # Number of parallel workers\n",
    "            num_gpus=1,                         # GPU allocation per worker\n",
    "            batch_format=\"pandas\"               # Input/output format\n",
    "        )\n",
    "        \n",
    "        # ========================================================================\n",
    "        # STEP 4: Collect and aggregate results from all workers\n",
    "        # ========================================================================\n",
    "        final_df = results.to_pandas()\n",
    "    \n",
    "    st.success(f\"\u2705 Inference complete! Processed {len(final_df)} cases.\")\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04fdae",
   "metadata": {
    "name": "step5_header"
   },
   "source": [
    "## Step 5: Load Test Dataset Paths\n",
    "\n",
    "Load file paths for the medical images we want to process. \n",
    "\n",
    "**Important**: This loads only metadata (file paths), not the actual image data! The actual images will be loaded just-in-time during inference by each worker.\n",
    "\n",
    "This function identifies pairs of:\n",
    "- Fixed images (expiration CT scans)\n",
    "- Moving images (inspiration CT scans)\n",
    "- Corresponding lung segmentation masks\n",
    "\n",
    "**Note**: In production, you might load a different test set than what was used for training validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83123615-c710-4edc-add6-1cf55986b203",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "load_test_paths"
   },
   "outputs": [],
   "source": [
    "def load_paired_paths(stage_location=f\"@{DATABASE_NAME}.UTILS.MONAI_MEDICAL_IMAGES_STG\"):\n",
    "    \"\"\"\n",
    "    Load paired CT scan paths for inference (metadata only, no binary data).\n",
    "    \n",
    "    This function:\n",
    "    1. Lists all NIfTI files in Snowflake stage using SQL\n",
    "    2. Categorizes files into fixed/moving images and masks\n",
    "    3. Pairs them by case ID\n",
    "    4. Returns a Ray dataset with file paths only\n",
    "    \n",
    "    Args:\n",
    "        stage_location (str): Snowflake stage containing medical images\n",
    "    \n",
    "    Returns:\n",
    "        ray.data.Dataset: Lightweight dataset containing only file paths\n",
    "    \"\"\"\n",
    "    session = get_active_session()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 1: List all NIfTI files using SQL (metadata only, fast)\n",
    "    # ========================================================================\n",
    "    df_files = session.sql(\n",
    "        f\"LIST {stage_location} PATTERN = '.*.nii.gz'\"\n",
    "    ).select('\"name\"').to_pandas()\n",
    "    \n",
    "    # Clean up column names and add fully-qualified path\n",
    "    df_files.rename(columns={'\"name\"': 'name'})\n",
    "    df_files[\"name\"] = f\"{DATABASE_NAME}.UTILS.\" + df_files[\"name\"]\n",
    "    \n",
    "    print(df_files)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 2: Categorize files by type using regex patterns\n",
    "    # ========================================================================\n",
    "    # Expected folder structure:\n",
    "    #   - scans/case_XXX_exp.nii.gz    (expiration CT)\n",
    "    #   - scans/case_XXX_insp.nii.gz   (inspiration CT)\n",
    "    #   - lungMasks/case_XXX_exp.nii.gz    (expiration mask)\n",
    "    #   - lungMasks/case_XXX_insp.nii.gz   (inspiration mask)\n",
    "    \n",
    "    fixed_imgs = df_files[df_files['name'].str.contains(\"scans.*_exp\", regex=True)]\n",
    "    moving_imgs = df_files[df_files['name'].str.contains(\"scans.*_insp\", regex=True)]\n",
    "    fixed_masks = df_files[df_files['name'].str.contains(\"lungMasks.*_exp\", regex=True)]\n",
    "    moving_masks = df_files[df_files['name'].str.contains(\"lungMasks.*_insp\", regex=True)]\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    # ========================================================================\n",
    "    # STEP 3: Match files into inference pairs by case ID\n",
    "    # ========================================================================\n",
    "    for _, row in fixed_imgs.iterrows():\n",
    "        f_path = row['name']\n",
    "        \n",
    "        # Extract case identifier (e.g., \"case_001\" from \"case_001_exp.nii.gz\")\n",
    "        filename = f_path.split('/')[-1] \n",
    "        case_id = filename.split('_exp')[0] \n",
    "        \n",
    "        # Find matching files for this case\n",
    "        m_path = moving_imgs[moving_imgs['name'].str.contains(f\"{case_id}_insp\")].iloc[0]['name']\n",
    "        fl_path = fixed_masks[fixed_masks['name'].str.contains(f\"{case_id}_exp\")].iloc[0]['name']\n",
    "        ml_path = moving_masks[moving_masks['name'].str.contains(f\"{case_id}_insp\")].iloc[0]['name']\n",
    "        \n",
    "        # Store complete pair\n",
    "        pairs.append({\n",
    "            \"case_id\": case_id,\n",
    "            \"fixed_path\": f_path,           # Expiration CT\n",
    "            \"moving_path\": m_path,          # Inspiration CT\n",
    "            \"fixed_label_path\": fl_path,    # Expiration lung mask\n",
    "            \"moving_label_path\": ml_path    # Inspiration lung mask\n",
    "        })\n",
    "        \n",
    "    print(f\"\u2705 Paired {len(pairs)} cases using paths only (no binary data loaded).\")\n",
    "    \n",
    "    # Convert to Ray dataset for distributed processing\n",
    "    return ray.data.from_pandas(pd.DataFrame(pairs))\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTE: Load test dataset paths\n",
    "# ============================================================================\n",
    "# This dataset is tiny (only strings), not the actual GB-sized medical images\n",
    "ray_dataset = load_paired_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2d44d5",
   "metadata": {
    "name": "step6_header"
   },
   "source": [
    "## Step 6: Verify Trained Model Availability\n",
    "\n",
    "Quick check to confirm our trained model exists in the Snowflake stage before running inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2c80c-17c2-41f1-9de4-b00e24ff260c",
   "metadata": {
    "codeCollapsed": false,
    "language": "sql",
    "name": "verify_model"
   },
   "outputs": [],
   "source": [
    "ls @RESULTS_STG;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c267e",
   "metadata": {
    "name": "step7_header"
   },
   "source": [
    "## Step 7: Execute Distributed Inference \ud83d\ude80\n",
    "\n",
    "**This is where the magic happens!**\n",
    "\n",
    "Launching parallel inference across multiple GPUs. The system will:\n",
    "1. Distribute the dataset across 4 GPU workers\n",
    "2. Each worker loads the trained model\n",
    "3. Process cases in parallel\n",
    "4. Save registered images to Snowflake stages\n",
    "5. Return metrics (Dice scores) for all cases\n",
    "\n",
    "**Expected Time**: Depends on dataset size and number of GPUs. With 4 GPUs processing 10 cases, expect ~2-5 minutes.\n",
    "\n",
    "**Monitoring**: Watch the output for per-case progress and Dice scores!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a3f073-6d03-4a6b-8ff5-9715bb6434de",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "execute_inference"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RUN DISTRIBUTED INFERENCE\n",
    "# ============================================================================\n",
    "\n",
    "# Execute inference with trained model from registry\n",
    "# This will process all cases in the ray_dataset using 4 parallel GPU workers\n",
    "out_df = run_distributed_inference_with_registry(ray_dataset)\n",
    "\n",
    "# Display summary statistics\n",
    "st.write(\"### Inference Results Summary\")\n",
    "st.write(f\"**Total cases processed**: {len(out_df)}\")\n",
    "st.write(f\"**Successful**: {(out_df['status'] == 'success').sum()}\")\n",
    "st.write(f\"**Failed**: {(out_df['status'] == 'failed').sum()}\")\n",
    "st.write(f\"**Average Dice Score**: {out_df[out_df['status'] == 'success']['dice_score'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e841c75",
   "metadata": {
    "name": "step8_header"
   },
   "source": [
    "## Step 8: Review Inference Results\n",
    "\n",
    "Display the complete results DataFrame with all inference metrics and output paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f046b73-d8a8-4186-a806-4011173be8e5",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "review_results"
   },
   "outputs": [],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc65f85",
   "metadata": {
    "name": "step9_header"
   },
   "source": [
    "## Step 9: Persist Results to Snowflake Table\n",
    "\n",
    "Save the inference results to a Snowflake table for:\n",
    "- **Long-term storage** - Results persist beyond the notebook session\n",
    "- **Analytics** - Query and analyze results using SQL\n",
    "- **Downstream processing** - Other applications can access the results\n",
    "- **Audit trail** - Track inference runs over time\n",
    "\n",
    "The table will contain case IDs, Dice scores, output paths, and model versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ae530b-8d5f-4b27-a9e7-0917a574174b",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "save_to_table"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVE RESULTS TO SNOWFLAKE TABLE\n",
    "# ============================================================================\n",
    "\n",
    "# Write results DataFrame to a Snowflake table\n",
    "# This enables SQL-based analysis and persistence beyond the notebook session\n",
    "session.write_pandas(\n",
    "    out_df,                              # Results DataFrame\n",
    "    'MONAI_PAIRED_LUNG_RESULTS',        # Table name\n",
    "    database=DATABASE_NAME,          # Target database\n",
    "    schema='RESULTS',                   # Target schema\n",
    "    auto_create_table=True,             # Create table if it doesn't exist\n",
    "    overwrite=True                      # Replace existing data (use False to append)\n",
    ")\n",
    "\n",
    "st.success(f\"\u2705 Results saved to {DATABASE_NAME}.RESULTS.MONAI_PAIRED_LUNG_RESULTS\")\n",
    "st.info(\"\ud83d\udca1 You can now query these results using SQL in Snowflake!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943b9b5",
   "metadata": {
    "name": "dynamic_summary"
   },
   "outputs": [],
   "source": [
    "# Dynamic summary with actual database name\n",
    "sql_example = f\"\"\"SELECT \n",
    "    case_id,\n",
    "    dice_score,\n",
    "    status,\n",
    "    model_name,\n",
    "    model_version\n",
    "FROM {DATABASE_NAME}.RESULTS.MONAI_PAIRED_LUNG_RESULTS\n",
    "ORDER BY dice_score DESC;\"\"\"\n",
    "\n",
    "st.markdown(f\"\"\"\n",
    "## \ud83c\udf89 Inference Complete!\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. \u2705 **Configured distributed environment** with Ray cluster (4 GPU workers)\n",
    "2. \u2705 **Loaded trained model** from Snowflake Model Registry\n",
    "3. \u2705 **Processed medical images** in parallel across multiple GPUs\n",
    "4. \u2705 **Generated registrations** for all test cases\n",
    "5. \u2705 **Calculated quality metrics** (Dice scores)\n",
    "6. \u2705 **Saved results** to Snowflake stages and tables\n",
    "\n",
    "### Results Analysis\n",
    "\n",
    "- **Registration Quality**: Check Dice scores in the results table\n",
    "  - Dice > 0.8: Excellent registration\n",
    "  - Dice 0.6-0.8: Good registration\n",
    "  - Dice < 0.6: May need review\n",
    "\n",
    "### Accessing Results\n",
    "\n",
    "**Registered Images**: Available in `@{DATABASE_NAME}.UTILS.RESULTS_STG`\n",
    "- Format: `registered_<case_id>_img.nii.gz` (registered CT scan)\n",
    "- Format: `registered_<case_id>_label.nii.gz` (registered lung mask)\n",
    "\n",
    "**Metrics Table**: `{DATABASE_NAME}.RESULTS.MONAI_PAIRED_LUNG_RESULTS`\n",
    "\"\"\")\n",
    "\n",
    "# Display SQL in a code block\n",
    "st.code(sql_example, language=\"sql\")\n",
    "\n",
    "st.markdown(f\"\"\"\n",
    "### Next Steps\n",
    "\n",
    "- **Visualize Results**: Load registered images and compare with originals\n",
    "- **Quality Control**: Review cases with low Dice scores\n",
    "- **Deploy to Production**: Use this pipeline for clinical workflows\n",
    "- **Retrain Model**: If results are suboptimal, retrain with more data or tuned hyperparameters\n",
    "\n",
    "---\n",
    "\n",
    "**Distributed medical image inference with Snowflake + Ray + MONAI = Complete! \ud83d\ude80**\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "lastEditStatus": {
   "authorEmail": "carlos.guzman@snowflake.com",
   "authorId": "4124439897012",
   "authorName": "CGUZMAN",
   "lastEditTime": 1763700645401,
   "notebookId": "acurk37bgiosjvacszva",
   "sessionId": "8ccd1d54-9c31-413c-bf06-a0fbc428d742"
  },
  "kernelspec": {
   "display_name": "py_spcs",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}