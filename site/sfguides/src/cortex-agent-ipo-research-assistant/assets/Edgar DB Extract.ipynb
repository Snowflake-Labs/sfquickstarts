{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "7szqj7kabedqas7raboz",
   "authorId": "7541437521474",
   "authorName": "JTAO",
   "authorEmail": "jonathan.tao@snowflake.com",
   "sessionId": "2cfee929-3e13-40b6-a543-db7a5d8b683c",
   "lastEditTime": 1767626908213
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "0b0d0462-bc58-41e2-8aac-64da7758658d",
   "metadata": {
    "language": "python",
    "name": "install_packages",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Cell 1: Install dependencies       \n!pip install pyrate_limiter-3.9.0-py3-none-any.whl\n!pip install sec_edgar_downloader-5.0.3-py3-none-any.whl\nfrom sec_edgar_downloader import Downloader                                                                                                                                      \nfrom snowflake.snowpark.context import get_active_session                                                                                                                        \nimport os                                                                                                                                                                        \nfrom datetime import datetime, timedelta                                                                                                                                         \nimport tempfile                                                                                                                                                                  \n                   ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64ef6ddb-ae8c-4d93-91a1-e02ec369a487",
   "metadata": {
    "language": "python",
    "name": "start_session"
   },
   "outputs": [],
   "source": "                                                                                                                                                               \n# Get active Snowflake session                                                                                                                                                   \nsession = get_active_session()   \n\nsession.sql(\"USE ROLE SYSADMIN\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f42a6f5e-ea9c-468f-823c-29ec80ff2926",
   "metadata": {
    "language": "python",
    "name": "create_s1_downloads_stage"
   },
   "outputs": [],
   "source": "                                                                                                                                                \n                                                                                                                                                                                  \n# Create a temporary directory for downloads                                                                                                                                     \ntemp_dir = tempfile.mkdtemp()                                                                                                                                                    \nprint(f\"Temporary download directory: {temp_dir}\")                                                                                                                               \n                                                                                                                                                                                                                                                                                                                                                            \nsession.sql(\"\"\"                                                                                                                                                                  \n     CREATE STAGE IF NOT EXISTS ipo_research_db.public.sec_filings_stage                                                                                                                                 \n     DIRECTORY = (ENABLE = TRUE)  \n     ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')\n     COMMENT = 'Stage for SEC EDGAR S-1 filings w/ SSE'                                                                                                                                  \n \"\"\").collect()                                                                                                                                                                   \n                                                                                                                                                                                  \nprint(\"Stage created/verified: sec_filings_stage\")                                                                                                                               \n                                                               ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1bc66718-e9ae-461b-a69d-6a2c12b3d76e",
   "metadata": {
    "language": "sql",
    "name": "create_sec_eai"
   },
   "outputs": [],
   "source": "USE ROLE ACCOUNTADMIN;\n\nUSE DATABASE IPO_RESEARCH_DB;\nUSE SCHEMA PUBLIC;\n\nCREATE OR REPLACE NETWORK RULE sec_edgar_network_rule\n    MODE = EGRESS\n    TYPE = HOST_PORT\n    VALUE_LIST = (\n    'sec.gov',\n    'www.sec.gov',\n    'efts.sec.gov',\n    'data.sec.gov'\n    )\n    COMMENT = 'Allow access to SEC EDGAR filing system'; \n\nCREATE OR REPLACE EXTERNAL ACCESS INTEGRATION sec_edgar_integration \n    ALLOWED_NETWORK_RULES = (sec_edgar_network_rule)\n    ENABLED = TRUE\n    COMMENT = 'Integration for accessing SEC EDGAR filings';  \n\nGRANT USAGE ON INTEGRATION sec_edgar_integration TO ROLE SYSADMIN;\n\nALTER NOTEBOOK \"Edgar DB Extract\"\n  SET EXTERNAL_ACCESS_INTEGRATIONS = (sec_edgar_integration);\n\nCREATE OR REPLACE FILE FORMAT html_single_row_format  \n     TYPE = 'CSV'\n     FIELD_DELIMITER = NONE\n     RECORD_DELIMITER = NONE \n     SKIP_HEADER = 0  \n     BINARY_FORMAT = 'UTF8';   ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2f529a9-9ed9-4a5c-801a-be36f36dac56",
   "metadata": {
    "language": "python",
    "name": "test_connection"
   },
   "outputs": [],
   "source": "                                                                                                                                                                                                    \nimport requests                                                                                                                                                                                    \n                                                                                                                                                                                                    \n # Test with explicit DNS                                                                                                                                                                           \nurl = \"https://www.sec.gov/files/company_tickers.json\"                                                                                                                                             \nheaders = {                                                                                                                                                                                        \n    'User-Agent': 'MyCompany myemail@company.com'                                                                                                                                                  \n}                                                                                                                                                                                                  \n                                                                                                                                                                                                    \ntry:                                                                                                                                                                                               \n    response = requests.get(url, headers=headers, timeout=10)                                                                                                                                      \n    print(f\"Status: {response.status_code}\")                                                                                                                                                       \n    print(f\"Response length: {len(response.content)}\")                                                                                                                                             \n    if response.status_code == 200:                                                                                                                                                                \n        print(\"✓ Connection successful!\")                                                                                                                                                          \n    else:                                                                                                                                                                                          \n        print(f\"✗ HTTP error: {response.status_code}\")                                                                                                                                             \nexcept Exception as e:                                                                                                                                                                             \n    print(f\"✗ Connection failed: {str(e)}\")   \n\n# If you get an error, restart the notebook",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "181c2783-542a-4edf-b63d-d835cab598b5",
   "metadata": {
    "language": "python",
    "name": "get_s1_filers_list"
   },
   "outputs": [],
   "source": "import requests                                                                                                                                                                  \nimport pandas as pd                                                                                                                                                              \nfrom datetime import datetime                                                                                                                                                    \n                                                                                                                                                                                \n# ========================================                                                                                                                                       \n# CONFIGURE YOUR DATE RANGE HERE                                                                                                                                                 \n# ========================================                                                                                                                                       \nSTART_YEAR = 2025                                                                                                                                                                \nSTART_QUARTER = 1  # 1, 2, 3, or 4                                                                                                                                               \n                                                                                                                                                                                \nEND_YEAR = 2025                                                                                                                                                                  \nEND_QUARTER = 1    # 1, 2, 3, or 4                                                                                                                                               \n# ========================================                                                                                                                                       \n                                                                                                                                                                                \ndef get_s1_filers(year, quarter):                                                                                                                                                \n    \"\"\"                                                                                                                                                                          \n    Fetch S-1 filers from SEC's quarterly index files                                                                                                                            \n    Quarter: 1, 2, 3, or 4                                                                                                                                                       \n    \"\"\"                                                                                                                                                                          \n    # SEC index URL format                                                                                                                                                       \n    url = f\"https://www.sec.gov/Archives/edgar/full-index/{year}/QTR{quarter}/form.idx\"                                                                                          \n                                                                                                                                                                                \n    headers = {                                                                                                                                                                  \n        'User-Agent': 'YourCompany your.email@company.com',  # Replace with your info                                                                                            \n        'Accept-Encoding': 'gzip, deflate',                                                                                                                                      \n        'Host': 'www.sec.gov'                                                                                                                                                    \n    }                                                                                                                                                                            \n                                                                                                                                                                                \n    try:                                                                                                                                                                         \n        response = requests.get(url, headers=headers)                                                                                                                            \n        response.raise_for_status()                                                                                                                                              \n    except Exception as e:                                                                                                                                                       \n        print(f\"Failed to fetch index for {year} Q{quarter}: {str(e)}\")                                                                                                          \n        return pd.DataFrame()                                                                                                                                                    \n                                                                                                                                                                                \n    # Parse the index file                                                                                                                                                       \n    lines = response.text.split('\\n')                                                                                                                                            \n                                                                                                                                                                                \n    # Find the separator line (dashes) that marks start of data                                                                                                                  \n    data_start_idx = None                                                                                                                                                        \n    for i, line in enumerate(lines):                                                                                                                                             \n        if '---' in line and len(line.strip()) > 50:  # Separator line is long                                                                                                   \n            data_start_idx = i + 1                                                                                                                                               \n            break                                                                                                                                                                \n                                                                                                                                                                                \n    # If no separator found, try alternative parsing                                                                                                                             \n    if data_start_idx is None:                                                                                                                                                   \n        print(f\"Warning: Could not find separator line for {year} Q{quarter}, trying alternative parsing\")                                                                       \n        # Skip first 10 lines as headers                                                                                                                                         \n        data_start_idx = 10                                                                                                                                                      \n                                                                                                                                                                                \n    data_lines = lines[data_start_idx:]                                                                                                                                          \n                                                                                                                                                                                \n    # Parse each line looking for S-1 filings                                                                                                                                    \n    # Format is fixed-width:                                                                                                                                                     \n    # Form Type (0-17), Company Name (17-75), CIK (75-87), Date Filed (87-99), File Name (99+)                                                                                   \n    filings = []                                                                                                                                                                 \n    for line in data_lines:                                                                                                                                                      \n        if not line.strip():                                                                                                                                                     \n            continue                                                                                                                                                             \n                                                                                                                                                                                \n        # Must be long enough to contain all fields                                                                                                                              \n        if len(line) < 99:                                                                                                                                                       \n            continue                                                                                                                                                             \n                                                                                                                                                                                \n        # Check if line starts with S-1                                                                                                                                          \n        form_type = line[0:17].strip()                                                                                                                                           \n                                                                                                                                                                                \n        if form_type not in ['S-1', 'S-1/A']:                                                                                                                                    \n            continue                                                                                                                                                             \n                                                                                                                                                                                \n        try:                                                                                                                                                                     \n            # Extract fields using fixed positions                                                                                                                               \n            company_name = line[17:75].strip()                                                                                                                                   \n            cik = line[75:87].strip()                                                                                                                                            \n            date_filed = line[87:99].strip()                                                                                                                                     \n            file_name = line[99:].strip()                                                                                                                                        \n                                                                                                                                                                                \n            # Remove leading zeros from CIK                                                                                                                                      \n            cik_clean = cik.lstrip('0') or '0'                                                                                                                                   \n                                                                                                                                                                                \n            filings.append({                                                                                                                                                     \n                'form_type': form_type,                                                                                                                                          \n                'company_name': company_name,                                                                                                                                    \n                'cik': cik_clean,                                                                                                                                                \n                'date_filed': date_filed,                                                                                                                                        \n                'file_name': file_name                                                                                                                                           \n            })                                                                                                                                                                   \n        except Exception as e:                                                                                                                                                   \n            # Skip lines that don't parse correctly                                                                                                                              \n            continue                                                                                                                                                             \n                                                                                                                                                                                \n    return pd.DataFrame(filings)                                                                                                                                                 \n                                                                                                                                                                                \n# Generate list of (year, quarter) tuples in the range                                                                                                                           \ndef generate_quarter_range(start_year, start_quarter, end_year, end_quarter):                                                                                                    \n    \"\"\"Generate all year/quarter combinations between start and end\"\"\"                                                                                                           \n    quarters = []                                                                                                                                                                \n                                                                                                                                                                                \n    current_year = start_year                                                                                                                                                    \n    current_quarter = start_quarter                                                                                                                                              \n                                                                                                                                                                                \n    while (current_year < end_year) or (current_year == end_year and current_quarter <= end_quarter):                                                                            \n        quarters.append((current_year, current_quarter))                                                                                                                         \n                                                                                                                                                                                \n        # Move to next quarter                                                                                                                                                   \n        current_quarter += 1                                                                                                                                                     \n        if current_quarter > 4:                                                                                                                                                  \n            current_quarter = 1                                                                                                                                                  \n            current_year += 1                                                                                                                                                    \n                                                                                                                                                                                \n    return quarters                                                                                                                                                              \n                                                                                                                                                                                \n# Get quarters to process                                                                                                                                                        \nquarters_to_process = generate_quarter_range(START_YEAR, START_QUARTER, END_YEAR, END_QUARTER)                                                                                   \n                                                                                                                                                                                \nprint(f\"Processing {len(quarters_to_process)} quarter(s): {START_YEAR} Q{START_QUARTER} to {END_YEAR} Q{END_QUARTER}\")                                                           \nprint(\"-\" * 60)                                                                                                                                                                  \n                                                                                                                                                                                \nall_s1_filers = []                                                                                                                                                               \n                                                                                                                                                                                \nfor year, quarter in quarters_to_process:                                                                                                                                        \n    print(f\"Fetching S-1 filings for {year} Q{quarter}...\")                                                                                                                      \n    df = get_s1_filers(year, quarter)                                                                                                                                            \n    if not df.empty:                                                                                                                                                             \n        all_s1_filers.append(df)                                                                                                                                                 \n        print(f\"  ✓ Found {len(df)} S-1 filings\")                                                                                                                                \n    else:                                                                                                                                                                        \n        print(f\"  ⚠ No S-1 filings found\")                                                                                                                                       \n                                                                                                                                                                                \n# Combine all quarters                                                                                                                                                           \nif all_s1_filers:                                                                                                                                                                \n    s1_filers_df = pd.concat(all_s1_filers, ignore_index=True)                                                                                                                   \n                                                                                                                                                                                \n    # Get unique CIKs (companies may file multiple times)                                                                                                                        \n    unique_ciks = s1_filers_df['cik'].unique().tolist()                                                                                                                          \n                                                                                                                                                                                \n    print(\"\\n\" + \"=\" * 60)                                                                                                                                                       \n    print(f\"✓ Total S-1 filings found: {len(s1_filers_df)}\")                                                                                                                     \n    print(f\"✓ Unique companies: {len(unique_ciks)}\")                                                                                                                             \n    print(f\"✓ Estimated download time: {len(unique_ciks) * 2 // 60} - {len(unique_ciks) * 3 // 60} minutes\")                                                                     \n    print(\"=\" * 60)                                                                                                                                                              \n                                                                                                                                                                                \n    # Display sample                                                                                                                                                             \n    print(\"\\nSample S-1 filers:\")                                                                                                                                                \n    display(s1_filers_df[['company_name', 'cik', 'date_filed', 'form_type']].head(20))                                                                                           \nelse:                                                                                                                                                                            \n    print(\"\\n⚠ No S-1 filings found in the specified range\")                                                                                                                     \n    unique_ciks = []   ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4b93c61-c1e9-4473-9270-311b2ea41bee",
   "metadata": {
    "language": "python",
    "name": "download_s1"
   },
   "outputs": [],
   "source": "# Create temporary directory                                                                                                                                                     \ntemp_dir = tempfile.mkdtemp()                                                                                                                                                    \nprint(f\"Download directory: {temp_dir}\")                                                                                                                                         \n                                                                                                                                                                                \n# Initialize downloader                                                                                                                                                          \ndl = Downloader(                                                                                                                                                                 \n    company_name=\"YourCompany\",                                                                                                                                                  \n    email_address=\"your.email@company.com\",                                                                                                                                      \n    download_folder=temp_dir                                                                                                                                                     \n)                                                                                                                                                                                \n                                                                                                                                                                                \n# Calculate date range                                                                                                                                                           \nend_date = datetime.now()                                                                                                                                                        \nstart_date = end_date - timedelta(days=365)                                                                                                                                      \n                                                                                                                                                                                \nprint(f\"\\nDownloading S-1 filings for {len(unique_ciks)} companies...\")                                                                                                          \nprint(\"This will take several minutes due to SEC rate limits (10 req/sec max)\\n\")                                                                                                \n                                                                                                                                                                                \nsuccessful_downloads = 0                                                                                                                                                         \nfailed_downloads = 0                                                                                                                                                             \n                                                                                                                                                                                \nfor i, cik in enumerate(unique_ciks):                                                                                                                                            \n    try:                                                                                                                                                                         \n        # Download S-1 filings for this CIK                                                                                                                                      \n        dl.get(                                                                                                                                                                  \n            \"S-1\",                                                                                                                                                               \n            cik,  # Use CIK instead of ticker                                                                                                                                    \n            after=start_date.strftime(\"%Y-%m-%d\"),                                                                                                                               \n            before=end_date.strftime(\"%Y-%m-%d\"),                                                                                                                                \n            download_details=True                                                                                                                                                \n        )                                                                                                                                                                        \n        successful_downloads += 1                                                                                                                                                \n                                                                                                                                                                                \n        if (i + 1) % 10 == 0:                                                                                                                                                    \n            print(f\"Progress: {i + 1}/{len(unique_ciks)} companies processed\")                                                                                                   \n                                                                                                                                                                                \n    except Exception as e:                                                                                                                                                       \n        failed_downloads += 1                                                                                                                                                    \n        if \"No filings\" not in str(e):  # Don't print if just no filings found                                                                                                   \n            print(f\"  ✗ CIK {cik}: {str(e)}\")                                                                                                                                    \n                                                                                                                                                                                \nprint(f\"\\n✓ Download Summary:\")                                                                                                                                                  \nprint(f\"    Successful: {successful_downloads}\")                                                                                                                                 \nprint(f\"    Failed: {failed_downloads}\")                                                                                                                                         \n                                                                                                                                                                                 \n                                                                   ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a83556ec-d2ca-42cb-a47c-4765ad050e5e",
   "metadata": {
    "language": "python",
    "name": "upload_to_stage"
   },
   "outputs": [],
   "source": "import glob                                                                                                                                                                      \n                                                                                                                                                                                \n# Find all downloaded filing files                                                                                                                                               \ndownloaded_files = []                                                                                                                                                            \nfor root, dirs, files in os.walk(temp_dir):                                                                                                                                      \n    for file in files:                                                                                                                                                           \n        file_path = os.path.join(root, file)                                                                                                                                     \n        downloaded_files.append(file_path)                                                                                                                                       \n                                                                                                                                                                                \nprint(f\"Found {len(downloaded_files)} files to upload\")                                                                                                                          \n                                                                                                                                                                                \nif len(downloaded_files) == 0:                                                                                                                                                   \n    print(\"⚠ No files found. Check if download completed successfully.\")                                                                                                         \nelse:                                                                                                                                                                            \n    # Upload files to stage                                                                                                                                                      \n    uploaded_count = 0                                                                                                                                                           \n    failed_count = 0                                                                                                                                                             \n                                                                                                                                                                                \n    for i, file_path in enumerate(downloaded_files):                                                                                                                             \n        try:                                                                                                                                                                     \n            # Get relative path to preserve directory structure                                                                                                                  \n            rel_path = os.path.relpath(file_path, temp_dir)                                                                                                                      \n            dir_path = os.path.dirname(rel_path)                                                                                                                                 \n                                                                                                                                                                                \n            # Upload using PUT command                                                                                                                                           \n            session.sql(f\"\"\"                                                                                                                                                     \n                PUT 'file://{file_path}' @sec_filings_stage/{dir_path if dir_path else ''}                                                                                       \n                AUTO_COMPRESS = FALSE                                                                                                                                             \n                OVERWRITE = TRUE                                                                                                                                                 \n            \"\"\").collect()                                                                                                                                                       \n                                                                                                                                                                                \n            uploaded_count += 1                                                                                                                                                  \n                                                                                                                                                                                \n            # Progress update every 50 files                                                                                                                                     \n            if (i + 1) % 50 == 0:                                                                                                                                                \n                print(f\"  Progress: {i + 1}/{len(downloaded_files)} files uploaded\")                                                                                             \n                                                                                                                                                                                \n        except Exception as e:                                                                                                                                                   \n            print(f\"  ✗ Failed: {os.path.basename(file_path)}: {str(e)}\")                                                                                                        \n            failed_count += 1                                                                                                                                                    \n                                                                                                                                                                                \n    print(f\"\\n✓ Upload Summary:\")                                                                                                                                                \n    print(f\"    Successfully uploaded: {uploaded_count}\")                                                                                                                        \n    print(f\"    Failed uploads: {failed_count}\")                 ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d10b331-eb57-42f7-9f39-4b1e4e80308c",
   "metadata": {
    "language": "python",
    "name": "truncate_docs"
   },
   "outputs": [],
   "source": "import re\nimport os\nimport tempfile\nimport shutil\n\ndef truncate_and_save_documents():\n    \"\"\"\n    Process all primary documents, truncate to first 100 pages, and save back to stage\n    \"\"\"\n    \n    # Create temporary directory for processing\n    temp_dir = tempfile.mkdtemp()\n    print(f\"Processing directory: {temp_dir}\")\n    \n    # Get all primary documents from stage\n    documents_query = \"\"\"\n    SELECT \n        metadata$filename AS file_path,\n        SPLIT_PART(metadata$filename, '/', 2) AS company_cik,\n        SPLIT_PART(metadata$filename, '/', 4) AS accession_number,\n        $1 as raw_content\n    FROM @sec_filings_stage (file_format => html_single_row_format)\n    WHERE metadata$filename LIKE '%/primary-document.html%'\n    \"\"\"\n    \n    documents_df = session.sql(documents_query).to_pandas()\n    print(f\"Retrieved {len(documents_df)} documents for processing\")\n    \n    processed_files = 0\n    truncated_count = 0\n    errors = []\n    \n    for idx, row in documents_df.iterrows():\n        try:\n            file_path = row['FILE_PATH']\n            company_cik = str(row['COMPANY_CIK'])\n            accession_number = str(row['ACCESSION_NUMBER'])\n            raw_content = row['RAW_CONTENT']\n            \n            #print(f\"\\nProcessing file {idx + 1}: {file_path}\")\n            #print(f\"Content type: {type(raw_content)}\")\n            \n            # Handle different data types\n            if raw_content is None:\n                #print(f\"  ⚠ Skipping - content is None\")\n                continue\n                \n            # Convert to string if needed\n            if isinstance(raw_content, bytes):\n                html_content = raw_content.decode('utf-8', errors='ignore')\n            elif isinstance(raw_content, str):\n                html_content = raw_content\n            else:\n                html_content = str(raw_content)\n            \n            # Check if content is meaningful\n            if len(html_content.strip()) < 100:\n                #print(f\"  ⚠ Skipping - content too short ({len(html_content)} chars)\")\n                continue\n            \n            #print(f\"  Original content length: {len(html_content):,} characters\")\n            \n            # Truncate to first 100 pages\n            result = truncate_to_pages(html_content, max_pages=100)\n            \n            # Create directory structure in temp folder matching original location\n            # Extract the directory path from the original file\n            original_dir = os.path.dirname(file_path)\n            full_dir = os.path.join(temp_dir, original_dir)\n            os.makedirs(full_dir, exist_ok=True)\n            \n            # Save truncated file in the same directory as original\n            truncated_path = os.path.join(full_dir, \"primary-document-100pg.html\")\n            with open(truncated_path, 'w', encoding='utf-8') as f:\n                f.write(result['content'])\n            \n            if result['was_truncated']:\n                #print(f\"  ✓ Truncated from {result['original_pages']} to {result['final_pages']} pages (method: {result['method']})\")\n                truncated_count += 1\n            else:\n                #print(f\"  ✓ No truncation needed - {result['original_pages']} pages (method: {result['method']})\")\n                continue\n                \n            processed_files += 1\n                \n        except Exception as e:\n            error_msg = f\"Error processing {row['FILE_PATH']}: {str(e)}\"\n            print(f\"  ✗ {error_msg}\")\n            errors.append(error_msg)\n    \n    # Upload truncated files back to stage\n    print(f\"\\nUploading truncated files to stage...\")\n    upload_count = 0\n    upload_errors = []\n    \n    for root, dirs, files in os.walk(temp_dir):\n        for file in files:\n            if file == 'primary-document-100pg.html':\n                try:\n                    local_file_path = os.path.join(root, file)\n                    rel_path = os.path.relpath(local_file_path, temp_dir)\n                    \n                    # Upload to stage preserving directory structure\n                    put_query = f\"\"\"\n                        PUT 'file://{local_file_path}' @sec_filings_stage/{os.path.dirname(rel_path.replace(os.sep, '/'))}\n                        AUTO_COMPRESS = FALSE\n                        OVERWRITE = TRUE       \n                    \"\"\"\n                    \n                    session.sql(put_query).collect()     \n                    upload_count += 1\n                    #print(f\"  ✓ Uploaded: {rel_path}\")\n                    \n                except Exception as e:\n                    error_msg = f\"Upload failed for {file}: {str(e)}\"\n                    #print(f\"  ✗ {error_msg}\")\n                    upload_errors.append(error_msg)\n    \n    print(f\"\\n\" + \"=\"*60)\n    print(\"PROCESSING SUMMARY\")\n    print(\"=\"*60)\n    print(f\"Files processed: {processed_files}\")\n    print(f\"Files truncated: {truncated_count}\")\n    print(f\"Files uploaded to stage: {upload_count}\")\n    print(f\"Processing errors: {len(errors)}\")\n    print(f\"Upload errors: {len(upload_errors)}\")\n    \n    if errors:\n        print(f\"\\nFirst few processing errors:\")\n        for error in errors[:3]:\n            print(f\"  - {error}\")\n    \n    # Cleanup temp directory\n    shutil.rmtree(temp_dir)\n    \n    return processed_files, truncated_count, upload_count\n\ndef truncate_to_pages(html_content, max_pages=100, paragraphs_per_page=15):\n    \"\"\"\n    Truncate HTML document to first N pages\n    \n    Args:\n        html_content: HTML string to truncate\n        max_pages: Maximum number of pages to keep\n        paragraphs_per_page: Number of <p> tags to consider as one page (default: 15)\n    \n    Returns dict with:\n    - content: truncated HTML content\n    - original_pages: number of pages in original\n    - final_pages: number of pages in result\n    - was_truncated: whether truncation occurred\n    - method: which method was used for truncation\n    \"\"\"\n    try:\n        # Ensure we have a string\n        if not isinstance(html_content, str):\n            raise ValueError(f\"Expected string, got {type(html_content)}\")\n        \n        # Method 1: Try SEC EDGAR <PAGE> tags first\n        page_pattern = r'<PAGE>'\n        page_breaks = list(re.finditer(page_pattern, html_content, re.IGNORECASE))\n        \n        if len(page_breaks) > 0:\n            # Use <PAGE> tags\n            original_pages = len(page_breaks)\n            \n            if original_pages <= max_pages:\n                return {\n                    'content': html_content,\n                    'original_pages': original_pages,\n                    'final_pages': original_pages,\n                    'was_truncated': False,\n                    'method': 'PAGE_TAGS'\n                }\n            \n            truncate_at_break = min(max_pages, len(page_breaks)) - 1\n            truncate_position = page_breaks[truncate_at_break].end()\n            truncated_content = html_content[:truncate_position]\n            truncated_content = ensure_html_closed(truncated_content)\n            \n            return {\n                'content': truncated_content,\n                'original_pages': original_pages,\n                'final_pages': max_pages,\n                'was_truncated': True,\n                'method': 'PAGE_TAGS'\n            }\n        \n        # Method 2: Fallback to paragraph-based pagination\n        # Find all paragraph opening tags\n        p_pattern = r'<p[\\s>]'\n        paragraphs = list(re.finditer(p_pattern, html_content, re.IGNORECASE))\n        \n        if len(paragraphs) >= paragraphs_per_page:\n            # Calculate pages based on paragraph count\n            total_paragraphs = len(paragraphs)\n            estimated_pages = max(1, total_paragraphs // paragraphs_per_page)\n            \n            if estimated_pages <= max_pages:\n                return {\n                    'content': html_content,\n                    'original_pages': estimated_pages,\n                    'final_pages': estimated_pages,\n                    'was_truncated': False,\n                    'method': f'PARAGRAPH_COUNT ({paragraphs_per_page} per page)'\n                }\n            \n            # Truncate after N paragraphs (where N = max_pages * paragraphs_per_page)\n            target_paragraph_index = max_pages * paragraphs_per_page - 1\n            \n            if target_paragraph_index < len(paragraphs):\n                # Find the end of the target paragraph's closing </p> tag\n                truncate_start = paragraphs[target_paragraph_index].start()\n                \n                # Look for the closing </p> tag after this point\n                closing_p = re.search(r'</p>', html_content[truncate_start:], re.IGNORECASE)\n                \n                if closing_p:\n                    truncate_position = truncate_start + closing_p.end()\n                else:\n                    # If no closing tag found, just use the start position\n                    truncate_position = truncate_start\n                \n                truncated_content = html_content[:truncate_position]\n                truncated_content = ensure_html_closed(truncated_content)\n                \n                return {\n                    'content': truncated_content,\n                    'original_pages': estimated_pages,\n                    'final_pages': max_pages,\n                    'was_truncated': True,\n                    'method': f'PARAGRAPH_COUNT ({paragraphs_per_page} per page)'\n                }\n        \n        # Method 3: Final fallback to character-based estimation\n        chars_per_page = 3000\n        total_chars = len(html_content)\n        estimated_pages = max(1, total_chars // chars_per_page)\n        \n        if estimated_pages <= max_pages:\n            return {\n                'content': html_content,\n                'original_pages': estimated_pages,\n                'final_pages': estimated_pages,\n                'was_truncated': False,\n                'method': 'CHARACTER_ESTIMATE'\n            }\n        \n        # Truncate based on character count\n        max_chars = max_pages * chars_per_page\n        truncate_position = min(max_chars, len(html_content))\n        \n        # Try to truncate at a tag boundary\n        tag_end = html_content.rfind('>', 0, truncate_position)\n        if tag_end > truncate_position - 500:\n            truncate_position = tag_end + 1\n        \n        truncated_content = html_content[:truncate_position]\n        truncated_content = ensure_html_closed(truncated_content)\n        \n        return {\n            'content': truncated_content,\n            'original_pages': estimated_pages,\n            'final_pages': max_pages,\n            'was_truncated': True,\n            'method': 'CHARACTER_ESTIMATE'\n        }\n        \n    except Exception as e:\n        print(f\"Error in truncate_to_pages: {str(e)}\")\n        # Return original content on error\n        return {\n            'content': html_content if isinstance(html_content, str) else '',\n            'original_pages': 0,\n            'final_pages': 0,\n            'was_truncated': False\n        }\n\n\n\ndef ensure_html_closed(html_content):\n    \"\"\"\n    Ensure HTML has proper closing tags\n    \"\"\"\n    # Close common unclosed tags\n    open_tags = []\n    tag_pattern = r'<(/?)(\\w+)[^>]*>'\n    \n    for match in re.finditer(tag_pattern, html_content):\n        is_closing = match.group(1) == '/'\n        tag_name = match.group(2).lower()\n        \n        # Skip self-closing tags\n        if tag_name in ['br', 'hr', 'img', 'input', 'meta', 'link']:\n            continue\n            \n        if is_closing:\n            if open_tags and open_tags[-1] == tag_name:\n                open_tags.pop()\n        else:\n            open_tags.append(tag_name)\n    \n    # Add closing tags for any unclosed tags\n    closing_tags = ''.join([f'</{tag}>' for tag in reversed(open_tags)])\n    \n    return html_content + closing_tags\n\n\n# Execute the truncation and upload\nprocessed, truncated, uploaded = truncate_and_save_documents()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c05860c-2d8f-48be-85e2-39305b9ab298",
   "metadata": {
    "language": "python",
    "name": "get_sec_company_names"
   },
   "outputs": [],
   "source": "import requests                                                                                                                                                                  \nimport pandas as pd                                                                                                                                                              \n                                                                                                                                                                                \n# Fetch SEC's company ticker mapping                                                                                                                                             \nurl = \"https://www.sec.gov/files/company_tickers.json\"                                                                                                                           \nheaders = {                                                                                                                                                                      \n    'User-Agent': 'YourCompany your.email@company.com'  # Replace with your info                                                                                                 \n}                                                                                                                                                                                \n                                                                                                                                                                                \nresponse = requests.get(url, headers=headers)                                                                                                                                    \ncompany_data = response.json()                                                                                                                                                   \n                                                                                                                                                                                \n# Convert to DataFrame                                                                                                                                                           \ncompanies_df = pd.DataFrame.from_dict(company_data, orient='index')                                                                                                              \ncompanies_df['cik_str'] = companies_df['cik_str'].astype(str)                                                                                                                    \n                                                                                                                                                                                \n# Rename columns for clarity                                                                                                                                                     \ncompanies_df.rename(columns={                                                                                                                                                    \n    'cik_str': 'cik',                                                                                                                                                            \n    'ticker': 'ticker',                                                                                                                                                          \n    'title': 'company_name'                                                                                                                                                      \n}, inplace=True)                                                                                                                                                                 \n                                                                                                                                                                                \nprint(f\"Loaded {len(companies_df)} companies\")                                                                                                                                   \ndisplay(companies_df[['cik', 'ticker', 'company_name']].head(10))                                                                                                                \n                                                                                                                                                                                \n# Create Snowflake table with company mappings                                                                                                                                   \n                                                                                                                                          \nsession.create_dataframe(\n    companies_df[['cik', 'ticker', 'company_name']].rename(columns=str.upper).reset_index(drop=True)\n).write.mode('overwrite').save_as_table('sec_company_names')\n\nprint(\"✓ Company names saved to sec_company_names table\")   ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11f9c090-db05-439a-a484-3737b38e0a1b",
   "metadata": {
    "language": "python",
    "name": "sec_company_names"
   },
   "outputs": [],
   "source": "import requests\nimport pandas as pd\nimport time\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport json\n\ndef get_company_description(cik, headers, max_retries=3):\n    \"\"\"\n    Fetch company description from SEC EDGAR API\n    \"\"\"\n    # Format CIK with leading zeros (10 digits)\n    formatted_cik = str(cik).zfill(10)\n    url = f\"https://data.sec.gov/submissions/CIK{formatted_cik}.json\"\n    \n    for attempt in range(max_retries):\n        try:\n            response = requests.get(url, headers=headers)\n            \n            # Handle rate limiting\n            if response.status_code == 429:\n                time.sleep(1)  # Wait 1 second for rate limiting\n                continue\n                \n            if response.status_code == 200:\n                data = response.json()\n                # Extract business description and other metadata\n                return {\n                    'cik': cik,\n                    'business_description': data.get('description', ''),\n                    'sic': data.get('sic', ''),\n                    'sicDescription': data.get('sicDescription', ''),\n                    'fiscalYearEnd': data.get('fiscalYearEnd', ''),\n                    'stateOfIncorporation': data.get('stateOfIncorporation', '')\n                }\n            else:\n                return {\n                    'cik': cik, \n                    'business_description': '', \n                    'sic': '', \n                    'sicDescription': '', \n                    'fiscalYearEnd': '', \n                    'stateOfIncorporation': ''\n                }\n                \n        except Exception as e:\n            if attempt == max_retries - 1:\n                return {\n                    'cik': cik, \n                    'business_description': '', \n                    'sic': '', \n                    'sicDescription': '', \n                    'fiscalYearEnd': '', \n                    'stateOfIncorporation': ''\n                }\n            time.sleep(0.5)\n    \n    return {\n        'cik': cik, \n        'business_description': '', \n        'sic': '', \n        'sicDescription': '', \n        'fiscalYearEnd': '', \n        'stateOfIncorporation': ''\n    }\n\n# Fetch SEC's company ticker mapping\nurl = \"https://www.sec.gov/files/company_tickers.json\"\nheaders = {\n    'User-Agent': 'YourCompany your.email@company.com'  # Replace with your info\n}\n\nresponse = requests.get(url, headers=headers)\ncompany_data = response.json()\n\n# Convert to DataFrame\ncompanies_df = pd.DataFrame.from_dict(company_data, orient='index')\ncompanies_df['cik_str'] = companies_df['cik_str'].astype(str)\n\n# Rename columns for clarity\ncompanies_df.rename(columns={\n    'cik_str': 'cik',\n    'ticker': 'ticker',\n    'title': 'company_name'\n}, inplace=True)\n\nprint(f\"Loaded {len(companies_df)} companies\")\n\n# Get company descriptions using threading for efficiency\nprint(\"Fetching company descriptions from SEC EDGAR API...\")\nprint(\"This may take several minutes due to API rate limits...\")\n\ndescriptions = []\nwith ThreadPoolExecutor(max_workers=10) as executor:  # Limit concurrent requests\n    # Submit all tasks\n    future_to_cik = {\n        executor.submit(get_company_description, cik, headers): cik \n        for cik in companies_df['cik'].tolist()\n    }\n    \n    # Process completed tasks\n    completed = 0\n    for future in as_completed(future_to_cik):\n        result = future.result()\n        descriptions.append(result)\n        completed += 1\n        \n        # Progress update every 100 companies\n        #if completed % 100 == 0:\n            #print(f\"  Progress: {completed}/{len(companies_df)} companies processed\")\n\n# Convert descriptions to DataFrame and merge\ndescriptions_df = pd.DataFrame(descriptions)\ncompanies_enhanced_df = companies_df.merge(descriptions_df, on='cik', how='left')\n\n# Display sample of enhanced data\nprint(f\"\\nEnhanced dataset with {len(companies_enhanced_df)} companies\")\ndisplay(companies_enhanced_df[['cik', 'ticker', 'company_name', 'business_description', 'sicDescription']].head(5))\n\n# Create Snowflake table with enhanced company data\nfinal_df = companies_enhanced_df[[\n    'cik', 'ticker', 'company_name', 'business_description', \n    'sic', 'sicDescription', 'fiscalYearEnd', 'stateOfIncorporation'\n]]\n\nsession.create_dataframe(\n    final_df.rename(columns=str.upper).reset_index(drop=True)\n).write.mode('overwrite').save_as_table('sec_company_names')\n\nprint(\"✓ Enhanced company data saved to sec_company_names table\")\nprint(f\"  New columns: CIK, TICKER, COMPANY_NAME, BUSINESS_DESCRIPTION, SIC, SICDESCRIPTION, FISCALYEAREND, STATEOFINCORPORATION\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a53f3291-43a2-4d09-b6d3-2090b8cd1c40",
   "metadata": {
    "language": "sql",
    "name": "create_s1_docs_table"
   },
   "outputs": [],
   "source": "USE ROLE SYSADMIN;\n\nCREATE OR REPLACE TABLE sec_s1_documents (\n    file_path VARCHAR,\n    trunc_file_path VARCHAR,\n    company_cik number,\n    accession_number VARCHAR,\n    company_name VARCHAR,\n    sicdescription VARCHAR,\n    filing_date VARCHAR,\n    raw_content VARCHAR,\n    upload_date TIMESTAMP_NTZ DEFAULT CURRENT_TIMESTAMP()\n);\n\n-- Enable change tracking (required for Cortex Search)                                                                                                                           \nALTER TABLE sec_s1_documents\nSET CHANGE_TRACKING = TRUE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "45e84198-46f8-404d-ab5a-4458ec2858fa",
   "metadata": {
    "language": "sql",
    "name": "load_s1_docs_table"
   },
   "outputs": [],
   "source": " -- load original documents                        \n COPY INTO sec_s1_documents (file_path, company_cik, accession_number, raw_content)    \n FROM (   \n     SELECT \n         metadata$filename,  \n         SPLIT_PART(metadata$filename, '/', 2),\n         SPLIT_PART(metadata$filename, '/', 4),\n         $1\n     FROM @sec_filings_stage \n     --WHERE metadata$filename LIKE '%primary-document.html%'\n )\n FILE_FORMAT = html_single_row_format \n PATTERN = '.*primary-document.html'\n ON_ERROR = CONTINUE;   \n\n -- Update truncated file paths from stage\nMERGE INTO sec_s1_documents AS docs\nUSING (\n    SELECT distinct\n        SPLIT_PART(metadata$filename, '/', 2) AS company_cik,\n        SPLIT_PART(metadata$filename, '/', 4) AS accession_number,\n        metadata$filename AS trunc_file_path\n    FROM @sec_filings_stage\n    WHERE metadata$filename RLIKE '.*primary-document-[0-9]+pg\\.html$'\n) AS stage_data\nON docs.company_cik = CAST(stage_data.company_cik AS NUMBER)\n   AND docs.accession_number = stage_data.accession_number\nWHEN MATCHED THEN\n    UPDATE SET trunc_file_path = stage_data.trunc_file_path;\n\n -- delete older accession_number filings per company\nDELETE FROM sec_s1_documents \nWHERE accession_number NOT IN (\n    SELECT accession_number \n    FROM (\n        SELECT\n            CAST(SPLIT_PART(metadata$filename, '/', 2) as number) AS company_cik,\n            MAX(SPLIT_PART(metadata$filename, '/', 4)) AS accession_number\n        FROM @sec_filings_stage \n        WHERE metadata$filename LIKE '%/primary-document.html%'\n        AND metadata$filename NOT LIKE '%full-submission%'\n        group by 1\n    )\n);\n\n -- go back and add in company name as attribute \nUPDATE sec_s1_documents\nSET company_name = ccnm.company_name, sicdescription = ccnm.sicdescription\nFROM(\n        SELECT DISTINCT\n            CIK,\n            COMPANY_NAME,\n            SICDESCRIPTION\n        FROM sec_company_names\n) ccnm\nWHERE company_cik = ccnm.cik;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "44863f70-ddea-4e76-be07-70663eb01d5e",
   "metadata": {
    "language": "sql",
    "name": "create_ai_extract_procedure"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE extract_s1_info( \n    company_name VARCHAR,  \n    extraction_fields VARCHAR  -- Changed from OBJECT to VARCHAR\n)\nRETURNS OBJECT \nLANGUAGE SQL \nAS  \n$$ \nDECLARE   \n    file_path VARCHAR;\n    extraction_result OBJECT;\n    parsed_fields OBJECT;  -- New variable to hold parsed JSON\nBEGIN\n    -- Parse the JSON string into an OBJECT\n    SELECT PARSE_JSON(:extraction_fields) INTO :parsed_fields;\n\n    -- Get the file path, adding LIMIT to prevent multiple row errors\n    SELECT \n        CASE WHEN trunc_file_path IS NOT NULL THEN trunc_file_path ELSE file_path END \n    INTO :file_path \n    FROM sec_s1_documents \n    WHERE JAROWINKLER_SIMILARITY(:company_name, company_name) > 80\n    LIMIT 1;  -- Added LIMIT for safety\n\n    IF (:file_path IS NULL) THEN                                                                                                                                                  \n        RETURN OBJECT_CONSTRUCT('error', 'No S-1 filing found for: ' || :company_name);                                                                                        \n    END IF; \n\n    -- Use parsed_fields instead of extraction_fields\n    BEGIN\n        SELECT AI_EXTRACT(                                                                                                                                                           \n            file => TO_FILE('@sec_filings_stage', :file_path),\n            responseFormat => :parsed_fields  -- Use the parsed OBJECT\n        ) INTO :extraction_result;\n    EXCEPTION\n        WHEN OTHER THEN\n            RETURN OBJECT_CONSTRUCT('error', 'AI_EXTRACT failed: ' || SQLERRM);\n    END;\n                                                                                                        \n    RETURN :extraction_result;       \nEND;\n$$;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "861346b9-41ca-467a-874b-3250fb7e6656",
   "metadata": {
    "language": "sql",
    "name": "create_semantic_view"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE SEMANTIC VIEW sec_s1_semantic_view\nTABLES (\n    docs as sec_s1_documents\n    PRIMARY KEY (company_cik, accession_number)\n    WITH SYNONYMS = ('s1 documents', 'sec filings', 'registration statements')\n    COMMENT = 'Core table containing SEC S-1 filing documents',\n    \n    companies as sec_company_names  \n    PRIMARY KEY (cik)\n    WITH SYNONYMS = ('company info', 'tickers', 'company mappings')\n    COMMENT = 'Additional company information from SEC Edgar Database'\n)\nRELATIONSHIPS (\n    docs (company_cik) REFERENCES companies (cik)\n)\nFACTS (\n    PUBLIC docs.raw_content AS raw_content\n    WITH SYNONYMS = ('filing content', 'document text', 'full text')\n    COMMENT = 'Full text content of the S-1 filing document for AI extraction',\n    \n    PUBLIC docs.file_path AS file_path\n    WITH SYNONYMS = ('document path', 'storage path')\n    COMMENT = 'Original document file path in storage',\n    \n    PUBLIC docs.trunc_file_path AS trunc_file_path\n    WITH SYNONYMS = ('short path', 'truncated path')\n    COMMENT = 'Truncated document file path (first 100 pages) for faster processing'\n)\nDIMENSIONS (\n    PUBLIC docs.company_cik AS company_cik\n    WITH SYNONYMS = ('cik', 'central index key', 'company id')\n    COMMENT = 'Central Index Key - unique SEC identifier for each company',\n    \n    PUBLIC docs.accession_number AS accession_number\n    WITH SYNONYMS = ('filing number', 'document id', 'accession')\n    COMMENT = 'SEC accession number - unique identifier for each filing',\n    \n    PUBLIC docs.company_name AS company_name\n    WITH SYNONYMS = ('company', 'business name', 'corporation')\n    COMMENT = 'Official company name as registered with SEC',\n\n    PUBLIC docs.sicdescription AS sicdescription\n    WITH SYNONYMS = ('company sic', 'company description', 'business type')\n    COMMENT = 'Official SIC business description of the company',\n    \n    PUBLIC companies.ticker AS ticker\n    WITH SYNONYMS = ('stock symbol', 'trading symbol')\n    COMMENT = 'Stock ticker symbol for the company'\n    \n)\nCOMMENT = 'Semantic view for SEC S-1 filing documents analysis and AI extraction. Enables natural language queries about company filings, IPO documents, risk factors, and document characteristics.';",
   "execution_count": null
  }
 ]
}