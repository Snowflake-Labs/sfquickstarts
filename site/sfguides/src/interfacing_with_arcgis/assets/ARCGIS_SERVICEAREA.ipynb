{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "cgflu6fgzh2o4oul5hxk",
   "authorId": "433832649156",
   "authorName": "VSEKAR",
   "authorEmail": "venkatesh.sekar@snowflake.com",
   "sessionId": "470e167e-6791-4b55-ad71-48b1f0a323b1",
   "lastEditTime": 1742860403102
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a8480d-295b-471f-9e8c-c099df3d09c5",
   "metadata": {
    "name": "md_overview",
    "collapsed": false
   },
   "source": "# Calculating ServiceArea using ArcGIS Location Services\n\nA service area, also known as an isochrone, is a polygon that represents the distance that can be reached when driving or walking on a street network. This type of analysis is common in real estate search or determining the driving proximity to schools, businesses, or other facilities. For example, you can create a drive time polygon that represents how far you can drive in any direction from the center of a city in 20 minutes.\n\nYou can use service areas to build applications that:\n\n- Visualize and measure the accessibility of locations that provide some kind of service. For example, a three-minute drive-time polygon around a grocery store can determine which residents are able to reach the store within three minutes and are thus more likely to shop there.\n\n- By generating multiple service areas around one or more locations that can show how accessibility changes with an increase in travel time or travel distance. It can be used, for example, to determine how many hospitals are within 5, 10, and 15 minute drive times of schools.\n\n- When creating service areas based on travel times, the service can make use of traffic data, which can influence the area that can be reached during different times of the day.\n\n### What is ArcGIS Location Services?\n\nThe [ArcGIS Location Services](https://developers.arcgis.com/documentation/mapping-and-location-services/) are services hosted by Esri that provide geospatial functionality and data for building mapping applications. You can use the service APIs to display maps, access basemaps styles, visualize data, find places, geocode addresses, find optimized routes, enrich data, and perform other mapping operations. The services also support advanced routing operations such as fleet routing, calculating service areas, and solving location-allocation problems. To build applications you can use ArcGIS Maps SDKs, open source libraries, and scripting APIs.\n\n### What Youâ€™ll Learn \n\nIn this notebook you will be go over the steps for defining an UDF that invokes the Service Area endpoint, part of the ArcGIS Location Services. And perform the calculation for a set of warehouse addresses.\n\n### Packages\n\nThis notebook requires the following packages to be added:\n- pydeck"
  },
  {
   "cell_type": "markdown",
   "id": "06f586f1-4d36-4b0e-b0f6-c6138fcaba35",
   "metadata": {
    "name": "md_initialization",
    "collapsed": false
   },
   "source": "Let us start by configuring the variables as per your environment. These are:\n\n- ESRI_API_KEY: The API key using which we can authenticate with the ArcGIS Location services api endpoints.\n- DB_ROLE: The role that will be used to create and own the various objects For the purpose of the demo, I am going to keep it simple as to just use the ACCOUNTADMIN role.\n- ARCGIS_DB: The database in which the tables, views, udf where the assets will be created.\n- ARCGIS_DB_SCHEMA: A schema within the above database, to keep it simple, I am going to be using the default public schema.\n\nOnce you have configured these, run the cell. This cell will establish a snowflake session."
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "initialization",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n#-----------------------\n# Populate the below variables as per the environment\nESRI_API_KEY = 'AAPT85fOqywZsicJupSmVSCGrilHxUHWit8PKfw3XzfCwgtIjRzTYW7tM8tB7B0FgmpUwJz6Ql27EQeS-'\nDB_ROLE = 'accountadmin'\nARCGIS_DB = 'arcgis_db'\nARCGIS_DB_SCHEMA = 'public'\n\n#-----------------------\nsession.use_role(DB_ROLE)\nsession.use_database(ARCGIS_DB)\nsession.use_schema(ARCGIS_DB_SCHEMA)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fbd1ffcc-16e8-4761-a68d-9bb2bef42e61",
   "metadata": {
    "name": "md_servicearea_udf",
    "collapsed": false
   },
   "source": "### Defining the Servicearea UDF\n\nThe UDF will be reaching out to ARCGIS Location servicearea endpoint. It will also be needing the API key to access this. Hence we define the following objects:\n - secret: arcgis_api_key\n - network rule: nw_arcgis_api\n - external access integration: eai_arcgis_api\n - internal stage: lib_stg to store udf, as we are defining it as permanent\n\n Run the cell below, to create these objects"
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "create_secret",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "sql_stmts = [\n    f'use role {DB_ROLE}'\n    ,f'use schema {ARCGIS_DB}.{ARCGIS_DB_SCHEMA}'\n    \n#   Create secret for holding ArcGis API Key\n#   Ref: https://docs.snowflake.com/en/sql-reference/sql/create-secret\n    ,f'''create or replace secret arcgis_api_key\n        type = generic_string\n        secret_string = '{ESRI_API_KEY}'\n        comment = 'api key used for connecting to arcgis rest api endpoint.'\n  '''\n\n#   Create network rule\n    ,f'''create or replace network rule {ARCGIS_DB}.{ARCGIS_DB_SCHEMA}.nw_arcgis_api\n        mode = egress\n        type = host_port\n        value_list = ('*.arcgis.com')\n        comment = 'Used for ESRI arcgis needs' '''\n\n#   Create external access integration\n    ,f''' create or replace external access integration eai_arcgis_api\n        allowed_network_rules = (nw_arcgis_api)\n        allowed_authentication_secrets = (arcgis_api_key)\n        enabled = true\n        comment = 'Used for ESRI arcgis needs' '''\n\n#   Create internal stage\n    ,f''' create stage if not exists {ARCGIS_DB}.{ARCGIS_DB_SCHEMA}.lib_stg\n        encryption = (type = 'SNOWFLAKE_FULL' ) '''\n  \n]\nfor sql_stmt in sql_stmts:\n  session.sql(sql_stmt).collect()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7b6cfaad-1f61-4534-a0c5-9348f93e15ea",
   "metadata": {
    "name": "md_define_udf",
    "collapsed": false
   },
   "source": "We define a Snowpark vectorized UDTF, that will be invoking the [ServiceArea API](https://developers.arcgis.com/rest/routing/serviceArea-service-direct/). \n\nAs you would see the API can take a batch of geolocations and does require the input to be formatted in a specific format, we will be formatting the input accordingly.\n\nWhile the service area has various optional parameter options, for this demo to keep it simple I am going to be using mainly the 'defaultBreaks' option. In this demo I am using 3 breaks (15, 30, 45). As a result the output from the API would also contain 3 service area, one for each breaks. Hence the implementation is UDTF rather than an UDF.\n\nThe response will contain the service area for each of the input location/facilities; hence we will be deconstructing the response into indivual service area/facility combination and returning them as the result."
  },
  {
   "cell_type": "code",
   "id": "0c05ca33-fd43-4976-b7c5-086ddaa6074e",
   "metadata": {
    "language": "python",
    "name": "define_servicearea_vudtf",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# define service area udf\n\nimport requests\nimport json\nimport snowflake.snowpark.functions as F\nimport snowflake.snowpark.types as T\nimport pandas as pd\nimport copy\n\nclass ArcGIS_ServiceArea:\n  def __init__(self):\n    self.api_endpoint = 'https://route.arcgis.com/arcgis/rest/services/World/ServiceAreas/NAServer/ServiceArea_World/solveServiceArea'\n\n  def _invoke_service_area_api(self, p_access_token ,p_facilities ,p_extra_params = {}):\n    _headers = {\n      'Authorization': f'Bearer {p_access_token}',\n      'Content-Type': 'application/x-www-form-urlencoded'\n    }\n    _params = {\n      'f' : 'json'\n      # ,'token' : p_access_token\n      ,'facilities' : json.dumps(p_facilities)\n      ,**p_extra_params # dictionary unpacking\n    }\n\n    _response = requests.post(self.api_endpoint\n                  ,data = _params \n                  ,headers = _headers)\n    _response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n    return _response\n\n  def _build_facilities_payload(self, df):\n    '''\n    This function formats the input dataframe as per the API spec\n    https://developers.arcgis.com/rest/routing/serviceArea-service-direct/#facilities\n    '''\n    # NOTE: Ensure Objectid is an integer, preferable sequence. otherwise the responses becomes invalid\n    _features = []\n    _facilityid_to_address_id_map = {}\n    for idx ,row in df.iterrows():\n      _facilityid_to_address_id_map[idx + 1] = row['address_id']\n      _f = {\n        \"attributes\": {\n          \"ObjectID\" : idx + 1\n          ,\"Name\" : row['address_id']\n        },\n        \"geometry\": {\n          \"x\": row['x']\n          ,\"y\": row['y']\n        }\n      }\n      _features.extend([_f])\n    _facilities = {\n      'features' : _features\n    }\n    return (_facilityid_to_address_id_map ,_facilities)\n\n  def _remap_response(self, p_facilityid_to_address_id_map ,p_response):\n    '''\n    This function remaps the response based on the objectid to address_id map\n    '''\n    _remapped_response = []\n    for _f in p_response['saPolygons']['features']:\n      _object_id = _f['attributes']['ObjectID']\n      _facility_id = _f['attributes']['FacilityID']\n      _address_id = p_facilityid_to_address_id_map.get(_facility_id, '-1')\n\n      # Make a copy of the input response  \n      _r_copy = copy.deepcopy( p_response )\n      _r_copy['saPolygons']['features'] = [_f]\n\n      _remapped_response.extend([\n        {\n          'address_id' : _address_id\n          ,'object_id' : _object_id  \n          ,'servicearea_response' : _r_copy\n            \n        }\n      ])\n    return _remapped_response\n\n  def end_partition(self, df: T.PandasDataFrame[str,float ,float]) -> T.PandasDataFrame[str ,int ,dict]:\n    import _snowflake # This is a private module that will be available during runtime.\n\n    # Rename the columns\n    df.columns = ['address_id' ,'x','y']\n\n    # Extract the api from the secret\n    _access_token = _snowflake.get_generic_secret_string('esri_api_key')\n\n    _facilityid_to_address_id_map ,_facilities_payload = self._build_facilities_payload(df)\n    # _travel_mode_payload = self._get_travel_mode()\n    _additional_params = {\n        'defaultBreaks' : '15,30,45'\n        ,'preserveObjectID' : True\n      }\n    \n    _response_payload = self._invoke_service_area_api(_access_token \n      ,_facilities_payload ,_additional_params)\n    _response_payload.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n\n    # To store the formatted response based on defaultBreaks\n    _vudtf_response = []\n\n    # If the response is not 200, then we will just return the \n    # content asis for each input record, so that user can be aware of the error.\n    # Another option is to log the event and raise an exception\n    if _response_payload.status_code != 200:\n    #if False: # For now, we will always process the response\n      for idx ,row in df.iterrows():\n        _vudtf_response.extend([\n            _response_payload.json()\n        ])\n    else:\n        _vudtf_response = self._remap_response(\n          _facilityid_to_address_id_map, _response_payload.json())\n      \n    # Convert the list of geocoded values to a pandas dataframe\n    r_df = pd.DataFrame(_vudtf_response)  \n    return r_df\n\n  end_partition._sf_vectorized_input = pd.DataFrame\n\n# --------------------------------------------------------------------------------------------\n# Ensure the current role and schema context\nsession.use_role(DB_ROLE)\nsession.use_database(ARCGIS_DB)\nsession.use_schema(ARCGIS_DB_SCHEMA)\n\n# Register the snowpark UDTF\n# Ref : https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.functions.pandas_udtf\nfn_servicearea_for_addresses = F.pandas_udtf(\n    ArcGIS_ServiceArea,\n\toutput_schema = ['address_id' ,'object_id' ,'servicearea_response'],\n\t\tinput_types = [\n            T.PandasDataFrameType([T.StringType() ,T.FloatType() ,T.FloatType()])\n        ], \n\t\tinput_names = ['\"address_id\"' ,'\"x\"' ,'\"y\"'],\n    name = 'arcgis_servicearea_for_address_vudtf',\n    replace=True, is_permanent=True, stage_location='@lib_stg',\n    packages=['pandas', 'requests'],\n    external_access_integrations=['eai_arcgis_api'],\n    secrets = {\n        'esri_api_key' : f'{ARCGIS_DB}.{ARCGIS_DB_SCHEMA}.arcgis_api_key'\n    },\n    max_batch_size = 100,\n\t\tcomment = 'UDTF that takes a list of location geocode (latitude and longitutde) and returns the service area/isochrone from this point'\n    )\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e53f70b9-26ae-40c7-9396-28127fe5f173",
   "metadata": {
    "name": "md_define_udf_coordinates_extraction",
    "collapsed": false
   },
   "source": "The raw response and geometries returned from the API would not be usable by various mapping libraries, hence we need to extract geometries and also reformat to geojson format.\nTo do this, we define an UDF."
  },
  {
   "cell_type": "code",
   "id": "bd47a7fb-b3c5-480a-94a7-1d17625b33d9",
   "metadata": {
    "language": "python",
    "name": "define_udf_for_coordinates_extraction"
   },
   "outputs": [],
   "source": "\ndef _convert_sapolygons_geometry_to_geojson(p_response: dict):\n    # Random point\n    _geojson =  {\n        \"coordinates\": [\n          -87.942989020543,\n          46.259970794197244\n        ],\n        \"type\": \"Point\"\n      }\n    \n    if 'saPolygons' not in p_response:\n        return _geojson\n\n    elif 'features' not in p_response['saPolygons']:\n        return _geojson\n\n    elif 'geometry' not in p_response['saPolygons']['features'][0]:\n        return _geojson\n\n    _g = p_response['saPolygons']['features'][0]['geometry']\n    _rings = _g['rings']\n    _geojson =  {\n        \"type\": \"MultiPolygon\"\n        ,\"coordinates\": [_rings]\n    }\n\n    return _geojson\n\ndef _extract_sapolygons_as_geojson(df :pd.DataFrame):\n\n    _geojsons = []\n    for idx ,row in df.iterrows():\n        _sa_response = row[0] \n        _g = _convert_sapolygons_geometry_to_geojson(_sa_response)\n        _geojsons.append(_g)\n\n    r_df = pd.Series(_geojsons)\n    return r_df\n\n# Ref : https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/snowpark/api/snowflake.snowpark.functions.pandas_udf\nfn_extract_sapolygons_as_geojson = F.pandas_udf(\n    func = _extract_sapolygons_as_geojson,\n    return_type = T.PandasSeriesType(T.VariantType()),\n    input_types=[T.PandasDataFrameType([T.VariantType()])],\n    name = 'extract_sapolygons_as_geojson',\n    replace=True, is_permanent=True,stage_location='@lib_stg',\n    packages=['snowflake-snowpark-python'],\n    max_batch_size = 100\n)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9cc85aff-52d2-4bb3-8a13-4ae332828fef",
   "metadata": {
    "name": "md_demo_dataset",
    "collapsed": false
   },
   "source": "---\n\n## Demo data and sample execution\n\nWe now define some sample datasets and invoke the UDF's to invoke the service and extract the corresponding polygon geometries into its own specific columns.\n\nWhen viewing the resulting table in ArcGISPro, a table with multiple geometry columns would not work. Hence we will be defining views on the table warehouses_serviceareas."
  },
  {
   "cell_type": "code",
   "id": "a18fea3b-bdca-42d7-9454-fd337e4c4fe2",
   "metadata": {
    "language": "sql",
    "name": "create_data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "\n-- 1. Create tables\n\ncreate or replace transient table store_warehouses (\n    address_id varchar\n    ,address varchar\n    ,latitude float\n    ,longitude float\n);\n\ncreate or replace transient table warehouses_serviceareas (\n    address_id varchar\n    ,object_id integer\n    ,address varchar\n    ,address_pt geography\n    ,servicearea_response variant\n    ,sa_feature_attributes variant\n    ,from_break int\n    ,to_break int\n    ,servicearea_isochrone geography\n);\n\n\n-- 1.1. Create a view for ArcGISPro\ncreate or replace view vw_warehouses_serviceareas_serviceareas_feature as\nselect * exclude(address_pt ,servicearea_response)\nfrom warehouses_serviceareas\n;\n\ncreate or replace view vw_warehouses_serviceareas_address_feature as\nselect distinct address_id, address ,address_pt\nfrom warehouses_serviceareas\n;\n\n-- 1.2 Add search optimization for improve speed \nalter table warehouses_serviceareas\n    add search optimization on geo(servicearea_isochrone);\n\nalter table warehouses_serviceareas\n    add search optimization on geo(address_pt);\n\n\n-- 2. Ingest sample data\ninsert into store_warehouses values\n('d56f6bc1328ab963f1462cb2d3830eb7','710 , Picaso Lane  ,Chico ,CA ,95926',\t39.7474427,\t-121.8656711)\n,('d56f6bd199be0c5cea4f2461b3a391c4','Stellar Lp   ,Myrtle Beach ,SC ,29577',\t\t33.6886227,\t-78.9451313)\n,('d56f6bd440a069311692b5a400098d0c','6816 , Southpoint Pkwy I   ,Jacksonville ,FL ,32216',\t\t30.2575787,\t-81.5890935)\n,('d56f6bd9cb9d8d864647b4c86dab4b77','502 ,E Harris Street  ,Savannah ,GA ,31401',\t\t32.07264,\t-81.0882603)\n,('d56f6c01502080a226ad897907e47bb0','1250 , Welch Road  ,Commerce Township ,MI ,48390',\t\t42.545633,\t-83.4578007)\n,('d56f6c03b7dd6a972ea5b5b5b2cd8787','3 , Carlisle Street  ,Lancaster ,NY ,14086',\t\t42.9291668,\t-78.6594399)\n,('d56f6c1bfa0618803d375c704650b5d4','25 ,E Delaware Parkway  ,Villas ,NJ ,08251',\t\t39.0291768,\t-74.932413)\n,('d56f6c29924c88d893745ca5c97b28d5','65432 , 73rd Street  ,Bend ,OR ,97703',\t\t44.1755873,\t-121.2557281)\n,('d56f6c3a921a017d32f5290f370a8e4e','1686 , Windriver Road  ,Clarksville ,TN ,37042',\t\t36.6111711,\t-87.3417787)\n,('d56f6c628607c455e2753868907af75e','152 , Covey Rise Circle  ,Clarksville ,TN ,37043',\t\t36.5659107,\t-87.2297272)\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "39f9e1cc-d43e-4bc3-8f1f-a60d4a9b0d57",
   "metadata": {
    "language": "sql",
    "name": "demonstrate_servicearea_and_store",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- 3 invoke the UDTF to calculate the servicearea\n\nselect t.*\nfrom store_warehouses as f\n    ,table(arcgis_servicearea_for_address_vudtf(address_id ,longitude ,latitude) \n        over (partition by 1) ) as t\n\n-- choose only those records to which the calculation was not done previously\nwhere f.address_id not in (\n    select distinct address_id from warehouses_serviceareas\n)\n;\n\n-- 3.1 insert records into the serviceareas table \nmerge into warehouses_serviceareas as t\nusing (\n        select *\n        from table(result_scan(last_query_id()))\n    ) as s\non t.address_id = s.address_id\n    and t.object_id = s.object_id\nwhen not matched then insert\n    (address_id ,object_id ,servicearea_response)\n    values(s.address_id ,s.object_id ,s.servicearea_response)\n;\n\n-- sample output\nselect *\nfrom warehouses_serviceareas\nlimit 1\n;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "622a4768-3b57-42ea-8031-233108318f9f",
   "metadata": {
    "name": "md_geojson_conversion",
    "collapsed": true
   },
   "source": "----- \n"
  },
  {
   "cell_type": "code",
   "id": "10da0cdf-3152-45e4-8598-4ad33e661fc9",
   "metadata": {
    "language": "sql",
    "name": "run_conversion_and_enrichment"
   },
   "outputs": [],
   "source": "-- 4. Update feature attributes\nupdate warehouses_serviceareas as l \nset\n    sa_feature_attributes = l.servicearea_response:\"saPolygons\":features[0]:attributes\n    ,address = r.address\n    ,address_pt = st_makepoint(r.longitude ,r.latitude)\n    ,from_break = l.servicearea_response:\"saPolygons\":features[0]:attributes:\"FromBreak\"::int \n    ,to_break = l.servicearea_response:\"saPolygons\":features[0]:attributes:\"ToBreak\"::int\nfrom store_warehouses as r\nwhere r.address_id = l.address_id\n;\n\n\n-- 4. convert the sa response and store it as geojson\nupdate warehouses_serviceareas set\n    servicearea_isochrone = try_to_geometry(\n            extract_sapolygons_as_geojson(servicearea_response)\n            ,4326 ,true\n        )\n;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7240822e-7b98-4a1a-b790-69466ae05563",
   "metadata": {
    "language": "sql",
    "name": "sample_output"
   },
   "outputs": [],
   "source": "select *\nfrom vw_warehouses_serviceareas_serviceareas_feature\nlimit 10;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eab369e1-4cf7-4254-a3be-d5ef3ef37321",
   "metadata": {
    "name": "md_visualization",
    "collapsed": false
   },
   "source": "---\n## Visualization\n\nWhile the dataset can be visualized using ArcGIS Pro; for a quick visualization we can do this using the PyDeck libraries as below. "
  },
  {
   "cell_type": "code",
   "id": "335ad653-56e4-4c27-b7ff-8884e427c305",
   "metadata": {
    "language": "python",
    "name": "fetch_data"
   },
   "outputs": [],
   "source": "import pydeck as pdk\nimport json\n\n# The PyDeck does not have capability to handle GeoJson specifically with MultiPolygon\n# Hence we need to extract the coordinates from its structure.\n# Following that we have to flatten out the MultiPolygons as it cannot handle the nested\n# nature.\n#\n# This operation can be done in python, but I find it easily doable at much speed when doing\n# it in Snowflake. Hence the below statement does this extraction and flattening operations\n#\n\nsql_stmt = '''\nwith base as (\n    select \n        address_id || '::' || object_id as addr_obj_id\n        ,object_id\n        ,address \n        ,from_break\n        ,to_break\n        ,st_x(address_pt) as lon\n        ,st_y(address_pt) as lat\n        ,st_asgeojson(servicearea_isochrone):type::varchar as geom_type\n        ,st_asgeojson(servicearea_isochrone):coordinates as coordinates\n        \n    from warehouses_serviceareas\n    \n), polygon_coords as (\n    select * \n    from base\n    where geom_type = 'Polygon'\n    \n), multipolygon_coords as (\n    select b.* exclude(coordinates) \n        ,f.value as coordinates\n    from base as b\n        ,lateral flatten(input => coordinates) as f\n    where geom_type = 'MultiPolygon'\n\n)\nselect * \nfrom polygon_coords\nunion all\nselect * \nfrom multipolygon_coords as b\n-- where addr_obj_id like 'd56f6bc1328ab963f1462cb2d3830eb7%'\norder by addr_obj_id\n'''\n\nspdf = session.sql(sql_stmt)\ndf = spdf.limit(50).to_pandas()\n\n# Ensure COORDINATES is parsed to correct data type and not str\ndf['COORDINATES'] = df['COORDINATES'].apply(lambda x: json.loads(x) if isinstance(x, str) else x)\n\ndf.head()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7af78237-c0ee-4d08-bcea-95787d67e0ef",
   "metadata": {
    "language": "python",
    "name": "visualize_on_map",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import random\nimport pydeck as pdk\n\n# Filter the records to the selected address\ntdf = df\n\n# ----\n#   Build initial view\n\n# Take lat/lon from first address\n_lon = tdf['LON'].iloc[0]\n_lat = tdf['LAT'].iloc[0]\n_initial_view_state = pdk.ViewState(\n        latitude= _lat, longitude= _lon,\n        zoom=10, max_zoom=16, \n        pitch=45, bearing=0\n    )\n\n# ----\n#   Build layers\n_deck_layers = []\n\n# For each service area add a fill color Method 1: Using apply() with a lambda function (Recommended)\ndf['sa_fill_color'] = tdf.apply(lambda row: [random.randint(0, 255) ,random.randint(0, 255) ,random.randint(0, 255)], axis=1)\n\n_l = pdk.Layer(\n    \"PolygonLayer\",\n    # data = df['COORDINATES_J'].to_list(), get_polygon='-',\n    data = tdf, get_polygon='COORDINATES',\n    get_fill_color = 'sa_fill_color',\n    # get_fill_color = [random.randint(0, 255) ,random.randint(0, 255) ,random.randint(0, 255)],\n    # get_line_color=[0, 0, 0, 255],\n    pickable=True,\n    auto_highlight=True,\n    # filled=True,\n    # extruded=True,\n    # wireframe=True,\n)\n_deck_layers.append(_l)\n\n\n# Add the address points\n_address_pt_lyr = pdk.Layer(\n            'ScatterplotLayer',\n            data= tdf,\n            get_position='[LON, LAT]',\n            get_color=[0,0,0],\n            get_radius=10,\n            radiusScale=100,\n            pickable=True)\n_deck_layers.append(_address_pt_lyr)\n\n# Build the pydeck map\ntooltip = {\n   \"html\": \"\"\"ADDR : {ADDRESS} FromBreak : {FROMBREAK} OBJ ID: {OBJECT_ID}  \"\"\",\n   \"style\": {\n       \"width\":\"10%\",\n        \"backgroundColor\": \"steelblue\",\n        \"color\": \"white\",\n       \"text-wrap\": \"balance\"\n   }\n}\n\n_map_style= 'mapbox://styles/mapbox/streets-v11'\n\ndeck = pdk.Deck(\n    layers = _deck_layers,\n    map_style = _map_style,\n    initial_view_state= _initial_view_state,\n    tooltip = tooltip\n)\n\n# Visualize the polygons\nst.pydeck_chart(deck)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4d17cc11-7af4-424f-99ef-73c2e8f4b79f",
   "metadata": {
    "name": "md_finished",
    "collapsed": false
   },
   "source": "---\n## Finished!!!"
  }
 ]
}