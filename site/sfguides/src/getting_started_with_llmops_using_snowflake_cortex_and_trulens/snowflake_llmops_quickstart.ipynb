{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with LLMOps using Snowflake Cortex and TruLens\n",
    "\n",
    "By completing this guide, you'll get started with LLMOps by building a RAG by combining [Cortex LLM Functions](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions) and [Cortex Search](https://github.com/Snowflake-Labs/cortex-search?tab=readme-ov-file), and then using [TruLens](https://www.trulens.org/) to add observability and guardrails.\n",
    "\n",
    "Along the way, you will also learn how run TruLens feedback functions with Snowflake Cortex as the [feedback provider](https://www.trulens.org/trulens_eval/api/provider/), and how to [log TruLens traces and evaluation metrics to a Snowflake table](https://www.trulens.org/trulens_eval/tracking/logging/where_to_log/log_in_snowflake/#logging-in-snowflake). Last, we'll show how to use [TruLens guardrails](https://www.trulens.org/trulens_eval/guardrails/) for filtering retrieved context and reducing hallucination.\n",
    "\n",
    "Here is a summary of what you will be able to learn in each step by following this quickstart:\n",
    "\n",
    "- **Setup Environment**: Create a session to use Snowflake Cortex capabilities.\n",
    "- **Cortex Complete**: Use Cortex `Complete()` to call Mistral Large.\n",
    "- **Add Data**: Load and preprocess raw documentation from GitHub, and load to Cortex Search.\n",
    "- **Search**: Search over the data loaded to Cortex Search.\n",
    "- **Create a RAG**: Create a RAG with Cortex Search and Complete and add TruLens instrumentation.\n",
    "- **Feedback Functions**: Add context relevance, groundedness and answer relevance evaluations to the RAG.\n",
    "- **Application Testing**: Understand the performance of your RAG across a test set.\n",
    "- **Guardrails**: Add context filter guardrails to reduce hallucinations.\n",
    "- **Measure Improvement**: See the improved evaluation results after adding guardrails.\n",
    "\n",
    "### What are Cortex LLM Functions?\n",
    "\n",
    "Snowflake Cortex gives you instant access to industry-leading large language models (LLMs) trained by researchers at companies like Mistral, Reka, Meta, and Google, including Snowflake Arctic, an open enterprise-grade model developed by Snowflake.\n",
    "\n",
    "### What is Cortex Search?\n",
    "\n",
    "Cortex Search enables low-latency, high-quality search over your Snowflake data. Cortex Search powers a broad array of search experiences for Snowflake users including Retrieval Augmented Generation (RAG) applications leveraging Large Language Models (LLMs).\n",
    "\n",
    "### What is TruLens?\n",
    "\n",
    "[TruLens](https://www.trulens.org/) is a library for tracking and evaluating Generative AI applications. It provides an extensive set of feedback functions to systematically measure the quality of your LLM based applications. It also traces the internal steps of your application, and allows you to run feedback functions on any internal step. Feedback function results can be examined in a TruLens dashboard, or used at runtime as guardrails.\n",
    "\n",
    "### What You Will Learn\n",
    "- How to build a RAG with Cortex Search and Cortex LLM Functions.\n",
    "- How to use TruLens Feedback Functions and Tracing.\n",
    "- How to log TruLens Evaluation Results and Traces to Snowflake.\n",
    "- How to use TruLens Feedback Functions as Guardrails to reduce hallucination.\n",
    "\n",
    "### What You Will Build\n",
    "- A retrieval-augmented generation (RAG) app\n",
    "- An LLMOps pipeline\n",
    "- Context filter guardrails\n",
    "\n",
    "### Prerequisites\n",
    "- A Snowflake account with Cortex LLM Functions and Cortex Search enabled.  If you do not have a Snowflake account, you can register for a [free trial account](https://signup.snowflake.com/?utm_cta=quickstarts_&_fsi=yYZEVo4S&_fsi=yYZEVo4S).\n",
    "- A Snowflake account login with ACCOUNTADMIN role. If you have this role in your environment, you may choose to use it. If not, you will need to 1) Register for a free trial, 2) Use a different role that has the ability to create database, schema, tables, stages, tasks, user-defined functions, and stored procedures OR 3) Use an existing database and schema in which you are able to create the mentioned objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a new SQL worksheet, run the following SQL commands to create the [warehouse](https://docs.snowflake.com/en/sql-reference/sql/create-warehouse.html), [database](https://docs.snowflake.com/en/sql-reference/sql/create-database.html) and [schema](https://docs.snowflake.com/en/sql-reference/sql/create-schema.html).\n",
    "\n",
    "```sql\n",
    "USE ROLE ACCOUNTADMIN;\n",
    "\n",
    "CREATE OR REPLACE WAREHOUSE LLMOPS_S WAREHOUSE_SIZE=SMALL;\n",
    "CREATE OR REPLACE DATABASE LLMOPS_DB;\n",
    "CREATE OR REPLACE SCHEMA LLMOPS_SCHEMA;\n",
    "\n",
    "USE LLMOPS_DB.LLMOPS_SCHEMA;\n",
    "```\n",
    "\n",
    "For this quickstart, you will need your Snowflake credentials and a GitHub PAT Token ready. \n",
    "\n",
    "In your development environment, create a new `.env` file that looks like this with your details filled in:\n",
    "\n",
    "```bash\n",
    "# Loading data from github\n",
    "GITHUB_TOKEN=\n",
    "\n",
    "# Snowflake details\n",
    "SNOWFLAKE_USER=\n",
    "SNOWFLAKE_USER_PASSWORD=\n",
    "SNOWFLAKE_ACCOUNT=\n",
    "SNOWFLAKE_DATABASE=\n",
    "SNOWFLAKE_SCHEMA=\n",
    "SNOWFLAKE_WAREHOUSE=\n",
    "SNOWFLAKE_ROLE=\n",
    "SNOWFLAKE_CORTEX_SEARCH_SERVICE=\n",
    "```\n",
    "\n",
    "Next create a new conda environment and install the packages required with the following commands in your terminal:\n",
    "\n",
    "```bash\n",
    "conda create -n getting_started_llmops python=3.12\n",
    "conda activate getting_started_llmops\n",
    "conda install -c https://repo.anaconda.com/pkgs/snowflake snowflake-snowpark-python snowflake-ml-python notebook ipykernel\n",
    "pip install trulens-eval llama-index llama-index-embeddings-huggingface llama-index-readers-github snowflake-sqlalchemy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have an environment with the right packages installed, we can load our credentials and set our Snowflake connection in a jupyter notebook notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from snowflake.snowpark.session import Session\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "connection_details = {\n",
    "  \"account\":  os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "  \"user\": os.environ[\"SNOWFLAKE_USER\"],\n",
    "  \"password\": os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "  \"role\": os.environ[\"SNOWFLAKE_ROLE\"],\n",
    "  \"database\": os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "  \"schema\": os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "  \"warehouse\": os.environ[\"SNOWFLAKE_WAREHOUSE\"]\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_details).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Cortex Complete\n",
    "\n",
    "With the session set, we have what need to call a Snowflake Cortex LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.cortex import Complete\n",
    "\n",
    "print(Complete(\"mistral-large\", \"how do snowflakes get their unique patterns?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cortex Search\n",
    "\n",
    "Next, we'll turn to the retrieval component of our RAG and set up Cortex Search.\n",
    "\n",
    "This requires three steps:\n",
    "\n",
    "1. Read and preprocess unstructured documents.\n",
    "2. Embed the cleaned documents with Arctic Embed.\n",
    "3. Call the Cortex search service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and preprocess unstructured documents\n",
    "\n",
    "For this example, we want to load Cortex Search with documentation from Github about a popular open-source library, Streamlit. To do so, we'll use a GitHub data loader available from LlamaHub.\n",
    "\n",
    "Here we'll also expend some effort to clean up the text so we can get better search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_index.readers.github import GithubRepositoryReader, GithubClient\n",
    "\n",
    "github_token = os.environ[\"GITHUB_TOKEN\"]\n",
    "client = GithubClient(github_token=github_token, verbose=False)\n",
    "\n",
    "reader = GithubRepositoryReader(\n",
    "  github_client=github_client,\n",
    "  owner=\"streamlit\",\n",
    "  repo=\"docs\",\n",
    "  use_parser=False,\n",
    "  verbose=True,\n",
    "  filter_directories=(\n",
    "    [\"content\"],\n",
    "    GithubRepositoryReader.FilterType.INCLUDE,\n",
    "  ),\n",
    "  filter_file_extensions=(\n",
    "    [\".md\"],\n",
    "    GithubRepositoryReader.FilterType.INCLUDE,\n",
    "  )\n",
    ")\n",
    "\n",
    "documents = reader.load_data(branch=\"main\")\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_up_text(content: str) -> str:\n",
    "  \"\"\"\n",
    "  Remove unwanted characters and patterns in text input.\n",
    "\n",
    "  :param content: Text input.\n",
    "\n",
    "  :return: Cleaned version of original text input.\n",
    "  \"\"\"\n",
    "\n",
    "  # Fix hyphenated words broken by newline\n",
    "  content = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', content)\n",
    "\n",
    "  unwanted_patterns = ['---\\nvisible: false','---', '#','slug:']\n",
    "  for pattern in unwanted_patterns:\n",
    "    content = re.sub(pattern, \"\", content)\n",
    "\n",
    "  # Remove all slugs starting with a \\ and stopping at the first space\n",
    "  content = re.sub(r'\\\\slug: [^\\s]*', '', content)\n",
    "\n",
    "  # normalize whitespace\n",
    "  content = re.sub(r'\\s+', ' ', content)\n",
    "  return content\n",
    "\n",
    "cleaned_documents = []\n",
    "\n",
    "for d in documents:\n",
    "  cleaned_text = clean_up_text(d.text)\n",
    "  d.text = cleaned_text\n",
    "  cleaned_documents.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the documents with Semantic Splitting\n",
    "\n",
    "We'll use Snowflake's Arctic Embed model available from HuggingFace to embed the documents. We'll also use Llama-Index's `SemanticSplitterNodeParser` for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\"Snowflake/snowflake-arctic-embed-m\")\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "  buffer_size=1, breakpoint_percentile_threshold=85, embed_model=embed_model\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the embed model and splitter, we can execute them in an ingestion pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "cortex_search_pipeline = IngestionPipeline(\n",
    "  transformations=[\n",
    "    splitter,\n",
    "  ],\n",
    ")\n",
    "\n",
    "results = cortex_search_pipeline.run(show_progress=True, documents=cleaned_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data to Cortex Search\n",
    "\n",
    "Now that we've embedded our documents, we're ready to load them to Cortex Search.\n",
    "\n",
    "Here we can use the same connection details as we set up for Cortex Complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snowflake.connector\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "  user=connection_details[\"user\"],\n",
    "  password=connection_details[\"password\"],\n",
    "  account=connection_details[\"account\"],\n",
    "  warehouse=connection_details[\"warehouse\"],\n",
    "  database=connection_details[\"database\"],\n",
    "  schema=connection_details[\"schema\"]\n",
    ")\n",
    "\n",
    "conn.cursor().execute(\"CREATE OR REPLACE TABLE streamlit_docs(doc_text VARCHAR)\")\n",
    "for curr in tqdm(results):\n",
    "  conn.cursor().execute(\"INSERT INTO streamlit_docs VALUES (%s)\", curr.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Cortex Search Service\n",
    "\n",
    "First we need to create a Cortex Search Service in Snowflake. To do so, you can opena SQL Worksheet in your Snowflake instance, and run the following SQL command, replacing `<database>` and `schema` with your database and schema:\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE CORTEX SEARCH SERVICE TRULENS_DEMO_CORTEX_SEARCH_SERVICE\n",
    "  ON doc_text\n",
    "  WAREHOUSE = JREINI_WH\n",
    "  TARGET_LAG = '1 hour'\n",
    "AS (\n",
    "  SELECT\n",
    "      doc_text\n",
    "  FROM <database>.<schema>.streamlit_docs\n",
    ");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Cortex Search Service\n",
    "\n",
    "Next, we can go back to our python notebook and create a `CortexSearchRetreiver` class to connect to our cortex search service and add the `retrieve` method that we can leverage for calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from snowflake.core import Root\n",
    "from typing import List\n",
    "\n",
    "class CortexSearchRetriever:\n",
    "\n",
    "    def __init__(self, session: Session, limit_to_retrieve: int = 4):\n",
    "        self._session = session\n",
    "        self._limit_to_retrieve = limit_to_retrieve\n",
    "\n",
    "    def retrieve(self, query: str) -> List[str]:\n",
    "        root = Root(self._session)\n",
    "        cortex_search_service = (\n",
    "        root\n",
    "        .databases[os.environ[\"SNOWFLAKE_DATABASE\"]]\n",
    "        .schemas[os.environ[\"SNOWFLAKE_SCHEMA\"]]\n",
    "        .cortex_search_services[os.environ[\"SNOWFLAKE_CORTEX_SEARCH_SERVICE\"]]\n",
    "    )\n",
    "        resp = cortex_search_service.search(\n",
    "                query=query,\n",
    "                columns=[\"doc_text\"],\n",
    "                limit=self._limit_to_retrieve,\n",
    "            )\n",
    "\n",
    "        if resp.results:\n",
    "            return [curr[\"doc_text\"] for curr in resp.results]\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the retriever is created, we can test it out. Now that we have grounded access to the Streamlit docs, we can ask questions about using Streamlit, like \"How do I launch a streamlit app\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = CortexSearchRetriever(session=session, limit_to_retrieve=4)\n",
    "\n",
    "retrieved_context = retriever.retrieve(query=\"How do I launch a streamlit app?\")\n",
    "\n",
    "len(retrieved_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.retrieve(query=\"How do I launch a streamlit app?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a RAG with built-in observability\n",
    "\n",
    "Now that we've set up the components we need from Snowflake Cortex, we can build our RAG.\n",
    "\n",
    "We'll do this by creating a custom python class with each the methods we need. We'll also add TruLens instrumentation with the `@instrument` decorator to our app.\n",
    "\n",
    "The first thing we need to do however, is to set the database connection where we'll log the traces and evaluation results from our application. This way we have a stored record that we can use to understand the app's performance. This is done when initializing `Tru`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Tru\n",
    "\n",
    "db_url = \"snowflake://{user}:{password}@{account}/{dbname}/{schema}?warehouse={warehouse}&role={role}\".format(\n",
    "  user=os.environ[\"SNOWFLAKE_USER\"],\n",
    "  account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "  password=os.environ[\"SNOWFLAKE_USER_PASSWORD\"],\n",
    "  dbname=os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "  schema=os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "  warehouse=os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "  role=os.environ[\"SNOWFLAKE_ROLE\"],\n",
    ")\n",
    "\n",
    "tru = Tru(database_url=db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct the RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.tru_custom_app import instrument\n",
    "\n",
    "class RAG_from_scratch:\n",
    "\n",
    "  def __init__(self):\n",
    "    self.retriever = CortexSearchRetriever(session=session, limit_to_retrieve=4)\n",
    "\n",
    "  @instrument\n",
    "  def retrieve_context(self, query: str) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve relevant text from vector store.\n",
    "    \"\"\"\n",
    "    return self.retriever.retrieve(query)\n",
    "\n",
    "  @instrument\n",
    "  def generate_completion(self, query: str, context_str: list) -> str:\n",
    "    \"\"\"\n",
    "    Generate answer from context.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    'You are an expert assistance extracting information from context provided.\n",
    "    Answer the question based on the context. Be concise and do not hallucinate.\n",
    "    If you don´t have the information just say so.\n",
    "    Context: {context_str}\n",
    "    Question:\n",
    "    {query}\n",
    "    Answer: '\n",
    "    \"\"\"\n",
    "    return Complete(\"mistral-large\", query)\n",
    "\n",
    "  @instrument\n",
    "  def query(self, query: str) -> str:\n",
    "    context_str = self.retrieve_context(query)\n",
    "    return self.generate_completion(query, context_str)\n",
    "\n",
    "rag = RAG_from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.query(\"How do I launch a streamlit app?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After constructing the RAG, we can set up the feedback functions we want to use to evaluate the RAG.\n",
    "\n",
    "Here, we'll use the [RAG Triad](https://www.trulens.org/trulens_eval/getting_started/core_concepts/rag_triad/). The RAG triad is made up of 3 evaluations along each edge of the RAG architecture: context relevance, groundedness and answer relevance.\n",
    "\n",
    "Satisfactory evaluations on each provides us confidence that our LLM app is free from hallucination.\n",
    "\n",
    "We will also use [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) evaluations, using Mistral Large on [Snowflake Cortex](https://www.trulens.org/trulens_eval/api/provider/cortex/) as the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval.feedback.provider.cortex import Cortex\n",
    "from trulens_eval.feedback import Feedback\n",
    "from trulens_eval import Select\n",
    "import numpy as np\n",
    "\n",
    "provider = Cortex(\"mistral-large\")\n",
    "\n",
    "f_groundedness = (\n",
    "    Feedback(\n",
    "    provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\")\n",
    "    .on(Select.RecordCalls.retrieve_context.rets[:].collect())\n",
    "    .on_output()\n",
    ")\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(\n",
    "    provider.context_relevance,\n",
    "    name=\"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(Select.RecordCalls.retrieve_context.rets[:])\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "f_answer_relevance = (\n",
    "    Feedback(\n",
    "    provider.relevance,\n",
    "    name=\"Answer Relevance\")\n",
    "    .on_input()\n",
    "    .on_output()\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "feedbacks = [f_context_relevance,\n",
    "            f_answer_relevance,\n",
    "            f_groundedness,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the feedback functions to use, we can just add them to the application along with giving the application an ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruCustomApp\n",
    "tru_rag = TruCustomApp(rag,\n",
    "    app_id = 'RAG v1',\n",
    "    feedbacks = [f_groundedness, f_answer_relevance, f_context_relevance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"How do I launch a streamlit app?\",\n",
    "    \"How can I capture the state of my session in streamlit?\",\n",
    "    \"How do I install streamlit?\",\n",
    "    \"How do I change the background color of a streamlit app?\",\n",
    "    \"What's the advantage of using a streamlit form?\",\n",
    "    \"What are some ways I should use checkboxes?\",\n",
    "    \"How can I conserve space and hide away content?\",\n",
    "    \"Can you recommend some resources for learning Streamlit?\",\n",
    "    \"What are some common use cases for Streamlit?\",\n",
    "    \"How can I deploy a streamlit app on the cloud?\",\n",
    "    \"How do I add a logo to streamlit?\",\n",
    "    \"What is the best way to deploy a Streamlit app?\",\n",
    "    \"How should I use a streamlit toggle?\",\n",
    "    \"How do I add new pages to my streamlit app?\",\n",
    "    \"How do I write a dataframe to display in my dashboard?\",\n",
    "    \"Can I plot a map in streamlit? If so, how?\",\n",
    "    \"How do vector stores enable efficient similarity search?\",\n",
    "    \"How do I prevent my child from using the internet?\",\n",
    "    \"What should I pack for a camping trip this weekend?\",\n",
    "    \"How do I defend myself against bear attacks?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the application is ready, we can run it on a test set of questions about streamlit to measure its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tru_rag as recording:\n",
    "    for prompt in prompts:\n",
    "        rag.query(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Guardrails\n",
    "\n",
    "In addition to making informed iteration, we can also directly use feedback results as guardrails at inference time. In particular, here we show how to use the context relevance score as a guardrail to filter out irrelevant context before it gets passed to the LLM. This both reduces hallucination and improves efficiency.\n",
    "\n",
    "To do so, we'll rebuild our RAG using the `@context-filter` decorator on the method we want to filter, and pass in the feedback function and threshold to use for guardrailing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: feedback function used for guardrail must only return a score, not also reasons\n",
    "f_context_relevance_score = (\n",
    "    Feedback(provider.context_relevance, name = \"Context Relevance\")\n",
    ")\n",
    "\n",
    "from trulens_eval.guardrails.base import context_filter\n",
    "\n",
    "# note: feedback function used for guardrail must only return a score, not also reasons\n",
    "f_context_relevance_score = (\n",
    "    Feedback(provider.context_relevance, name = \"Context Relevance\")\n",
    "    .on_input()\n",
    "    .on(Select.RecordCalls.retrieve.rets)\n",
    ")\n",
    "\n",
    "from trulens_eval.guardrails.base import context_filter\n",
    "\n",
    "class filtered_RAG_from_scratch:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.retriever = CortexSearchRetriever(session=session, limit_to_retrieve=4)\n",
    "    @instrument\n",
    "    @context_filter(f_context_relevance_score, 0.75, keyword_for_prompt=\"query\")\n",
    "    def retrieve_context(self, query: str) -> list:\n",
    "        \"\"\"\n",
    "        Retrieve relevant text from vector store.\n",
    "        \"\"\"\n",
    "        results = self.retriever.retrieve(query)\n",
    "        return results\n",
    "\n",
    "    @instrument\n",
    "    def generate_completion(self, query: str, context_str: list) -> str:\n",
    "        \"\"\"\n",
    "        Generate answer from context.\n",
    "        \"\"\"\n",
    "        completion = Complete(\"mistral-large\",query)\n",
    "        return completion\n",
    "\n",
    "    @instrument\n",
    "    def query(self, query: str) -> str:\n",
    "        context_str = self.retrieve_context(query=query)\n",
    "        completion = self.generate_completion(query=query, context_str=context_str)\n",
    "        return completion\n",
    "\n",
    "filtered_rag = filtered_RAG_from_scratch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the new version of our app with the feedback functions we already defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import TruCustomApp\n",
    "filtered_tru_rag = TruCustomApp(filtered_rag,\n",
    "    app_id = 'RAG v2',\n",
    "    feedbacks = [f_groundedness, f_answer_relevance, f_context_relevance])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we run it on a test set of questions about streamlit to measure its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with filtered_tru_rag as recording:\n",
    "    for prompt in prompts:\n",
    "        filtered_rag.query(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.get_leaderboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion And Resources\n",
    "\n",
    "Congratulations! You've successfully built a RAG by combining Cortex Search and LLM Functions, adding in TruLens Feedback Functions as Observability. You also set up logging for TruLens to Snowflake, and added TruLens Guardrails to reduce hallucination.\n",
    "\n",
    "### What You Learned\n",
    "\n",
    "- How to build a RAG with Cortex Search and Cortex LLM Functions.\n",
    "- How to use TruLens Feedback Functions and Tracing.\n",
    "- How to log TruLens Evaluation Results and Traces to Snowflake.\n",
    "- How to use TruLens Feedback Functions as Guardrails to reduce hallucination.\n",
    "\n",
    "### Related Resources\n",
    "\n",
    "- [Snowflake Cortex Documentation](https://docs.snowflake.com/en/guides-overview-ai-features)\n",
    "- [TruLens Documentation](https://trulens.org/)\n",
    "- [TruLens GitHub Repository](https://github.com/truera/trulens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowday",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
