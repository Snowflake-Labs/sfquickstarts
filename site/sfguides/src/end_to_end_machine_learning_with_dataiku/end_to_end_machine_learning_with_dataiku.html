<!DOCTYPE html>
<html>
<head>
<title>end_to_end_machine_learning_with_dataiku.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<p>author: Prash Medirattaa
id: end_to_end_machine_learning_with_dataiku
summary: This is a sample Snowflake Guide
categories: Getting-Started
environments: web
status: Published
feedback link: https://github.com/Snowflake-Labs/sfguides/issues
tags: Getting Started, Data Science, Data Engineering, Twitter</p>
<h1 id="end-to-end-machine-learning-with-snowflake-and-dataiku">End to End Machine learning with Snowflake and Dataiku</h1>
<!-- ------------------------ -->
<h2 id="overview">Overview</h2>
<p>Duration: 5</p>
<p>This Snowflake Quickstart introduces you to the using Snowflake together with Dataiku Cloud as part of a Machine learning project, and build an end-to-end machine learning solution. This lab will showcase seamless integration of both Snowflake and Dataiku at every stage of ML life cycle. We will also use Snowflake Data Marketplace to enrich the dataset.</p>
<h3 id="business-problem">Business Problem</h3>
<p>Will go through a <strong>supervised machine learning</strong> by building a binary classification model to predict if a lender will default on a loan. <strong>LOAN_STATUS (yes/no)</strong>  considering multiple features.</p>
<p><strong>Supervised machine learning</strong> is the process of taking a historical dataset with KNOWN outcomes of what we would like to predict, to train a model, that can be used to make future predictions. After building a model we will deploy back to Snowflake for scoring by using Snowpark-java udf.</p>
<h3 id="dataset">Dataset</h3>
<p>We will be exploring a financial service use of evaluating loan information to predict if a lender will default on a loan. The base data set was derived from loan data from the Lending Club.</p>
<p>In addition to base data, this will then be enriched with unemployment data from Knoema on the Snowflake Data Marketplace.</p>
<h3 id="what-were-going-to-build">What We’re Going To Build</h3>
<p>We will build a project. The project contains the input datasets from Snowflake. We’ll build a data science pipeline by applying data transformations, enriching from Marketplace employment data, building a machine learning model, and deploying it to the Flow. We will then see how you can score the model against fresh data from Snowflake and automate</p>
<p><img src="assets/SF-1.jpg" alt="1"></p>
<h3 id="prerequisites">Prerequisites</h3>
<ul>
<li>Familiarity with Snowflake, basic SQL knowledge and Snowflake objects</li>
<li>Basic knowledge  Machine Learning</li>
<li>Basic knowledge Python, Jupyter notebook for <strong>Bonus</strong></li>
</ul>
<h3 id="what-youll-need-during-the-lab">What You'll Need During the Lab</h3>
<p>To participate in the virtual hands-on lab, attendees need the following:</p>
<ul>
<li>A <a href="https://trial.snowflake.com/">Snowflake free 30-day trial</a> <strong>ACCOUNTADMIN</strong> access</li>
<li>Dataiku Cloud trial version via Snowflake’s Partner Connect</li>
</ul>
<h3 id="what-youll-build">What You'll Build</h3>
<p>Operational end-to-end ML project using joint capabilities of Snowflake and Dataiku from Data collection to deployement</p>
<ul>
<li>Create a Data Science project in Dataiku and perform analysis on data via Dataiku within Snowflake</li>
<li>The analysis and feature engineering using Dataiku</li>
<li>Create, run, and evaluate simple Machine Learning models in Dataiku,  measure their performance and interpret</li>
<li>Building and deploying Pipelines</li>
<li>Creating  Snowpark-Java UDF, and using it to score result on test dataset writing back to Snowflake</li>
<li>Use cloning and time travel for test environment</li>
</ul>
<!-- ------------------------ -->
<h2 id="setting-up-snowflake">Setting up Snowflake</h2>
<p>Duration: 5</p>
<ul>
<li>
<p>If you haven’t already, register for a <a href="https://trial.snowflake.com/">Snowflake free 30-day trial</a></p>
</li>
<li>
<p><strong>Region</strong>  - Kindly choose which is physically closest to you</p>
</li>
<li>
<p><strong>Snowflake edition</strong>  - Select the <strong>Enterprise edition</strong> so you can leverage some advanced capabilities that are not available in the Standard Edition.</p>
</li>
</ul>
<p><img src="assets/sf-2-signup.png" alt="2"></p>
<p><img src="assets/SF-3.jpg" alt="3"></p>
<ul>
<li>After registering, you will receive an <strong>email</strong> with an <strong>activation</strong> link and your Snowflake account URL. Kindly activate the account.</li>
</ul>
<p><img src="assets/SF-4.jpg" alt="4"></p>
<ul>
<li>After activation, you will create a <strong>user name</strong> and <strong>password</strong>. Write down these credentials</li>
</ul>
<p><img src="assets/sf-5-user_id_password.png" alt="5"></p>
<!-- ------------------------ -->
<h2 id="logging-in-snowflake">Logging in  Snowflake</h2>
<p>Duration: 2</p>
<h4 id="step-1">Step 1</h4>
<p>Log in with your credentials</p>
<p><img src="assets/sf-6-login.png" alt="6"></p>
<p><strong>Bookmark this URL for easy, future access</strong></p>
<p>Resize your browser window, so that you can view this guide and your web browser side-by-side and follow the lab instructions. If possible, use a secondary display dedicated to the lab guide.</p>
<h4 id="step-2">Step 2</h4>
<p>Log into your Snowflake account. By default it will open up</p>
<p><img src="assets/sf-7-firstpage.png" alt="7"></p>
<p>If you have just created a free trial account, feel free to minimize or close and hint boxes that are looking to help guide you. These will not be needed for this lab and most of the hints will be covered throughout the remainder of this exercise</p>
<h4 id="step-3">Step 3</h4>
<p>Create <strong>Worksheet</strong></p>
<p><img src="assets/sf-8-createworksheet.png" alt="8"></p>
<h4 id="step-4">Step 4</h4>
<p>Adding a <strong>Worksheet</strong></p>
<p><img src="assets/sf-9-createworksheet2.png" alt="9"></p>
<h4 id="step-5">Step 5</h4>
<ul>
<li>Creating a new  <strong>Worksheet</strong> and <strong>Renaming</strong> it to <strong>Data Loading</strong></li>
</ul>
<p><img src="assets/sf-10-createworksheet.png" alt="10"></p>
<p><img src="assets/sf-11-renameworksheet.png" alt="11"></p>
<h2 id="load-data-in-snowflake">Load data in  Snowflake</h2>
<p>Download the following .sql file that contains a series of SQL commands we will execute throughout this lab. You can either execute cell by cell commands from the sql file or copy the below code blocks and follow.</p>
<p><button><a href="https://snowflake-corp-se-workshop.s3.us-west-1.amazonaws.com/Summit_Snowflake_Dataiku/src/Snowflake_Dataiku_ML.sql">Snowflake_Dataiku_ML.sql</a></button></p>
<p><strong>Part 1</strong> : <code>Step 1 - Step 4</code></p>
<p>Creating database, Warehouse, loading dataset</p>
<p><strong>Part 2</strong> : <code>Step 5 - Step 8</code></p>
<p>Tapping Snowflake Marketplace dataset</p>
<p>After creating the worksheet in the last step we can import the sql file provided .
<img src="assets/SF-13.jpg" alt="13"></p>
<p>Importing  <strong>Sql</strong> to <strong>Worksheet</strong>
To ingest our script in the Snowflake UI, Import SQL from File.</p>
<p><img src="assets/SF-12.jpg" alt="13"></p>
<h4 id="data-loading--steps">Data Loading : Steps</h4>
<p>Each step throughout the guide has an associated SQL command to perform the work we are looking to execute, and so feel free to step through each action running the code line by line as we walk through the lab. If you wish to run the code at once <strong>Part 1</strong> : <code>Step 1 - Step 4</code>  need to run first and then additional <code>Steps</code> are required before executing  <strong>Part 2</strong> : <code>Step 5 - Step 8</code> can be executed.</p>
<p>To execute this code, all we need to do is place our cursor on the line we wish to run and then either hit the &quot;run&quot; button at the top left of the worksheet or press <code>Cmd/Ctrl + Enter</code></p>
<p><strong>Step 1</strong> : Virtual warehouse that we will use to compute with the <strong>SYSADMIN</strong> role, and then grant all privileges to the <strong>ML_ROLE</strong>.</p>
<pre class="hljs"><code><div>
USE ROLE SYSADMIN;

CREATE OR REPLACE WAREHOUSE ML_WH

  WITH WAREHOUSE_SIZE = 'XSMALL'

  AUTO_SUSPEND = 120

  AUTO_RESUME = true

  INITIALLY_SUSPENDED = TRUE;

</div></code></pre>
<p><strong>Step 2</strong> : Create <strong>Loan_data</strong> table in the database</p>
<pre class="hljs"><code><div>
USE WAREHOUSE ML_WH;

CREATE DATABASE IF NOT EXISTS ML_DB;

USE DATABASE ML_DB;

CREATE OR REPLACE TABLE loan_data (
  
        LOAN_ID NUMBER(38,0),
  
        LOAN_AMNT FLOAT,

        FUNDED_AMNT FLOAT,

        TERM VARCHAR(4194304),

        INT_RATE VARCHAR(4194304),

        INSTALLMENT FLOAT,

        GRADE VARCHAR(4194304),

        SUB_GRADE VARCHAR(4194304),

        EMP_TITLE VARCHAR(4194304),

        EMP_LENGTH_YEARS NUMBER(38,0),

        HOME_OWNERSHIP VARCHAR(4194304),

        ANNUAL_INC FLOAT,

        VERIFICATION_STATUS VARCHAR(4194304),

        ISSUE_DATE_PARSED TIMESTAMP_TZ(9),

        LOAN_STATUS VARCHAR(4194304),

        PYMNT_PLAN BOOLEAN,
        
        PURPOSE VARCHAR(4194304),

        TITLE VARCHAR(4194304),
    
        ZIP_CODE VARCHAR(4194304),

        ADDR_STATE VARCHAR(4194304),

        DTI FLOAT,

        DELINQ_2YRS FLOAT,

        EARLIEST_CR_LINE VARCHAR(4194304),

        INQ_LAST_6MTHS FLOAT,

        MTHS_SINCE_LAST_DELINQ FLOAT,

        MTHS_SINCE_LAST_RECORD FLOAT,

        OPEN_ACC FLOAT,

        REVOL_BAL FLOAT,

        REVOL_UTIL FLOAT,

        TOTAL_ACC FLOAT,

        TOTAL_PYMNT FLOAT,

        MTHS_SINCE_LAST_MAJOR_DEROG FLOAT,

        TOT_CUR_BAL FLOAT,

        ISSUE_MONTH NUMBER(38,0),

        ISSUE_YEAR NUMBER(38,0)
  
);


</div></code></pre>
<p>After running the cell above, we have successfully created a <strong>loan data</strong> table.</p>
<p><img src="assets/sf-15-dataloading2.png" alt="15"></p>
<p><strong>Step 3</strong> :Creating a external stage to load the lab data into the table. This is done from a public S3 bucket to simplify the workshop. Typically an external stage will be using various secure integrations as described in this <a href="https://docs.snowflake.com/en/user-guide/data-load-s3-config.html">link</a>.</p>
<pre class="hljs"><code><div>CREATE OR REPLACE STAGE LOAN_DATA

  url='s3://snowflake-corp-se-workshop/Summit_Snowflake_Dataiku/data/';
  
 
 ---- List the files in the stage 

 list @LOAN_DATA;
</div></code></pre>
<p><strong>Screen shot again after moving to new s3 folder</strong>
<img src="assets/sf-16-dataloading3.png" alt="16"></p>
<p><strong>Step 4</strong> :Cloning  the data in the database</p>
<pre class="hljs"><code><div>
COPY INTO loan_data FROM @LOAN_DATA/loans_data.csv
FILE_FORMAT = (TYPE = 'CSV' field_optionally_enclosed_by='&quot;',SKIP_HEADER = 1);  

SELECT * FROM loan_data LIMIT 100;

</div></code></pre>
<p>Below is the snapshot of the data and it represents aggregation from various internal systems for lender information and loans. We can have a quick look and see the various attributes in it.</p>
<p><img src="assets/sf-17-dataloading4.png" alt="17"></p>
<p>We have successfully loaded the data from <strong>external stage</strong> to snowflake.</p>
<pre><code>                          ------- End of Part 1 ---------
</code></pre>
<p><strong>Step 5</strong> : Time to switch to get <strong>Konema Employement Data</strong> from Snowflake Market place</p>
<p>We can now look at additional data in the Snowflake Marketplace that can be helpful for improving ML models. It may be good to look at employment data in the region when analyzing loan defaults. Let’s look in the Snowflake Data Marketplace and see what external data is available from the data providers.</p>
<p>Lets go to home screen</p>
<p><img src="assets/sf-18-dataloading5.png" alt="18"></p>
<h4 id="imp-note">Imp Note</h4>
<ol>
<li>
<p><strong>Click Market place tab</strong></p>
</li>
<li>
<p>Make Sure <strong>ACCOUNTADMIN</strong> role is selected</p>
</li>
<li>
<p>In search bar <strong>Labor Data Atlas</strong></p>
</li>
</ol>
<p><img src="assets/sf-19-marketplace1.png" alt="19"></p>
<p>Click on the tile with <strong>Labor Data Atlas.</strong></p>
<p><img src="assets/sf-20-marketplace2.png" alt="20"></p>
<p>Next click on the <strong>Get Data</strong> button. This will provide a pop up window in which you can create a database in your account that will provide the data from the data provider.</p>
<h4 id="important--steps">Important : Steps</h4>
<ol>
<li>
<p>Change the name of the database to  <strong>KNOEMA_LABOR_DATA_ATLAS</strong></p>
</li>
<li>
<p>Select additional roles drop down <strong>PUBLIC</strong></p>
</li>
<li>
<p>Click <strong>Get Data</strong></p>
</li>
</ol>
<p><img src="assets/sf-21-marketplace3.png" alt="21"></p>
<p>When the confirmation is provided click on done and then you can close the browser tab with the Preview App.</p>
<p><img src="assets/sf-22-marketplace4.png" alt="22"></p>
<p>Other advantage of using Snowflake Data Marketplace does not require any additional work and will show up as a database in your account. A further benefit is that the data will automatically update as soon as the data provider does any updates to the data on their account.After done just to confirm the datasets are properly configured.</p>
<p>Click on Data tab <strong>Database</strong></p>
<p>You should see <strong>KNOEMA_LABOR_DATA_ATLAS</strong>  and <strong>ML_DB</strong></p>
<p><img src="assets/sf-23-marketplace5.png" alt="23"></p>
<p>After confirming <strong>Databases</strong>.  Lets go to <strong>Worksheets tab</strong> and <strong>open</strong> the <strong>Data Loading</strong> worksheet</p>
<p><img src="assets/sf-24-marketplace6.png" alt="24"></p>
<p><strong>Step 6</strong> :Querying the <strong>Market Place dataset</strong> for some basic analysis</p>
<p>There are multiple datasets in <strong>Labor Atlas dataset</strong>. Lets try to find unemployment dataset in US to narrow down our search.</p>
<pre class="hljs"><code><div>USE WAREHOUSE ML_WH;

USE DATABASE KNOEMA_LABOR_DATA_ATLAS;

SELECT * 
FROM &quot;LABOR&quot;.&quot;DATASETS&quot;
WHERE &quot;DatasetName&quot; ILIKE '%unemployment%' 
AND &quot;DatasetName&quot; ILIKE '%U.S%';

</div></code></pre>
<p><img src="assets/sf-22-marketplace4a.png" alt="22"></p>
<p>Amazing! isn't  we have successfully tapped into live data collection of the most important, used, and high-quality datasets on the labor market and human resources on national and sub-national levels from a dozen of sources.</p>
<p>We can find answers such as what is the number of initial claims for unemployment insurance in the US over time?</p>
<pre class="hljs"><code><div>SELECT * FROM &quot;LABOR&quot;.&quot;USUID2017Sep&quot; WHERE &quot;Region Name&quot; = 'United States' AND 
      &quot;Indicator Name&quot; = 'Initial Claims' AND &quot;Measure Name&quot; = 'Value' AND 
       &quot;Seasonal Adjustment Name&quot; = 'Seasonally Adjusted' ORDER BY &quot;Date&quot;;

</div></code></pre>
<p><img src="assets/sf-22-marketplace4b.png" alt="22"></p>
<p>Now for this exercise we are going to <strong>Enrich</strong> the <strong>Loan dataset</strong> we created earlier using <code>BLSLA</code> dataset</p>
<p><strong>Step 7</strong> :Creating a <strong>KNOEMA_EMPLOYMENT_DATA</strong> marketplace data view to pivot the data for the different employment metrics to columns for easier consumption.</p>
<pre class="hljs"><code><div>USE DATABASE ML_DB;

CREATE OR REPLACE VIEW KNOEMA_EMPLOYMENT_DATA AS (

SELECT *

FROM (SELECT &quot;Measure Name&quot; MeasureName, &quot;Date&quot;, 
      &quot;RegionId&quot; State, 
      AVG(&quot;Value&quot;) Value 
      FROM &quot;KNOEMA_LABOR_DATA_ATLAS&quot;.&quot;LABOR&quot;.&quot;BLSLA&quot; WHERE &quot;RegionId&quot; is not null 
      and &quot;Date&quot; &gt;= '2018-01-01' AND &quot;Date&quot; &lt; '2018-12-31' GROUP BY &quot;RegionId&quot;, &quot;Measure Name&quot;, &quot;Date&quot;)
  PIVOT(AVG(Value) FOR MeasureName
  IN ('civilian noninstitutional population', 'employment', 'employment-population ratio', 
     'labor force', 'labor force participation rate', 'unemployment', 'unemployment rate')) AS 
        p (Date, State, civilian_noninstitutional_population, employment, employment_population_ratio, 
           labor_force, labor_force_participation_rate, unemployment, unemployment_rate)
);

SELECT * FROM KNOEMA_EMPLOYMENT_DATA LIMIT 100;

</div></code></pre>
<p><img src="assets/sf-25-marketplace7.png" alt="25"></p>
<p><strong>Step 8</strong> : Create a new table <strong>UNEMPLOYMENT DATA</strong> using the geography and time periods. This will provide us with unemployment data in the region associated with the specific loan.</p>
<pre class="hljs"><code><div>
CREATE OR REPLACE TABLE UNEMPLOYMENT_DATA AS

 SELECT l.LOAN_ID, e.CIVILIAN_NONINSTITUTIONAL_POPULATION, 
        e.EMPLOYMENT, e.EMPLOYMENT_POPULATION_RATIO, e.LABOR_FORCE, 
        e.LABOR_FORCE_PARTICIPATION_RATE, e.UNEMPLOYMENT, e.UNEMPLOYMENT_RATE

  FROM LOAN_DATA l LEFT JOIN KNOEMA_EMPLOYMENT_DATA e

 on l.ADDR_STATE = right(e.state,2) and l.issue_month = month(e.date) and l.issue_year = year(e.date);

SELECT * FROM UNEMPLOYMENT_DATA LIMIT 100;

</div></code></pre>
<p><img src="assets/sf-26-marketplace8.png" alt="26"></p>
<pre><code>                       ------- End of Part 2 ---------
</code></pre>
<h4 id="important-database-for-machine-learning-consumption-will-be-created-after-connecting-snowflake-with-dataiku-using-partner-connect">IMPORTANT: Database for Machine learning consumption will be created after connecting Snowflake with Dataiku using partner connect.</h4>
<!-- ------------------------ -->
<h2 id="connect-dataiku-with-snowflake">Connect Dataiku with Snowflake</h2>
<p>Duration: 8</p>
<p>Verify that your user is operating under the Account Admin role.</p>
<p>To do this:</p>
<ul>
<li>
<p>Click your account name in the upper left-hand corner (if you are using the Classic Console this is top-right)</p>
</li>
<li>
<p>Choose <strong>Switch Role</strong> from the drop-down list</p>
</li>
<li>
<p>Click <strong>ACCOUNTADMIN</strong></p>
</li>
</ul>
<p><img src="assets/SF-17.jpg" alt="27"></p>
<p><img src="assets/SF-16.jpg" alt="28"></p>
<ul>
<li>Click on the <strong>Dataiku</strong> tile. This will launch the following window, which will automatically create the <strong>connection parameters</strong> required for Dataiku to connect to Snowflake.</li>
</ul>
<p>Snowflake will create a dedicated database, warehouse, system user, system password and system role, with the intention of those being used by the Dataiku account.</p>
<p><img src="assets/sf-27-partnerconnect1.png" alt="29"></p>
<p>We’d like to use the <strong>PC_DATAIKU_USER</strong> to connect from Dataiku to Snowflake, and use the <strong>PC_DATAIKU_WH</strong> when performing activities within Dataiku that are pushed down into Snowflake.</p>
<p>Note that the user password (which is autogenerated by Snowflake and never displayed), along with all of the other Snowflake connection parameters, are passed to the Dataiku server so that they will automatically be used for the Dataiku connection.  <strong>DO NOT CHANGE THE PC_DATAIKU_USER</strong> password, otherwise Dataiku will not be able to connect to the Snowflake database.</p>
<p>Click on <strong>Connect</strong>. You may be asked to provide your first and last name.  If so, add them and click Connect. Your partner account has been created. Click on <strong>Activate</strong> to get it activated.</p>
<p><img src="assets/dk-1_100_PC_created.png" alt="30"></p>
<p>This will launch a new page that will redirect you to a launch page from Dataiku.
Here, you will have two options:</p>
<ol>
<li>Login with an existing Dataiku username</li>
<li>Sign up for a new Dataiku account</li>
</ol>
<p>We assume that you’re new to <strong>Dataiku</strong>, so ensure the “Sign Up” box is selected, and sign up with either GitHub, Google or your email address and your new password.</p>
<p>Click sign up. &lt;&lt;NOTE: ADD INSTRUCTION FOR EXISTING IN ADDITION&gt;&gt;</p>
<p><img src="assets/dk-2_signin.jpg" alt="31"></p>
<p>When using your email address, ensure your password fits the following criteria:</p>
<ol>
<li>
<p><strong>At least 8 characters in length</strong></p>
</li>
<li>
<p><strong>Should contain:</strong>
<strong>Lower case letters (a-z)</strong></p>
<p><strong>Upper case letters (A-Z)</strong></p>
<p><strong>Numbers (i.e. 0-9)</strong></p>
</li>
</ol>
<p>Upon clicking on the activation link, please briefly review the Terms of Service of Dataiku Cloud. In order to do so, please scroll down to the bottom of the page. Click on <strong>I AGREE</strong></p>
<p><img src="assets/dk-3_100_DKU_Online_T&amp;Cs.png" alt="32"></p>
<p>Next, you’ll need to complete your sign up information then click on <strong>Start</strong>.</p>
<!-- ![33](assets/dk-4_sign_in_details.png) -->
<p>You will be redirected to the Dataiku Cloud Launchpad site. Click <strong>GOT IT!</strong> to continue.</p>
<p><img src="assets/dk-5_100_DKU_Online_welcome.png" alt="34"></p>
<p><img src="assets/dk-6_100_DKU_Online_launch_screen.png" alt="35"></p>
<p>You’ve now successfully set up your Dataiku trial account via Snowflake’s Partner Connect. We are now ready to continue with the lab. For this, move back to your Snowflake browser.</p>
<h4 id="database-for-machine-learning">Database for Machine Learning</h4>
<p>After connecting  <strong>Snowflake</strong> to <strong>Dataiku</strong> via partner connect. We will clone the table created in <strong>ML_DB</strong> to <strong>PC_DATAIKU_DB</strong> for the Dataiku consumption. Snowflake provides a very unique feature called <a href="https://www.youtube.com/watch?v=yQIMmXg7Seg">Zero Copy Cloning</a> that will create a new copy of the data by <strong>only making a copy of the metadata of the objects</strong>. This drastically speeds up creation of copies and also drastically reduces the storage space needed for data copies.</p>
<p><img src="assets/sf-28-partnerconnect2.png" alt="36"></p>
<p>You should see three database now  <strong>PC_DATAIKU_DB</strong> is the system generated database created. Go back to <strong>Worksheet</strong> you are working and run below commands.</p>
<h4 id="granting-previlages-of-mldb-to-pcdataikurole">Granting Previlages of ML_DB to PC_Dataiku_role</h4>
<pre class="hljs"><code><div>
grant all privileges on database ML_DB to role PC_Dataiku_role;
grant usage on all schemas in database ML_DB to role PC_Dataiku_role;
grant select on all tables in schema ML_DB.public to role PC_Dataiku_role;
grant select on all views in schema ML_DB.public to role PC_Dataiku_role;

</div></code></pre>
<h4 id="cloning-tables-to-dataiku-database-before-consuming-it-for-dataiku-dss">Cloning tables to DATAIKU Database before consuming it for Dataiku DSS</h4>
<pre class="hljs"><code><div>USE ROLE PC_DATAIKU_ROLE;
USE DATABASE PC_DATAIKU_DB;
USE WAREHOUSE PC_DATAIKU_WH;

--- cloning 

CREATE OR REPLACE TABLE LOANS_ENRICHED CLONE ML_DB.PUBLIC.LOAN_DATA;
CREATE OR REPLACE TABLE UNEMPLOYMENT_DATA CLONE ML_DB.PUBLIC.UNEMPLOYMENT_DATA;


SELECT * FROM LOANS_ENRICHED LIMIT 10;
</div></code></pre>
<p>After running above commands, we have created clones for the tables to be used for analysis. Kindly check <strong>PC_DATAIKU_DB</strong> you should have two datasets <strong>LOANS_ENRICHED</strong> and <strong>UNEMPLOYMENT_DATA</strong></p>
<p><img src="assets/sf-29-partnerconnect4.png" alt="37"></p>
<h4 id="now-lets-move-to-dataiku-console-for-feature-engineering-model-building-scoring-and-deployment">Now lets move to Dataiku console for feature engineering, model building, Scoring and deployment.</h4>
<!-- ------------------------ -->
<h2 id="getting-started-with-a-dataiku-project">Getting Started with a Dataiku Project</h2>
<p>Duration: 2</p>
<p>Return to Dataiku Online and if you haven't already click on <strong>OPEN DATAIKU DSS</strong> from the Launchpad to start your instance of Dataiku DSS</p>
<p><img src="assets/dk-6_100_DKU_Online_launch_screen.png" alt="35"></p>
<p>At the end of the lab, the project Flow will look like this:</p>
<p><img src="assets/dk-6c-flow.png" alt="35a"></p>
<ul>
<li>
<p>A <strong>dataset</strong> is represented by a blue square with a symbol that depicts the dataset type or connection. The initial datasets (also known as input datasets) are found on the left of the Flow. In this project, the input datasets will be the ones we just created in Snowflake.</p>
</li>
<li>
<p>A <strong>recipe</strong> in Dataiku DSS (represented by a circle icon with a symbol that depicts its function) can be either visual or code-based, and it contains the processing logic for transforming datasets.</p>
</li>
<li>
<p><strong>Machine learning processes</strong> are represented by green icons.</p>
</li>
<li>
<p>The <strong>Actions Menu</strong> is shown on the right pane and is context sensitive.</p>
</li>
<li>
<p>Whatever screen you are currently in you can always return to the main <strong>Flow</strong> by clicking the <strong>Flow</strong> symbol from the top menu (also clicking the project name will take you back to the main Project page).</p>
</li>
</ul>
<p><strong>Input dataset:</strong>
<em>In the interests of time we have performed some initial steps of the data pipeline such as cleansing and transformations on the loans dataset. These steps can be created in Dataiku from the raw datasets from the Lending Club to form a complete pipeline with the data and execution happening in Snowflake</em></p>
<h3 id="how-well-build-the-project">How We’ll Build The Project</h3>
<p>Our goal is to build an optimized machine learning model that can be used to predict the risk of default on loans for customers and advise them on how to reduce their risk.
To do this, we’ll join the input datasets, perform transformations &amp; feature engineering so that they are ready to use for building a binary classification model.</p>
<h3 id="creating-a-dataiku-project">Creating a Dataiku Project</h3>
<p>Once you’ve logged in, <code>click</code> on <code>+NEW PROJECT</code> and <code>select Blank project</code> to create a new project.</p>
<p><img src="assets/dk6d-new_project.png" alt="35d"></p>
<!-- ------------------------ -->
<h2 id="data-import-analysis--join">Data Import, Analysis &amp; Join</h2>
<p>Duration: 5</p>
<p>After creating our project let’s add our datasets from Snowflake to the Flow.</p>
<ul>
<li>From the Flow click <code>+ Import Your First Dataset</code> in the centre of the screen.</li>
</ul>
<p><img src="assets/dk7b-first_import.png" alt="37"></p>
<ul>
<li>Select the <code>Search and import option</code></li>
</ul>
<p><img src="assets/dk-8_400_Search&amp;Import_option_closeup.png" alt="38"></p>
<ul>
<li>Select the <code>PC_DATAIKU_DB</code> connection from the dropdown then click the refresh icon next to the database or schema dropdowns to populate these options.</li>
<li>Select the database and schema as below then click on <code>LIST TABLES</code></li>
</ul>
<p><img src="assets/dk-9_400_Connection_explorer_with_filled_out_values.png" alt="39"></p>
<ul>
<li>Select the <code>Loans_Enriched</code> and <code>Unemployment_Data</code> datasets and click <code>CREATE 2 DATASETS</code></li>
</ul>
<p><img src="assets/dk-10_400_Renamed_tables.png" alt="40"></p>
<p><img src="assets/dk-11_400_Datasets_imported_screen.png" alt="41"></p>
<ul>
<li>Navigate back to the Flow from the left-most menu in the top navigation bar <code>(or use the keyboard shortcut G+F)</code>.</li>
</ul>
<p><img src="assets/dk-12_500_Two_datasets_in_flow.png" alt="42"></p>
<p>Now we have all of the raw data needed for this lab. Let’s explore what’s inside these datasets.</p>
<ul>
<li>
<p>From the Flow, double click on the <code>loans_enriched dataset</code> to open it.</p>
</li>
<li>
<p>You can analyze column metrics to better understand your data: Either click on the column name and <code>select Analyze</code> or, if you wish for a quick overview of columns key statistics, <code>select Quick Column Stats</code> button on the top-right.</p>
</li>
</ul>
<p><img src="assets/dk-13_analyze.png" alt="43"></p>
<h3 id="join-the-data">Join the Data</h3>
<p>So far, your Flow only contains datasets. To take action on datasets, you need to apply recipes. The <strong>LOANS_ENRICHED</strong> and <strong>UNEMPLOYMENT_DATA</strong> datasets both contain a column of Loan IDs. Let’s join these two datasets together using a visual recipe.</p>
<ul>
<li>Select the <code>LOANS_ENRICHED</code> dataset from the Flow by <code>single clicking</code> on it.</li>
<li>Choose <code>Join With…</code> from the <code>Visual recipes</code> section of the Actions sidebar near the top right of the screen (note: click the <code>Open Panel</code> arrow if it is minimized and notice there are three different types of join recipe, we want <code>Join With…</code>).</li>
<li>Choose <code>UNEMPLOYMENT_DATA</code> as the second input dataset.</li>
</ul>
<p><img src="assets/dk-15_700_join_tables.png" alt="44"></p>
<ul>
<li>Leave the default option of <code>PC_DATAIKU_DB for “Store into”</code> and <code>Create</code> the recipe.</li>
<li>On the Join step you can <code>click on Left Join</code> to observe the selected join type and conditions.</li>
</ul>
<p><img src="assets/dk-16_700_join_conditions.png" alt="45"></p>
<ul>
<li>On the Selected columns step you can leave the defaults</li>
<li>On the Output step, note the output column names</li>
<li>Before running it, <code>Save the recipe</code></li>
<li>Ensure that <code>In-database (SQL)</code> is selected as the engine. You can view this underneath the <code>Run button</code> (Bottom left). If it is set to a different engine <code>click on the three cogs</code> to change it</li>
<li>Click <code>RUN</code> and <code>Update Schema</code> if prompted, then return to the Flow</li>
</ul>
<p>Note: You can view the SQL query as well as the execution plan generated by selecting <code>VIEW QUERY</code> on the <code>Output</code> screen.</p>
<p>Your flow should now look like this
<img src="assets/dk-17_700_Flow_join.png" alt="45"></p>
<h2 id="prepare-the-data">Prepare the Data</h2>
<p>Duration: 5</p>
<p>Data cleaning and preparation is typically one of the most time-consuming tasks for anyone working with data. In our lab, in order to save some of that time, our main lending dataset has already been largley cleaned. In the real world this would be done by other colleagues, say, from the data analytics team collaborating on this project and you would see their work as steps in our projects flow.</p>
<p>Let’s take a brief look at the <code>Prepare recipe</code>, the workhorse of the visual recipes in Dataiku, and perform some final investigations and transformations.</p>
<ul>
<li><code>Single click</code> on the output dataset of our join from the flow and <code>select Prepare</code> from the visual recipes in the <code>Actions Panel</code>. (If the automatically generated output dataset name is starting to get unwieldy feel free to change it)</li>
</ul>
<p>In a Prepare recipe you assemble a series of steps to transform your data from a library of ~100 processors. There are a couple of ways you can select these processors to build your script. Firstly you can select these processors directly by using the <code>+ADD A NEW STEP</code> button on the left.
Secondly because Dataiku DSS infers meanings for each column, it suggests relevant actions in many cases. In the example below although the column is stored in Snowflake as a String Dataiku DSS recognises it as a date format so infers a <code>Date(unparsed)</code> meaning and suggests the <code>Parse Date</code> processor, by selecting the <code>More actions</code> menu item further suggestions are made.</p>
<p><img src="assets/dk-prepare_overview.png" alt="46"></p>
<p>Let's try using processors with both methods, firstly via the suggested actions:</p>
<ul>
<li>Click on the <code>EARLIEST_CR_LINE</code> column header and from the dropdown, <code>select Parse date</code></li>
<li>In <code>Add a custom format</code> set the format to <code>d-MMM-yyyy</code> and click on <code>USE DATE FORMAT</code></li>
<li>A step is generated on the left. Change the <code>Locale</code> to <code>en_US</code></li>
</ul>
<p><img src="assets/dk-18_800_parse_date.jpg" alt="46"></p>
<p><img src="assets/dk-19_800_date_format.jpg" alt="47"></p>
<p><img src="assets/dk-20_800_parse_en.jpg" alt="48"></p>
<ul>
<li>
<p>Click on the newly created column (click outside the step to action the change) and select <code>Compute time difference</code></p>
</li>
<li>
<p>Change <code>Until</code> to <code>Another Date Column</code> and add <strong>ISSUE_DATE_PARSED</strong> as that column.</p>
</li>
<li>
<p>Change the unit to <code>Years</code> and name the new column <code>since_Earliest_CR_LINE_years</code></p>
</li>
</ul>
<p><img src="assets/dk-21_800_compute_time_since.jpg" alt="48"></p>
<p>Now we have our desired feature we can remove the two date columns.</p>
<ul>
<li>Click on <code>EARLIEST_CR_LINE</code> and select <code>delete</code>, do the same for <code>EARLIEST_CR_LINE_parsed</code></li>
</ul>
<p>Your script steps should now look like this:</p>
<p><img src="assets/dk-22_800_final_steps_dates.jpg" alt="49"></p>
<p>Optionally you can place the three date transformation script steps into their own group with comments to make it simple for a colleague to follow everything you have done
Let’s turn our attention to the <code>INT_RATE</code> column. The interest rate is likely to be a powerful predictive feature when modeling credit defaults but currently its stored as a string:</p>
<ul>
<li>Click on the <code>+ADD A NEW STEP</code> button at the bottom of your script steps.</li>
<li>Select the <code>Find and Replace</code> processor either by looking in the <code>Strings</code> menu or using the search function.</li>
</ul>
<p><img src="assets/dk-23b_replace.png" alt="50"></p>
<ul>
<li>Select <code>INT_RATE</code> as the column then click <code>+ADD REPLACEMENT</code> and <code>replace % with a blank value</code>. Ensure the <code>Matching Mode</code> dropdown is set to <code>Substring</code></li>
</ul>
<p><img src="assets/dk-23c_replace_sub.png" alt="50a"></p>
<p>Our <code>INT_RATE</code> column has some suspiciously high values. Let’s use the Analyze tool again and see how it can be used to take certain actions in a Prepare recipe</p>
<ul>
<li>From the <code>INT_RATE</code> column header dropdown, select <code>Analyze</code>.</li>
<li>In the Outliers section, choose <code>Remove rows outside 1.5 IQR</code> from the menu.</li>
</ul>
<p><img src="assets/dk-25_800_outliers.jpg" alt="52"></p>
<p>As before you can optionally group and comment on these int_rate transformation steps.</p>
<ul>
<li>ensure <code>In-database (SQL)</code> engine is selected and then click <code>RUN</code></li>
</ul>
<!-- ------------------------ -->
<h2 id="feature-engineering-with-code">Feature Engineering with Code</h2>
<p>Duration: 5</p>
<p>Till now we've used visual tools but lets see how users who prefer to code can collaborate alongside their low/no code colleagues</p>
<ul>
<li>Return to the Flow.</li>
<li><code>Click on the output dataset</code> of the prepare recipe (in this case <code>LOANS_ENRICHED_prepared</code> but you may have renamed your output)</li>
<li>Once selected <code>click on the Python Code recipe</code> from the <code>Actions panel</code></li>
</ul>
<p><img src="assets/dk-26a_python.png" alt="53a"></p>
<ul>
<li>Now <code>Add a new output dataset and click CREATE RECIPE</code></li>
</ul>
<p>Dataiku DSS generates some starter code for us, we can also use code samples our colleagues have created and tagged and, if we prefer, work from Jupyter notebooks or a range of IDE’s. For this lab we will stick with the standard code editor.</p>
<ul>
<li>To save some typing let's <code>change our dataframe name to df</code> on line 8</li>
<li>Remove the to-do starter code on lines <code>11 - 15</code></li>
<li>Replace with the following lines to generate new features</li>
</ul>
<pre class="hljs"><code><div>df['DEBT_AMNT'] = [d*df.INSTALLMENT.values[idx]/100.0 for idx,d in enumerate(df.DTI.values)]


df[&quot;DEBT_AMNT_NORM&quot;] = (df.DEBT_AMNT.values - np.mean(df.DEBT_AMNT.values))/np.std(df.DEBT_AMNT.values)

df[&quot;INSTALL_NORM&quot;] = (df.INSTALLMENT.values - np.mean(df.INSTALLMENT.values))/np.std(df.INSTALLMENT.values)

</div></code></pre>
<ul>
<li>Ensure you replace the name of the dataframe in the final line (.write_with_schema(your_dataframe_name) with df.</li>
</ul>
<p>Your code should now look like this</p>
<p><img src="assets/dk-26-900_Python_Code_highlighted.jpg" alt="53"></p>
<ul>
<li><code>click RUN</code></li>
</ul>
<p>Dataiku DSS allows you to create an arbitrary number of <code>Code environments</code> to address managing dependencies and versions when writing code in R and Python. Code environments in Dataiku DSS are similar to the Python virtual environments. In each location where you can run Python or R code (e.g., code recipes, notebooks, and when performing visual machine learning/deep learning) in your project, you can select which code environment to use.</p>
<!-- ------------------------ -->
<h2 id="training">Training</h2>
<p>Duration: 5</p>
<p>Having sufficiently explored and prepared the loans and employment data, the next stage of the AI lifecycle is to experiment with machine learning models.</p>
<p>This experimentation stage encompasses two key phases: model building and model assessment.</p>
<p><code>Model building</code>: Users have full control over the choice and design of a model — its features, algorithms, hyperparameters and more.</p>
<p><code>Model assessment</code>: Tools such as visualizations and statistical summaries allow users to compare model performance.</p>
<p>These two phases work in tandem to realize the idea of Responsible AI. Either through a visual interface or code, building models with DSS can be transparently done in an automated fashion. At the same time, the model assessment tools provide a window into ensuring the model is not a black box.</p>
<p>Before building our model first we will split our output dataset from our python step.</p>
<p>This is how your flow should look like before splitting
<img src="assets/dk-27_900_finish_flow_before_split.jpg" alt="54"></p>
<ul>
<li>Return to the flow and select the output dataset of the python recipe and the <code>Split</code> recipe from the <code>Actions</code> menu.</li>
<li>Add two datasets named <code>Test</code> and <code>Train</code> and click <code>CREATE RECIPE</code></li>
<li>Choose <code>Dispatch Percentiles</code> as the splitting strategy and have 80% go to Train and 20% to Test.</li>
<li>Choose <code>ISSSUE_DATE_PARSED</code> to sort by and then click <code>RUN</code></li>
</ul>
<p><img src="assets/dk-28-1000_split_tables.jpg" alt="55"></p>
<p><img src="assets/dk-29-1000_percentiles.jpg" alt="56"></p>
<ul>
<li>Return to the flow and select the <code>Train</code> dataset and click the <code>LAB</code> button in the Actions menu</li>
<li>Select <code>AutoML Prediction</code> and set <code>LOAN_STATUS</code> as the target and leave the default template of <code>Quick Prototypes</code> then click <code>CREATE</code></li>
</ul>
<p><img src="assets/dk-30_1100_Lab_button.jpg" alt="57"></p>
<p><img src="assets/dk-31_1100_lab_options.jpg" alt="58"></p>
<p>When building a visual model, users can choose a template instructing DSS to prioritize considerations like speed, performance, and interpretability. Having decided on the basic type of machine learning task, you retain full freedom to adjust the default settings chosen by DSS before training any models. These options include the metric for which to optimize, what features to include, and what algorithms should be tested.</p>
<p>Feel free to try some experiments of your own in the <code>Design</code> tab. Here are some suggestions to try. Don't forget to click :</p>
<ul>
<li>Run with the employment features <code>off/on</code> to see if the Marketplace enrichment data makes a difference to our models accuracy. Click <code>SAVE</code> and then <code>TRAIN</code> in the top right after you've made your changes in <code>DESIGN</code></li>
</ul>
<p><img src="assets/dk-features_emp_off.png" alt="58"></p>
<ul>
<li>While in the <code>Features handling</code> menu look at <strong>MTHS_SINCE_LAST_DELINQ</strong>, <strong>MTHS_SINCE_LAST_RECORD</strong> and <strong>MTHS_SINCE_LAST_MAJOR_DEROG</strong>. In the Distribution table we can see most cells are empty. We have various techniques available to us in the <code>Missing Values</code> dropdown but given that there are so few values in these columns lets just turn reject the features.</li>
</ul>
<p><img src="assets/dk-model_features_months.png" alt="58"></p>
<ul>
<li>
<p>You may notice on the <code>RESULT</code> screen that ML Diagnostics are flagged against a model. These identify and help troubleshoot potential problems and suggest possible improvements at different stages of training and building machine learning models.</p>
</li>
<li>
<p>Hover your cursor over <code>Diagnostics</code> to see the potential issues</p>
</li>
</ul>
<p><img src="assets/dk-model_rebalance1.png" alt="58"></p>
<p>In this example I can see we have an imbalanced dataset, let's fix that.</p>
<ul>
<li>Go to the <code>DESIGN</code> page and the <code>TRAIN/TEST SET</code> menu. Here you can rebablance your dataset.</li>
</ul>
<p><img src="assets/dk-model_rebalance.png" alt="58"></p>
<ul>
<li>
<p>Try different <code>Algorithms</code></p>
</li>
<li>
<p>In <code>Runtime Environment</code> choose Select a <code>container configuration</code> from the drop down for <code>Containerized execution</code> and run with a larger container</p>
</li>
<li>
<p>You can directly compare models from different experiments by selecting them via the <code>checkbox</code> and then selecting <code>Compare</code> from the <code>ACTIONS</code> menu.</p>
</li>
</ul>
<p><img src="assets/dk-model_compare1.png" alt="58"></p>
<ul>
<li>Select <code>Create a new comparison</code> and then click <code>compare</code></li>
</ul>
<p><img src="assets/dk-model_compare.png" alt="58"></p>
<p>After having trained as many models as desired, DSS offers tools for full training management to track and compare model performance across different algorithms. DSS also makes it easy to update models as new data becomes available and to monitor performance across sessions over time.</p>
<p>In the <code>Result</code> pane of any machine learning task, DSS provides a single interface to compare performance in terms of sessions or models, making it easy to find the best performing model in terms of the chosen metric.</p>
<p>In the example below we see an improvement between session 4 and 5 when the employment data is added and then a further minor improvement when using the LightGBM algo</p>
<p><img src="assets/dk-32_1100_model_comparisons.jpg" alt="59"></p>
<p>Clicking on any model produces a full report of tables and visualizations of performance against a range of different possible metrics.</p>
<ul>
<li><code>Click</code> on your best performing model</li>
<li>Step through the various graphs and interactive charts to better understand your model.</li>
<li>For example <code>Subpopulations Analysis</code> allows you to identify potential bias in your model by seeing how it performs across different sub-groups</li>
<li><code>Interactive Scoring</code> allows you to run real time <code>“what-if” analysis</code> to understand the impact of given features</li>
</ul>
<p><img src="assets/dk-33_1100_subpop.jpg" alt="60"></p>
<p>Here we can see <code>Variable importance</code></p>
<p><img src="assets/dk-35-1100_variable_imp.jpg" alt="61"></p>
<!-- ------------------------ -->
<h2 id="deployment">Deployment</h2>
<p>Duration: 2</p>
<p>After experimenting with a range of models built on historic training data, the next stage is to deploy our chosen model to score new, unseen records.</p>
<p>For many AI applications, batch scoring, where new data is collected over some period of time before being passed to the model, is the most effective scoring pattern.
Deploying a model creates a “saved” model in the Flow, together with its lineage. A saved model is the output of a Training recipe which takes as input the original training data used while designing the model.</p>
<ul>
<li>Click on <code>DEPLOY</code>, accept the default model name and click <code>CREATE</code></li>
</ul>
<p>Your flow should now look like this:</p>
<p><img src="assets/dk-34-1200_flow_pre_score.jpg" alt="61"></p>
<!-- ------------------------ -->
<h2 id="scoring-and-evaluation">Scoring and Evaluation</h2>
<p>Duration: 2</p>
<ul>
<li>Select the <code>LOANS_TEST</code> dataset and the <code>Score</code> recipe from the <code>actions menu</code></li>
<li>Select your model</li>
<li>Ensure <code>In-Database (Snowflake native)</code> is selected as the engine in order to use the Java UDF capability</li>
</ul>
<p><img src="assets/dk-36-1200_score_tables.jpg" alt="62"></p>
<p><img src="assets/dk-37-1200_score.jpg" alt="62"></p>
<p>We can now We can see the results back on the Snowflake tab. If you hit the refresh icon near the top left of our screen by your databases, you should see the <code>CREDIT_SCORING_LOANS_TEST_SCORED</code> table that was created once we kicked off our prediction job.</p>
<p><code>Preview Data</code> will give you glimplse of additional column added to the list.</p>
<pre class="hljs"><code><div>USE ROLE SYSADMIN;
USE DATABASE PC_DATAIKU_DB;
USE WAREHOUSE PC_DATAIKU_WH;

SELECT * 
FROM CREDIT_SCORING_LOANS_TEST_SCORED 
LIMIT 10;

</div></code></pre>
<p>Additional info,</p>
<pre class="hljs"><code><div>SELECT 
	EMP_TITLE ,
	SUM(CASE WHEN &quot;prediction&quot; = 'ok' THEN 1 ELSE 0 END) AS prediction_yes,
	SUM(CASE WHEN &quot;prediction&quot; = 'incident' THEN 1 ELSE 0 END) AS prediction_no
	FROM CREDIT_SCORING_LOANS_TEST_SCORED 
GROUP BY 
	EMP_TITLE 
order by prediction_yes DESC;

</div></code></pre>
<!-- ------------------------ -->
<h2 id="conclusion-and-next-steps">Conclusion  and Next Steps</h2>
<p>Duration: 2</p>
<p>Congratulations  you have now successfully built,  deployed and scored your model results back to Snowflake. Your final flow should look like this.</p>
<p><img src="assets/dk-38-1400_complete_flow.jpg" alt="63"></p>
<h2 id="bonus-material---snowpark--python">Bonus Material - Snowpark -Python</h2>
<p>Duration: 5
To be added soon</p>

</body>
</html>
