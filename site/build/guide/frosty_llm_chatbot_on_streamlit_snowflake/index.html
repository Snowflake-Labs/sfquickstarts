
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Frosty: Build an LLM Chatbot in Streamlit on your Snowflake Data</title>

  
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5Q8R2G');</script>
  

  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-08H27EW14N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-08H27EW14N');
</script>

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5Q8R2G"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  
  <google-codelab-analytics gaid="UA-41491190-9"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="frosty_llm_chatbot_on_streamlit_snowflake"
                  title="Frosty: Build an LLM Chatbot in Streamlit on your Snowflake Data"
                  environment="web"
                  feedback-link="https://github.com/Snowflake-Labs/sfguides/issues">
    
      <google-codelab-step label="Overview" duration="2">
        <p class="image-container"><img alt="Preview of final app" src="img/116649d8e7f5dd55.gif"></p>
<p>In this guide, we will build an LLM-powered chatbot named &#34;Frosty&#34; that performs data exploration and answers questions by writing and executing SQL queries on Snowflake data.</p>
<p>The application uses Streamlit and Snowflake and can be plugged into your LLM of choice, alongside data from Snowflake Marketplace. By the end of the session, you will have an interactive web application chatbot that can converse and answer questions based on a financial dataset.</p>
<h2 is-upgraded>Key features &amp; technology</h2>
<ul>
<li>Large language models (LLMs)</li>
<li>Streamlit</li>
<li>Snowflake Marketplace</li>
</ul>
<h2 is-upgraded>What is Streamlit?</h2>
<p>Streamlit is an open-source Python library that enables developers to quickly create, deploy, and share web apps from Python scripts. Learn more about <a href="https://streamlit.io/" target="_blank">Streamlit</a>.</p>
<h2 is-upgraded>What is a large language model (LLM)?</h2>
<p>A large language model, or LLM, is a deep learning algorithm that can recognize, summarize, translate, predict and generate text and other content based on knowledge gained from massive datasets. Some examples of popular LLMs are <a href="https://openai.com/research/gpt-4" target="_blank">GPT-4</a>, <a href="https://openai.com/blog/gpt-3-apps" target="_blank">GPT-3</a>, <a href="https://cloud.google.com/ai-platform/training/docs/algorithms/bert-start" target="_blank">BERT</a>, <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/" target="_blank">LLaMA</a>, and <a href="https://blog.google/technology/ai/lamda/" target="_blank">LaMDA</a>.</p>
<h2 is-upgraded>What is OpenAI?</h2>
<p>OpenAI is the AI research and deployment company behind ChatGPT, GPT-4 (and its predecessors), DALL-E, and other notable offerings. Learn more about <a href="https://openai.com/" target="_blank">OpenAI</a>. We use OpenAI in this guide, but you are welcome to use the large language model of your choice in its place.</p>
<h2 is-upgraded>What is the Snowflake Marketplace?</h2>
<p>The <a href="https://www.snowflake.com/en/data-cloud/marketplace/" target="_blank">Snowflake Marketplace</a> provides users with access to a wide range of datasets from third-party data stewards, expanding the data available for transforming business processes and making decisions. Data providers can publish datasets and offer data analytics services to Snowflake customers. Customers can securely access shared datasets directly from their Snowflake accounts and receive automatic real-time updates.</p>
<h2 is-upgraded>Prerequisites</h2>
<ul>
<li>Accountadmin role access in Snowflake or a <a href="https://signup.snowflake.com/?utm_cta=quickstarts_" target="_blank">Snowflake trial account</a></li>
<li>An API key for OpenAI or another Large Language Model</li>
<li>Basic knowledge of SQL, database concepts, and objects</li>
<li>Familiarity with Python (all code for the lab is provided)</li>
<li>Ability to install and run software on your computer</li>
<li><a href="https://code.visualstudio.com/download" target="_blank">VSCode</a> or the IDE of your choice installed</li>
</ul>
<h2 class="checklist" is-upgraded>What you&#39;ll learn</h2>
<ul class="checklist">
<li>How to create a web application from a Python script with Streamlit</li>
<li>How to build a chatbot in just a few lines of code using <a href="https://docs.streamlit.io/library/api-reference/chat" target="_blank">Streamlit&#39;s new chat UI</a></li>
<li>How to use <a href="https://docs.streamlit.io/library/api-reference/connections/st.experimental_connection" target="_blank"><code>st.experimental_connection</code></a> to connect your Streamlit app to Snowflake</li>
<li>How to use <a href="https://docs.streamlit.io/library/api-reference/session-state" target="_blank"><code>session state</code></a> to store your chatbot&#39;s message history</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Prepare your environment" duration="8">
        <p>Complete the following steps in your local machine (or an equivalent dev environment):</p>
<ol type="1">
<li>Install [Anaconda Distribution](https://docs.conda.io/en/latest/miniconda.html](https://www.anaconda.com/download) to manage a separate environment by selecting the appropriate installer link for your operating system and Python version.</li>
<li>Open the terminal or command prompt and create a folder for your project. Let&#39;s call it <code>llm-chatbot</code>.</li>
<li>Make sure you are running the latest version of conda by running the following command: <pre><code>conda update -n base conda
</code></pre>
</li>
<li>Run the following command to create a Python 3.11 conda virtual environment: <pre><code>conda create --name snowpark-llm-chatbot python=3.11
</code></pre>
</li>
<li>Activate the conda environment by running the following command: <pre><code>conda activate snowpark-llm-chatbot
</code></pre>
</li>
<li>Install Snowpark for Python, Streamlit, and OpenAI by running the following command: <pre><code>conda install snowflake-snowpark-python &#34;openai&gt;=1.0.0&#34;
conda install conda-forge::&#34;streamlit&gt;=1.28.2&#34;
</code></pre>
</li>
</ol>
<h2 is-upgraded>Troubleshooting <code>pyarrow</code> related issues</h2>
<ul>
<li>If you do not have <code>pyarrow</code> installed, you do not need to install it yourself; installing Snowpark automatically installs the appropriate version.</li>
<li>Do not reinstall a different version of <code>pyarrow</code> after installing Snowpark.</li>
</ul>
<h2 is-upgraded>Running in GitHub Codespaces</h2>
<p>If you prefer to run through the tutorial in a remote environment instead of setting up a Python environment locally, you can use GitHub Codespaces.</p>
<ul>
<li>You can launch a pre-configured Codespace <a href="https://codespaces.new/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake?quickstart=1" target="_blank">here</a> with the environment setup and app code already available.</li>
<li>You&#39;ll just need to add a <code>.streamlit/secrets.toml</code> file with configuration for connecting to Snowflake and an OpenAI API Key as described in &#34;Setting up Streamlit environment&#34;.</li>
<li>More information and references on running this quickstart in Codespaces <a href="https://github.com/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake#run-in-codespaces" target="_blank">here</a>.</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Accessing data on Snowflake Marketplace" duration="4">
        <p>Snowflake Marketplace provides visibility to a wide variety of datasets from third-party data stewards which broaden access to data points used to transform business processes. Snowflake Marketplace also removes the need to integrate and model data by providing secure access to data sets fully maintained by the data provider.</p>
<h2 is-upgraded>Log into Snowsight</h2>
<p>If you don&#39;t have a Snowflake account, sign up for a 30-day free trial <a href="https://signup.snowflake.com/?utm_cta=quickstarts_" target="_blank">here</a>.</p>
<ol type="1">
<li>In a supported web browser, navigate to <a href="https://app.snowflake.com" target="_blank">https://app.snowflake.com</a>.</li>
<li>Provide your account name or account URL. If you&#39;ve previously signed in to Snowsight, you might see an account name that you can select.</li>
<li>Sign in using your Snowflake account credentials.</li>
</ol>
<p>You can also access Snowsight from the Classic Console:</p>
<ol type="1">
<li>Sign in to the Classic Console.</li>
<li>In the navigation menu, select Snowsight.</li>
<li>Snowsight opens in a new tab.</li>
</ol>
<h2 is-upgraded>Obtain dataset from Snowflake Marketplace</h2>
<ol type="1">
<li>At the top left corner, make sure you are logged in as ACCOUNTADMIN (switch role to ACCOUNTADMIN if not).</li>
<li>Navigate to the Cybersyn Financial &amp; Economic Essentials listing in the Snowflake Marketplace by clicking <a href="https://app.snowflake.com/marketplace/listing/GZTSZAS2KF7/cybersyn-inc-cybersyn-financial-economic-essentials" target="_blank">here</a>.</li>
<li>Select <strong>&#34;Get.&#34;</strong></li>
<li>Select the appropriate roles to access the database being created and accept the Snowflake consumer terms and Cybersyn&#39;s terms of use.</li>
<li>Select <strong>&#34;Query Data,&#34;</strong> which will open a worksheet with example queries.</li>
</ol>
<p class="image-container"><img alt="Example queries for the Cybersyn Financial &amp; Economic Essentials dataset from the Snowflake Marketplace" src="img/43b582c742f938ce.png"></p>
<h2 is-upgraded>Prep database</h2>
<p>Before building our app, we need to run a set of SQL statements in Snowflake to create two views. The first view is <code>FROSTY_SAMPLE.CYBERSYN_FINANCIAL.FINANCIAL_ENTITY_ATTRIBUTES_LIMITED</code>, which includes:</p>
<ul>
<li>A subset of cybersyn_financial__economic_essentials.cybersyn.financial_institution_attributes: <ul>
<li>Totals for assets, real estate loans, securities, deposits; % of deposits insured; total employees</li>
</ul>
</li>
</ul>
<p>The second view is <code>FROSTY_SAMPLE.CYBERSYN_FINANCIAL.FINANCIAL_ENTITY_ANNUAL_TIME_SERIES</code>, which includes:</p>
<ul>
<li>A modified version of cybersyn_financial__economic_essentials.cybersyn.financial_institution_timeseries as follows: <ul>
<li>Entity and attribute metadata is joined directly <ul>
<li>Only the set of attributes from FINANCIAL_ENTITY_ATTRIBUTES_LIMITED are exposed</li>
<li>Only the end-of-year metrics (YYYY-12-31) are included, and a YEAR column is provided instead of the date column</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>You can copy the SQL statements from <a href="https://github.com/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake/blob/main/src/create-views.sql" target="_blank">this file</a> and run them in the worksheet created for your sample queries.</p>
<p class="image-container"><img alt="GIF showing the SQL statements being run in Snowflake" src="img/9b714ad4856da810.gif"></p>
<p>Now that we&#39;ve configured the dataset we&#39;ll be using for our application, we can get started with Streamlit.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Setting up Streamlit environment" duration="8">
        <h2 is-upgraded>Run an example Streamlit app</h2>
<ol type="1">
<li>Head back over to the command line and navigate to your <code>llm-chatbot</code> folder.</li>
<li>Run an example Streamlit app by entering <code>streamlit hello</code>. <img alt="alt_text" src="img/2f33d933ce3df9bf.png"></li>
</ol>
<h2 is-upgraded>Configure secrets file</h2>
<p>Since our application will connect to Snowflake and OpenAI, we need a way to securely store our credentials. Luckily, <a href="https://docs.streamlit.io/streamlit-community-cloud/get-started/deploy-an-app/connect-to-data-sources/secrets-management" target="_blank">Streamlit&#39;s secrets management feature</a> allows us to store secrets securely and access them in our Streamlit app as environment variables.</p>
<ol type="1">
<li>Add a folder within your <code>llm-chatbot</code> folder called <code>.streamlit</code>. Using the command line, you can do this by entering <code>mkdir .streamlit</code>.</li>
<li>Within the <code>.streamlit</code> folder, add a file called <code>secrets.toml</code>. Using the command line, you can do this by first navigating to the <code>.streamlit</code> folder via <code>cd .streamlit</code> and then entering <code>touch secrets.toml.</code></li>
</ol>
<h3 is-upgraded>Add OpenAI credentials to <code>secrets.toml</code></h3>
<p>We need to add our OpenAI API key to our secrets file. Add your OpenAI key to the secrets file with the following format (replace the placeholder API key with your actual API key).</p>
<pre><code language="language-toml" class="language-toml"># .streamlit/secrets.toml

OPENAI_API_KEY = &#34;sk-2v...X&#34;
</code></pre>
<h3 is-upgraded>Add Snowflake credentials to <code>secrets.toml</code></h3>
<p>We also need to add the Snowflake <code>user</code>, <code>password</code>, <code>warehouse</code>, <code>role</code>, and <code>account</code> to our secrets file. Copy the following format, replacing the placeholder credentials with your actual credentials. <code>account</code> should be your Snowflake account identifier, which you can locate by following the instructions outlined <a href="https://docs.snowflake.com/en/user-guide/admin-account-identifier" target="_blank">here</a>.</p>
<p>If you prefer to use browser-based SSO to authenticate, replace <code>password = "<my_trial_pass>"</code> with <code>authenticator=EXTERNALBROWSER</code>.</p>
<pre><code language="language-toml" class="language-toml"># .streamlit/secrets.toml

[connections.snowflake]
user = &#34;&lt;jdoe&gt;&#34;
password = &#34;&lt;my_trial_pass&gt;&#34;
warehouse = &#34;COMPUTE_WH&#34;
role = &#34;ACCOUNTADMIN&#34;
account = &#34;&lt;account-id&gt;&#34;
</code></pre>
<h3 is-upgraded>Full contents of secrets.toml</h3>
<pre><code language="language-toml" class="language-toml"># .streamlit/secrets.toml

OPENAI_API_KEY = &#34;sk-2v...X&#34;

[connections.snowflake]
user = &#34;&lt;username&gt;&#34;
password = &#34;&lt;password&gt;&#34;
warehouse = &#34;COMPUTE_WH&#34;
role = &#34;ACCOUNTADMIN&#34;
account = &#34;&lt;account-id&gt;&#34;
</code></pre>
<h2 is-upgraded>Validate credentials</h2>
<p>Let&#39;s validate that our Snowflake and OpenAI credentials are working as expected.</p>
<h3 is-upgraded>OpenAI credentials</h3>
<p>First, we&#39;ll validate our OpenAI credentials by asking GPT-3.5 a simple question: what is Streamlit?</p>
<ol type="1">
<li>Add a file called <code>validate_credentials.py</code> at the root of your <code>llm-chatbot</code> folder.</li>
<li>Add the below code to <code>validate_credentials.py</code>. This snippet does the following: <ul>
<li>Imports the Streamlit and OpenAI Python packages</li>
<li>Retrieves our OpenAI API key from the secrets file</li>
<li>Sends GPT-3.5 the question &#34;What is Streamlit?&#34;</li>
<li>Prints GPT-3.5&#39;s response to the UI using <code>st.write</code></li>
</ul>
</li>
</ol>
<pre><code language="language-python" class="language-python">import streamlit as st
from openai import OpenAI

client = OpenAI(api_key=st.secrets[&#34;OPENAI_API_KEY&#34;])

completion = client.chat.completions.create(
  model=&#34;gpt-3.5-turbo&#34;,
  messages=[
    {&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: &#34;What is Streamlit?&#34;}
  ]
)

st.write(completion.choices[0].message.content)
</code></pre>
<ol type="1" start="3">
<li>Run your Streamlit app by entering <code>streamlit run validate_credentials.py</code> in the command line. <img alt="alt_text" src="img/d410955289bccb8f.png"></li>
</ol>
<h3 is-upgraded>Snowflake credentials</h3>
<p>Next, let&#39;s validate that our Snowflake credentials are working as expected.</p>
<ol type="1">
<li>Append the following to <code>validate_credentials.py</code>. This snippet does the following: <ul>
<li>Creates a Snowpark connection</li>
<li>Executes a query to pull the current warehouse and writes the result to the UI</li>
</ul>
</li>
</ol>
<pre><code language="language-python" class="language-python">conn = st.connection(&#34;snowflake&#34;)
df = conn.query(&#34;select current_warehouse()&#34;)
st.write(df)
</code></pre>
<ol type="1" start="2">
<li>Run your Streamlit app by entering <code>streamlit run validate_credentials.py</code> in the command line. <img alt="alt_text" src="img/83912b7fe9fb21f3.png"></li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Build a simple chatbot application" duration="8">
        <p>We&#39;re ready to start building our app! We&#39;re going to first build a simple version of the chatbot app that simply passes user-inputted messages to GPT-3.5 and returns GPT-3.5&#39;s response. We&#39;ll build on the app&#39;s complexity in subsequent sections.</p>
<p>We&#39;ll break down the Python file snippet-by-snippet so that you understand the functionality of each section, but if you&#39;d like to skip ahead and download the full file, you can do so <a href="https://github.com/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake/blob/main/src/simple_chatbot.py" target="_blank">here</a>.</p>
<ol type="1">
<li>Create a file called <code>simple_chatbot.py</code>. Add import statements and give your app a title.</li>
</ol>
<pre><code language="language-python" class="language-python">from openai import OpenAI
import streamlit as st

st.title(&#34;☃️ Frosty&#34;)
</code></pre>
<ol type="1" start="2">
<li>Initialize the chatbot&#39;s message history by adding the first message that we want the chatbot to display, &#34;How can I help?&#34;, to <a href="https://docs.streamlit.io/library/api-reference/session-state" target="_blank">session state</a>.</li>
</ol>
<pre><code language="language-python" class="language-python"># Initialize the chat messages history
if &#34;messages&#34; not in st.session_state.keys():
    st.session_state.messages = [{&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;How can I help?&#34;}]
</code></pre>
<ol type="1" start="3">
<li>Prompt the user to enter chat input by using Streamlit&#39;s <code>st.chat_input()</code> feature. If the user has entered a message, add that message to the chat history by storing it in session state.</li>
</ol>
<pre><code language="language-python" class="language-python"># Prompt for user input and save
if prompt := st.chat_input():
    st.session_state.messages.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt})
</code></pre>
<ol type="1" start="4">
<li>Display the chatbot&#39;s message history by iterating through the values stored in session state associated with the key &#34;messages&#34; and printing each value.</li>
</ol>
<pre><code language="language-python" class="language-python"># display the existing chat messages
for message in st.session_state.messages:
    with st.chat_message(message[&#34;role&#34;]):
        st.write(message[&#34;content&#34;])
</code></pre>
<ol type="1" start="4">
<li>If the last message is not from the assistant, send the message to GPT-3.5 via the <code>openai</code> Python package. Display a spinner while the app is retrieving GPT-3.5&#39;s response via Streamlit&#39;s <a href="https://docs.streamlit.io/library/api-reference/status/st.spinner" target="_blank"><code>st.spinner</code></a> feature and use <code>st.write</code> to display the chatbot&#39;s response in the UI. Append the chatbot&#39;s response to the chat history stored in session state.</li>
</ol>
<pre><code language="language-python" class="language-python"># If last message is not from assistant, we need to generate a new response
if st.session_state.messages[-1][&#34;role&#34;] != &#34;assistant&#34;:
    # Call LLM
    with st.chat_message(&#34;assistant&#34;):
        with st.spinner(&#34;Thinking...&#34;):
            r = OpenAI().chat.completions.create(
                messages=[{&#34;role&#34;: m[&#34;role&#34;], &#34;content&#34;: m[&#34;content&#34;]} for m in st.session_state.messages],
                model=&#34;gpt-3.5-turbo&#34;,
            )
            response = r.choices[0].message.content
            st.write(response)

    message = {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: response}
    st.session_state.messages.append(message)
</code></pre>
<ol type="1" start="6">
<li>Run the Streamlit app via <code>streamlit run simple_chatbot.py</code>. Give it a whirl – ask Frosty a question!</li>
</ol>
<p class="image-container"><img alt="GIF demonstrating the simple chatbot app" src="img/82adedbf545f2b38.gif"></p>
<p>The full contents of the Python file for this simple chatbot app are below, or you can download the file from <a href="https://github.com/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake/blob/main/src/simple_chatbot.py" target="_blank">GitHub</a>.</p>
<pre><code language="language-python" class="language-python">from openai import OpenAI
import streamlit as st

st.title(&#34;☃️ Frosty&#34;)

# Initialize the chat messages history
if &#34;messages&#34; not in st.session_state.keys():
    st.session_state.messages = [{&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: &#34;How can I help?&#34;}]

# Prompt for user input and save
if prompt := st.chat_input():
    st.session_state.messages.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt})

# display the existing chat messages
for message in st.session_state.messages:
    with st.chat_message(message[&#34;role&#34;]):
        st.write(message[&#34;content&#34;])

# If last message is not from assistant, we need to generate a new response
if st.session_state.messages[-1][&#34;role&#34;] != &#34;assistant&#34;:
    # Call LLM
    with st.chat_message(&#34;assistant&#34;):
        with st.spinner(&#34;Thinking...&#34;):
            r = OpenAI().chat.completions.create(
                messages=[{&#34;role&#34;: m[&#34;role&#34;], &#34;content&#34;: m[&#34;content&#34;]} for m in st.session_state.messages],
                model=&#34;gpt-3.5-turbo&#34;,
            )
            response = r.choices[0].message.content
            st.write(response)

    message = {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: response}
    st.session_state.messages.append(message)
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Add prompt engineering and SQL extraction" duration="10">
        <p>Now that we&#39;ve built a simple version of the chatbot app, let&#39;s expand the functionality to enable Frosty to translate our requests into SQL statements and execute those statements using the Cybersyn dataset stored in our Snowflake database.</p>
<h2 is-upgraded>Create a prompt file</h2>
<p>We&#39;re also going to create a prompt Python file before building out the main file of our chatbot app. The primary purpose of this file is to create the function <code>get_system_prompt()</code>, which will be called in our main Python file and will do a few things:</p>
<ul>
<li>Retrieves basic information about the database we&#39;re going to be using, including the table name, table description, and variable names</li>
<li>Composes a system message for GPT-3.5, which shares basic information about the dataset with the model and instructs the model to: <ul>
<li>Respond in the character of an AI Snowflake SQL expert named Frosty</li>
<li>Include a SQL query in each answer based on the question and the table.</li>
<li>Format SQL queries properly via markdown.</li>
<li>Limit the number of responses to a SQL query to 10 (unless otherwise specified).</li>
<li>Generate a single SQL code snippet.</li>
<li>Only use the specified table columns and table.</li>
<li>Avoid starting variable names with numbers.</li>
<li>Use &#34;ilike %keyword%&#34; for fuzzy match queries.</li>
<li>Start the conversation by briefly introducing yourself, describing the table, sharing available metrics in a few sentences, and providing three example questions.</li>
</ul>
</li>
</ul>
<p>This file should be placed in the root of your <code>llm-chatbot</code> folder. You can download the file from <a href="https://github.com/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake/blob/main/src/prompts.py" target="_blank">here</a> or create an empty Python file and paste the following code:</p>
<pre><code language="language-python" class="language-python">import streamlit as st

SCHEMA_PATH = st.secrets.get(&#34;SCHEMA_PATH&#34;, &#34;FROSTY_SAMPLE.CYBERSYN_FINANCIAL&#34;)
QUALIFIED_TABLE_NAME = f&#34;{SCHEMA_PATH}.FINANCIAL_ENTITY_ANNUAL_TIME_SERIES&#34;
TABLE_DESCRIPTION = &#34;&#34;&#34;
This table has various metrics for financial entities (also referred to as banks) since 1983.
The user may describe the entities interchangeably as banks, financial institutions, or financial entities.
&#34;&#34;&#34;
# This query is optional if running Frosty on your own table, especially a wide table.
# Since this is a deep table, it&#39;s useful to tell Frosty what variables are available.
# Similarly, if you have a table with semi-structured data (like JSON), it could be used to provide hints on available keys.
# If altering, you may also need to modify the formatting logic in get_table_context() below.
METADATA_QUERY = f&#34;SELECT VARIABLE_NAME, DEFINITION FROM {SCHEMA_PATH}.FINANCIAL_ENTITY_ATTRIBUTES_LIMITED;&#34;

GEN_SQL = &#34;&#34;&#34;
You will be acting as an AI Snowflake SQL Expert named Frosty.
Your goal is to give correct, executable sql query to users.
You will be replying to users who will be confused if you don&#39;t respond in the character of Frosty.
You are given one table, the table name is in &lt;tableName&gt; tag, the columns are in &lt;columns&gt; tag.
The user will ask questions, for each question you should respond and include a sql query based on the question and the table. 

{context}

Here are 6 critical rules for the interaction you must abide:
&lt;rules&gt;
1. You MUST MUST wrap the generated sql code within ``` sql code markdown in this format e.g
```sql
(select 1) union (select 2)
```
2. If I don&#39;t tell you to find a limited set of results in the sql query or question, you MUST limit the number of responses to 10.
3. Text / string where clauses must be fuzzy match e.g ilike %keyword%
4. Make sure to generate a single snowflake sql code, not multiple. 
5. You should only use the table columns given in &lt;columns&gt;, and the table given in &lt;tableName&gt;, you MUST NOT hallucinate about the table names
6. DO NOT put numerical at the very front of sql variable.
&lt;/rules&gt;

Don&#39;t forget to use &#34;ilike %keyword%&#34; for fuzzy match queries (especially for variable_name column)
and wrap the generated sql code with ``` sql code markdown in this format e.g:
```sql
(select 1) union (select 2)
```

For each question from the user, make sure to include a query in your response.

Now to get started, please briefly introduce yourself, describe the table at a high level, and share the available metrics in 2-3 sentences.
Then provide 3 example questions using bullet points.
&#34;&#34;&#34;

@st.cache_data(show_spinner=&#34;Loading Frosty&#39;s context...&#34;)
def get_table_context(table_name: str, table_description: str, metadata_query: str = None):
    table = table_name.split(&#34;.&#34;)
    conn = st.connection(&#34;snowflake&#34;)
    columns = conn.query(f&#34;&#34;&#34;
        SELECT COLUMN_NAME, DATA_TYPE FROM {table[0].upper()}.INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = &#39;{table[1].upper()}&#39; AND TABLE_NAME = &#39;{table[2].upper()}&#39;
        &#34;&#34;&#34;, show_spinner=False,
    )
    columns = &#34;\n&#34;.join(
        [
            f&#34;- **{columns[&#39;COLUMN_NAME&#39;][i]}**: {columns[&#39;DATA_TYPE&#39;][i]}&#34;
            for i in range(len(columns[&#34;COLUMN_NAME&#34;]))
        ]
    )
    context = f&#34;&#34;&#34;
Here is the table name &lt;tableName&gt; {&#39;.&#39;.join(table)} &lt;/tableName&gt;

&lt;tableDescription&gt;{table_description}&lt;/tableDescription&gt;

Here are the columns of the {&#39;.&#39;.join(table)}

&lt;columns&gt;\n\n{columns}\n\n&lt;/columns&gt;
    &#34;&#34;&#34;
    if metadata_query:
        metadata = conn.query(metadata_query, show_spinner=False)
        metadata = &#34;\n&#34;.join(
            [
                f&#34;- **{metadata[&#39;VARIABLE_NAME&#39;][i]}**: {metadata[&#39;DEFINITION&#39;][i]}&#34;
                for i in range(len(metadata[&#34;VARIABLE_NAME&#34;]))
            ]
        )
        context = context + f&#34;\n\nAvailable variables by VARIABLE_NAME:\n\n{metadata}&#34;
    return context

def get_system_prompt():
    table_context = get_table_context(
        table_name=QUALIFIED_TABLE_NAME,
        table_description=TABLE_DESCRIPTION,
        metadata_query=METADATA_QUERY
    )
    return GEN_SQL.format(context=table_context)

# do `streamlit run prompts.py` to view the initial system prompt in a Streamlit app
if __name__ == &#34;__main__&#34;:
    st.header(&#34;System prompt for Frosty&#34;)
    st.markdown(get_system_prompt())
</code></pre>
<p>Finally, you can run this file as a Streamlit app to verify the output is working correctly. Run the prompts generation via <code>streamlit run prompts.py</code>. Make sure the table information is showing up as expected in the rendered prompt - this will get passed to the chatbot in the next section.</p>
<h2 is-upgraded>Build the chatbot</h2>
<p>We&#39;ll break down the Python file snippet-by-snippet so that you understand the functionality of each section, but if you&#39;d like to skip ahead and download the full file, you can do so <a href="https://github.com/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake/blob/main/src/frosty_app.py" target="_blank">here</a>.</p>
<ol type="1">
<li>Create a file called <code>frosty_app.py</code> and add the below code snippet, which does the following: <ul>
<li>Adds import statements and a title</li>
<li>Retrieves our OpenAI API key from the secrets file</li>
<li>Initializes the message history using session state <ul>
<li>This time, the first assistant message from the chatbot will display information about the current table in the database this app is using. <code>get_system_prompt()</code> retrieves this information.</li>
</ul>
</li>
<li>Prompts the user to enter a message and upon receiving a message, adds that message to the chat history</li>
<li>Iterates through the message history and displays each message in the app</li>
</ul>
</li>
</ol>
<pre><code language="language-python" class="language-python">from openai import OpenAI
import re
import streamlit as st
from prompts import get_system_prompt

st.title(&#34;☃️ Frosty&#34;)

# Initialize the chat messages history
client = OpenAI(api_key=st.secrets.OPENAI_API_KEY)
if &#34;messages&#34; not in st.session_state:
    # system prompt includes table information, rules, and prompts the LLM to produce
    # a welcome message to the user.
    st.session_state.messages = [{&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: get_system_prompt()}]

# Prompt for user input and save
if prompt := st.chat_input():
    st.session_state.messages.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt})

# display the existing chat messages
for message in st.session_state.messages:
    if message[&#34;role&#34;] == &#34;system&#34;:
        continue
    with st.chat_message(message[&#34;role&#34;]):
        st.write(message[&#34;content&#34;])
        if &#34;results&#34; in message:
            st.dataframe(message[&#34;results&#34;])
</code></pre>
<ol type="1" start="2">
<li>Check the last entry in the chat history to see if it was sent by the user or the chatbot. If it was sent by the user, use GPT-3.5 to generate a response. Instead of displaying the entire response at once, use OpenAI&#39;s <code>stream</code> parameter to signify that GPT-3.5&#39;s response should be sent incrementally in chunks via an event stream, and display the chunks as they&#39;re received.</li>
</ol>
<pre><code language="language-python" class="language-python"># If last message is not from assistant, we need to generate a new response
if st.session_state.messages[-1][&#34;role&#34;] != &#34;assistant&#34;:
    with st.chat_message(&#34;assistant&#34;):
        response = &#34;&#34;
        resp_container = st.empty()
        for delta in client.chat.completions.create(
            model=&#34;gpt-3.5-turbo&#34;,
            messages=[{&#34;role&#34;: m[&#34;role&#34;], &#34;content&#34;: m[&#34;content&#34;]} for m in st.session_state.messages],
            stream=True,
        ):
            response += (delta.choices[0].delta.content or &#34;&#34;)
            resp_container.markdown(response)
</code></pre>
<ol type="1" start="3">
<li>Use a regular expression to search the newly generated response for the SQL markdown syntax that we instructed GPT-3.5 to wrap any SQL queries in. If a match is found, use <code>st.experimental_connection</code> to execute the SQL query against the database we created in Snowflake. Write the result to the app using <code>st.dataframe</code>, and append the result to the associated message in the message history.</li>
</ol>
<pre><code language="language-python" class="language-python">        message = {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: response}
        # Parse the response for a SQL query and execute if available
        sql_match = re.search(r&#34;```sql\n(.*)\n```&#34;, response, re.DOTALL)
        if sql_match:
            sql = sql_match.group(1)
            conn = st.connection(&#34;snowflake&#34;)
            message[&#34;results&#34;] = conn.query(sql)
            st.dataframe(message[&#34;results&#34;])
        st.session_state.messages.append(message)
</code></pre>
<ol type="1" start="4">
<li>Run the Streamlit app via <code>streamlit run frosty_app.py</code>.</li>
</ol>
<p class="image-container"><img alt="Preview of final app" src="img/116649d8e7f5dd55.gif"></p>
<p>The full contents of the Python file for this app are below, or you can download the file from <a href="https://github.com/Snowflake-Labs/sfguide-frosty-llm-chatbot-on-streamlit-snowflake/blob/main/src/frosty_app.py" target="_blank">GitHub</a>.</p>
<pre><code language="language-python" class="language-python">from openai import OpenAI
import re
import streamlit as st
from prompts import get_system_prompt

st.title(&#34;☃️ Frosty&#34;)

# Initialize the chat messages history
client = OpenAI(api_key=st.secrets.OPENAI_API_KEY)
if &#34;messages&#34; not in st.session_state:
    # system prompt includes table information, rules, and prompts the LLM to produce
    # a welcome message to the user.
    st.session_state.messages = [{&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: get_system_prompt()}]

# Prompt for user input and save
if prompt := st.chat_input():
    st.session_state.messages.append({&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: prompt})

# display the existing chat messages
for message in st.session_state.messages:
    if message[&#34;role&#34;] == &#34;system&#34;:
        continue
    with st.chat_message(message[&#34;role&#34;]):
        st.write(message[&#34;content&#34;])
        if &#34;results&#34; in message:
            st.dataframe(message[&#34;results&#34;])

# If last message is not from assistant, we need to generate a new response
if st.session_state.messages[-1][&#34;role&#34;] != &#34;assistant&#34;:
    with st.chat_message(&#34;assistant&#34;):
        response = &#34;&#34;
        resp_container = st.empty()
        for delta in client.chat.completions.create(
            model=&#34;gpt-3.5-turbo&#34;,
            messages=[{&#34;role&#34;: m[&#34;role&#34;], &#34;content&#34;: m[&#34;content&#34;]} for m in st.session_state.messages],
            stream=True,
        ):
            response += (delta.choices[0].delta.content or &#34;&#34;)
            resp_container.markdown(response)

        message = {&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: response}
        # Parse the response for a SQL query and execute if available
        sql_match = re.search(r&#34;```sql\n(.*)\n```&#34;, response, re.DOTALL)
        if sql_match:
            sql = sql_match.group(1)
            conn = st.connection(&#34;snowflake&#34;)
            message[&#34;results&#34;] = conn.query(sql)
            st.dataframe(message[&#34;results&#34;])
        st.session_state.messages.append(message)
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Explore the data via natural language" duration="5">
        <p>Finally, it&#39;s time to explore the Cybersyn Financial &amp; Economic Essentials using natural language. Try asking Frosty any of the following questions:</p>
<ol type="1">
<li>Which financial institution had the highest total assets in the year 2020?</li>
<li>Which financial institutions in California had the highest total assets value between 2010 to 2015?</li>
<li>What was the highest % insured (estimated) value for all financial institutions in the state of New Jersey?</li>
<li>What is the lowest value of total securities for all financial institutions in Texas?</li>
<li>What was the % change in all real estate loans for banks headquartered in California between 2015 and 2020?</li>
<li>What was the average total securities value for banks in the state of Wisconsin between 2015 and 2020?</li>
<li>How have the total securities value changed over time for financial institutions in New York City?</li>
<li>What was the maximum % insured (estimated) value for a single financial entity in Illinois between 2010 and 2020?</li>
<li>What was the value of all real estate loans for banks located in Massachusetts in 2020?</li>
<li>How many banks headquartered in New Hampshire experienced more than 50% growth in their total assets between 2015 and 2020?</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Conclusion and next steps" duration="1">
        <p>Congratulations – you&#39;ve just built an LLM-powered chatbot capable of translating natural language to SQL queries and running those queries on data stored in Snowflake!</p>
<h2 is-upgraded>Where to go from here</h2>
<p>This tutorial is just a starting point for exploring the possibilities of LLM-powered chat interfaces for data exploration and question-answering using Snowflake and Streamlit. A few next things to try:</p>
<ul>
<li><strong>Update to run against your private data in Snowflake</strong>, or other relevant Snowflake Marketplace datasets. The table-specific logic in the app is all specified at the top of <code>prompts.py</code>, so it should be easy to swap and start playing around!</li>
<li><strong>Add more capabilities</strong>, such as using the LLM to choose from a set of available tables, summarize the returned data, or even write Streamlit code to visualize the results. You could even use a library like LangChain to convert Frosty into an &#34;Agent&#34; with improved chain of thought reasoning and the ability to respond to errors.</li>
<li><strong>Prepare to run in Streamlit in Snowflake</strong> (currently in Private Preview): The functionality shown here will soon be available in Streamlit in Snowflake, especially when paired with External Access (also in Private Preview) to simplify access to an external LLM.</li>
</ul>
<p>Check out the Frosty session (ML103) from Snowflake Summit 2023 for more ideas and what&#39;s coming soon from Snowflake!</p>
<h2 is-upgraded>Additional resources</h2>
<p>Want to learn more about the tools and technologies used by your app? Check out the following resources:</p>
<ul>
<li><a href="https://docs.streamlit.io/library/api-reference/chat" target="_blank">Streamlit&#39;s new chat UI</a></li>
<li><a href="https://docs.streamlit.io/library/api-reference/connections/st.experimental_connection" target="_blank">st.experimental_connection</a></li>
<li><a href="https://docs.streamlit.io/library/api-reference/session-state" target="_blank">Session state</a></li>
<li><a href="https://docs.streamlit.io/streamlit-community-cloud/get-started/deploy-an-app/connect-to-data-sources/secrets-management" target="_blank">Secrets management</a></li>
<li><a href="https://platform.openai.com/docs/api-reference/chat" target="_blank">OpenAI&#39;s ChatCompetion feature</a></li>
<li><a href="https://blog.streamlit.io/generative-ai-and-streamlit-a-perfect-match/" target="_blank">Generative AI and Streamlit: A perfect match</a></li>
<li><a href="https://streamlit.io/generative-ai" target="_blank">Build powerful generative AI apps with Streamlit</a></li>
<li><a href="https://developers.snowflake.com/demos/data-exploration-llm-chatbot/" target="_blank">Demo on Snowflake Demo Hub</a></li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/native-shim.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/custom-elements.min.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/prettify.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>
  <script type="text/javascript">
    window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=t.forceSSL||"https:"===document.location.protocol,a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src=(r?"https:":"http:")+"//cdn.heapanalytics.com/js/heap-"+e+".js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(a,n);for(var o=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","removeEventProperty","setEventProperties","track","unsetEventProperty"],c=0;c<p.length;c++)heap[p[c]]=o(p[c])};
    heap.load("2025084205");
  </script>
</body>
</html>
