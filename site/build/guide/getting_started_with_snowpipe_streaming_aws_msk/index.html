
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Getting Started with Snowpipe Streaming and Amazon MSK</title>

  
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-5Q8R2G');</script>
  

  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-08H27EW14N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-08H27EW14N');
</script>

  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5Q8R2G"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  
  <google-codelab-analytics gaid="UA-41491190-9"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="getting_started_with_snowpipe_streaming_aws_msk"
                  title="Getting Started with Snowpipe Streaming and Amazon MSK"
                  environment="web"
                  feedback-link="https://github.com/Snowflake-Labs/sfguides/issues">
    
      <google-codelab-step label="Overview" duration="5">
        <p>Snowflake&#39;s Snowpipe streaming capabilities are designed for rowsets with variable arrival frequency. It focuses on lower latency and cost for smaller data sets. This helps data workers stream rows into Snowflake without requiring files with a more attractive cost/latency profile.</p>
<p>Here are some of the use cases that can benefit from Snowpipe streaming:</p>
<ul>
<li>IoT time-series data ingestion</li>
<li>CDC streams from OLTP systems</li>
<li>Log ingestion from SIEM systems</li>
<li>Ingestion into ML feature stores</li>
</ul>
<p>In our demo, we will use real-time commercial flight data over the San Francisco Bay Area from the <a href="https://opensky-network.org" target="_blank">Opensky Network</a> to illustrate the solution using Snowflake&#39;s Snowpipe streaming and <a href="https://aws.amazon.com/msk/" target="_blank">MSK (Amazon Managed Streaming for Apache Kafka)</a>.</p>
<p>Note that you can either stream the data into a regular Snowflake table or a <a href="https://docs.snowflake.com/en/user-guide/tables-iceberg" target="_blank">Snowflake managed Apache Iceberg table</a> depending on your use case.</p>
<p>The architecture diagram below shows the deployment. An MSK cluster and a Linux EC2 instance (jumphost) will be provisioned in private subnets of an AWS VPC. The Linux jumphost will host the Kafka producer and Snowpipe streaming via <a href="https://docs.snowflake.com/en/user-guide/kafka-connector-overview.html" target="_blank">Kafka Connect</a>.</p>
<p>The Kafka producer calls the data sources&#39; REST API and receives time-series data in JSON format. This data is then ingested into the Kafka cluster before being picked up by the Kafka connector and delivered to a Snowflake table. The data in Snowflake table can be visualized in real-time with <a href="https://aws.amazon.com/grafana/" target="_blank">AMG (Amazon Managed Grafana)</a> and <a href="https://streamlit.io" target="_blank">Streamlit</a> The historical data can also be analyzed by BI tools like <a href="https://aws.amazon.com/quicksight/?trk=56601b48-df3f-4cb4-9ef7-9f52efa1d0b8&sc_channel=ps&ef_id=Cj0KCQiA_bieBhDSARIsADU4zLebWWM6ZmxRODjR9Xlc7ztNm5JGwqEMSi0EjCLZ9CXYa1YvXL3LMYYaAnV_EALw_wcB:G:s&s_kwcid=AL!4422!3!629393324770!!!g!!" target="_blank">Amazon Quicksight</a>.</p>
<p class="image-container"><img alt="Architecture diagram for the Demo" src="img/de5b2af138096c4d.png"></p>
<p class="image-container"><img alt="Data visualization" src="img/bc685d6756700e6d.png"></p>
<h2 is-upgraded>Prerequisites</h2>
<ul>
<li>Familiarity with Snowflake, basic SQL knowledge, Snowsight UI and Snowflake objects</li>
<li>Familiarity with AWS Services (e.g. EC2, MSK, etc), Networking and the Management Console</li>
<li>Basic knowledge of Python and Linux shell scripting</li>
</ul>
<h2 is-upgraded>What You&#39;ll Need Before the Lab</h2>
<p>To participate in the virtual hands-on lab, attendees need the following resources.</p>
<ul>
<li>A <a href="https://signup.snowflake.com/?utm_cta=quickstarts_" target="_blank">Snowflake Enterprise Account on preferred AWS region</a> with <code>ACCOUNTADMIN</code> access</li>
<li>An <a href="https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/" target="_blank">AWS Account</a> with <code>Administrator Access</code></li>
<li>Create your own VPC and subnets (This is optional if you have an existing VPC with subnets you can leverage. Please refer to this <a href="https://docs.aws.amazon.com/whitepapers/latest/amazon-msk-migration-guide/amazon-managed-streaming-for-apache-kafka-amazon-msk.html" target="_blank">AWS document</a> for the MSK networking topology) <ul>
<li>In the AWS account, <a href="https://docs.aws.amazon.com/vpc/latest/userguide/working-with-vpcs.html" target="_blank">create a VPC</a>, preferrably in the same region as the Snowflake account</li>
<li>In the VPC, <a href="https://docs.aws.amazon.com/vpc/latest/userguide/working-with-subnets.html" target="_blank">create subnets</a> and attach an <a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html" target="_blank">internet gateway</a> to allow egress traffic to the internet by using a routing table and security group for outbound traffic. Note that the subnets can be public or private, for private subnets, you will need to attach a <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html" target="_blank">NAT gateway</a> to allow egress traffic to the internet. Public subnets are sufficient for this lab.</li>
<li>Now if you have decided to create your own VPC/subnets, for your convenience, click <a href="https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=MSK-Snowflake-VPC&templateURL=https://snowflake-corp-se-workshop.s3.us-west-1.amazonaws.com/VHOL_Snowflake_Snowpipe_Streaming_MSK/MyFullVPC-2pub-2priv.json" target="_blank">here</a> to deploy a VPC with a pair of public and private subnets, internet gateway and NAT gateway for you. Note that you must have network administrator permissions to deploy these resources.</li>
</ul>
</li>
</ul>
<h2 class="checklist" is-upgraded>What You&#39;ll Learn</h2>
<ul class="checklist">
<li>Using <a href="https://aws.amazon.com/msk/" target="_blank">MSK (Amazon Managed Streaming for Apache Kafka)</a></li>
<li>Connecting to EC2 instances with <a href="https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html" target="_blank">Amazon System Session Manager</a>, this is an alternative to SSH if your instance is in a private subnet</li>
<li>Using <a href="https://docs.snowflake.com/en/user-guide/snowsql.html" target="_blank">SnowSQL</a>, the command line client for connecting to Snowflake to execute SQL queries and perform all DDL and DML operations, including loading data into and unloading data out of database tables.</li>
<li>Using Snowflake to query tables populated with time-series data</li>
</ul>
<h2 is-upgraded>What You&#39;ll Build</h2>
<ul>
<li><a href="https://docs.aws.amazon.com/msk/latest/developerguide/msk-create-cluster.html" target="_blank">Create a provisioned Kafka cluster</a></li>
<li>Create Kafka producers and connectors</li>
<li>Create topics in a Kafka cluster</li>
<li>A Snowflake database for hosting real-time flight data</li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Create a provisioned Kafka cluster and a Linux jumphost in AWS" duration="30">
        <h3 is-upgraded>1. Create an MSK cluster and an EC2 instance</h3>
<p>The MSK cluster is created in a VPC managed by Amazon. We will deploy our Kafka clients in our own VPC and use security groups to ensure the communications between the MSK cluster and clients are secure.</p>
<p>First, click <a href="https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=MSK-Snowflake&templateURL=https://snowflake-corp-se-workshop.s3.us-west-1.amazonaws.com/VHOL_Snowflake_Snowpipe_Streaming_MSK/msk-CFT-for-SE-Sandbox.json" target="_blank">here</a> to launch a provisioned MSK cluster. Note the default AWS region is <code>us-west-2 (Oregon)</code>, feel free to select a region you prefer to deploy the environment.</p>
<p>Click <code>Next</code> at the <code>Create stack</code> page. Set the Stack name or modify the default value to customize it to your identity. Leave the default Kafka version as is. For <code>Subnet1</code> and <code>Subnet2</code>, in the drop-down menu, pick two different subnets respectively, they can be either public or private subnets depending on the network layout of your VPC. Please note that if you plan to use <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank">Amazon MSK Connect</a> later, you should choose private subnets here. For <code>MSKSecurityGroupId</code>, we recommend using the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/default-custom-security-groups.html" target="_blank">default security group</a> in your VPC, if you do not have the default security group, <a href="https://docs.aws.amazon.com/vpc/latest/userguide/default-security-group.html" target="_blank">create one</a> on your own before moving forward. Leave <code>TLSMutualAuthentication</code> as false and the jumphost instance type and AMI id as default before clicking <code>Next</code>.</p>
<p>See below sample screen capture for reference.</p>
<p class="image-container"><img src="img/bb4af4e9a4e9831f.png"></p>
<p>Leave everything as default in the <code>Configure stack options</code> page and click <code>Next</code>. In the <code>Review</code> page, click <code>Submit</code>.</p>
<p>In about 10-30 minutes depending on your AWS region, the Cloudformation template provisions an MSK cluster with two brokers. It will also provision a Linux EC2 instance in the subnet you selected. We will then use it to run the Kafka connector with Snowpipe streaming SDK and the producer.</p>
<h3 is-upgraded>2. Configure the Linux session for timeout and default shell</h3>
<p>In this step we need to connect to the EC2 instance in order to interact with the MSK cluster.</p>
<p>Go to the AWS <a href="https://us-west-2.console.aws.amazon.com/systems-manager/home" target="_blank">Systems Manager</a> console in the same region where you setup the MSK cluster, Click <code>Session Manager</code> on the left pane.</p>
<p class="image-container"><img src="img/f1625af9fff73b45.png"></p>
<p>Next, we will set the preferred shell as bash.</p>
<p>Click the <code>Preferences</code> tab. <img src="img/bb9a12f7155b64fc.png"></p>
<p>Click the <code>Edit</code> button. <img src="img/e20b6a8ce135c1d4.png"></p>
<p>Go to <code>General preferences</code> section, type in 60 minutes for idle session timeout value.</p>
<p class="image-container"><img src="img/1b83cece69e1650f.png"></p>
<p>Further scroll down to <code>Linux shell profile</code> section, and type in <code>/bin/bash</code> before clicking <code>Save</code> button.</p>
<p class="image-container"><img src="img/b88863e27392a2de.png"></p>
<h3 is-upgraded>3. Connect to the Linux EC2 instance console</h3>
<p>Now go back to the <code>Session</code> tab and click the <code>Start session</code> button. <img src="img/6a05c2c6089304a0.png"></p>
<p>Now you should see the EC2 instance created by the Cloudformation template under <code>Target instances</code>. Its name should be <code><Cloudformation stack name>-jumphost</code>, select it and click <code>Start session</code>.</p>
<p class="image-container"><img src="img/a0f3cc55c1fecaef.png"></p>
<h3 is-upgraded>4. Create a key-pair to be used for authenticating with Snowflake</h3>
<p>Create a key pair in AWS Session Manager console by executing the following commands. You will be prompted to give an encryption password, remember this phrase, you will need it later.</p>
<pre><code language="language-commandline" class="language-commandline">cd $HOME
openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8
</code></pre>
<p>See below example screenshot:</p>
<p class="image-container"><img src="img/71780971351526bb.png"></p>
<p>Next we will create a public key by running following commands. You will be prompted to type in the phrase you used in above step.</p>
<pre><code>openssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub
</code></pre>
<p>see below example screenshot:</p>
<p class="image-container"><img src="img/63a114806cbeec31.png"></p>
<p>Next we will print out the public key string in a correct format that we can use for Snowflake.</p>
<pre><code>grep -v KEY rsa_key.pub | tr -d &#39;\n&#39; | awk &#39;{print $1}&#39; &gt; pub.Key
cat pub.Key
</code></pre>
<p>see below example screenshot:</p>
<p class="image-container"><img src="img/569ba68bc98a143d.png"></p>
<h3 is-upgraded>5. Install the Kafka connector for Snowpipe streaming</h3>
<p>Run the following command to install the Kafka connector and Snowpipe streaming SDK</p>
<pre><code language="language-commandline" class="language-commandline">passwd=changeit  # Use the default password for the Java keystore, you should change it after finishing the lab
directory=/home/ssm-user/snowpipe-streaming # Installation directory

cd $HOME
mkdir -p $directory
cd $directory
pwd=`pwd`
sudo yum clean all
sudo yum -y install openssl vim-common java-1.8.0-openjdk-devel.x86_64 gzip tar jq python3-pip
wget https://archive.apache.org/dist/kafka/2.8.1/kafka_2.12-2.8.1.tgz
tar xvfz kafka_2.12-2.8.1.tgz -C $pwd
wget https://github.com/aws/aws-msk-iam-auth/releases/download/v1.1.1/aws-msk-iam-auth-1.1.1-all.jar -O $pwd/kafka_2.12-2.8.1/libs/aws-msk-iam-auth-1.1.1-all.jar
rm -rf $pwd/kafka_2.12-2.8.1.tgz
cd /tmp &amp;&amp; cp /usr/lib/jvm/java-openjdk/jre/lib/security/cacerts kafka.client.truststore.jks
cd /tmp &amp;&amp; keytool -genkey -keystore kafka.client.keystore.jks -validity 300 -storepass $passwd -keypass $passwd -dname &#34;CN=snowflake.com&#34; -alias snowflake -storetype pkcs12

#Snowflake kafka connector, must be v3.0.0 or later for Iceberg support, here we use v3.1.0
wget https://repo1.maven.org/maven2/com/snowflake/snowflake-kafka-connector/3.1.0/snowflake-kafka-connector-3.1.0.jar -O $pwd/kafka_2.12-2.8.1/libs/snowflake-kafka-connector-3.1.0.jar

wget https://repo1.maven.org/maven2/org/bouncycastle/bc-fips/1.0.1/bc-fips-1.0.1.jar -O $pwd/kafka_2.12-2.8.1/libs/bc-fips-1.0.1.jar
wget https://repo1.maven.org/maven2/org/bouncycastle/bcpkix-fips/1.0.3/bcpkix-fips-1.0.3.jar -O $pwd/kafka_2.12-2.8.1/libs/bcpkix-fips-1.0.3.jar

</code></pre>
<p>Note that the version numbers for Kafka, the Snowflake Kafka connector, and the Snowpipe Streaming SDK are dynamic, as new versions are continually published. We are using the version numbers that have been validated to work.</p>
<h3 is-upgraded>6. Retrieve the broker string from the MSK cluster.</h3>
<p>Go to the <a href="https://us-west-2.console.aws.amazon.com/msk/#/clusters" target="_blank">MSK</a> console and click the newly created MSK cluster, it should have a substring <code>MSKCluster</code> in its name.</p>
<p class="image-container"><img src="img/c6977a440cba2b34.png"></p>
<p>Click <code>View client information</code><img src="img/3ac0d3b4124d20ae.png"></p>
<p>We are going to use TLS authentication between the client and brokers. Copy down the broker string under <code>Private endpoint</code> for TLS authentication type. <img src="img/55920ffd9ae4fab.png"></p>
<p>Now switch back to the Session Manager window and execute the following command by replacing <code><broker string></code> with the copied values.</p>
<pre><code language="language-commandline" class="language-commandline">export BS=&lt;broker string&gt;
</code></pre>
<p>Now run the following command to add <code>BS</code> as an environment variable so it is recognized across the Linux sessions.</p>
<pre><code>echo &#34;export BS=$BS&#34; &gt;&gt; ~/.bashrc
</code></pre>
<p>See the following example screen capture.</p>
<p class="image-container"><img src="img/617c98fa1bf38059.png"></p>
<h3 is-upgraded>7. Create a configuration file <code>connect-standalone.properties</code> for the Kafka connector</h3>
<p>Run the following commands to generate a configuration file <code>connect-standalone.properties</code> in directory <code>/home/ssm-user/snowpipe-streaming/scripts</code> for the client to authenticate with the Kafka cluster.</p>
<pre><code language="language-commandline" class="language-commandline">dir=/home/ssm-user/snowpipe-streaming/scripts
mkdir -p $dir &amp;&amp; cd $dir
cat &lt;&lt; EOF &gt; $dir/connect-standalone.properties
#************CREATING SNOWFLAKE Connector****************
bootstrap.servers=$BS

#************SNOWFLAKE VALUE CONVERSION****************
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=com.snowflake.kafka.connector.records.SnowflakeJsonConverter
key.converter.schemas.enable=true
value.converter.schemas.enable=true
#************SNOWFLAKE ****************

offset.storage.file.filename=/tmp/connect.offsets
# Flush much faster than normal, which is useful for testing/debugging
offset.flush.interval.ms=10000

#*********** FOR SSL  ****************
security.protocol=SSL
ssl.truststore.location=/tmp/kafka.client.truststore.jks
ssl.truststore.password=changeit
ssl.enabled.protocols=TLSv1.1,TLSv1.2

consumer.security.protocol=SSL
consumer.ssl.truststore.location=/tmp/kafka.client.truststore.jks
consumer.ssl.truststore.password=changeit
consumer.ssl.enabled.protocols=TLSv1.1,TLSv1.2
EOF

</code></pre>
<p>A configuration file <code>connect-standalone.properties</code> is created in directory <code>/home/ssm-user/snowpipe-streaming/scripts</code></p>
<h3 is-upgraded>8. Create a security client.properties configuration file for the producer</h3>
<p>Run the following commands to create a security configuration file <code>client.properties</code> for the MSK cluster</p>
<pre><code language="language-commandline" class="language-commandline">dir=/home/ssm-user/snowpipe-streaming/scripts
cat &lt;&lt; EOF &gt; $dir/client.properties
security.protocol=SSL
ssl.truststore.location=/tmp/kafka.client.truststore.jks
ssl.truststore.password=changeit
ssl.enabled.protocols=TLSv1.1,TLSv1.2
EOF

</code></pre>
<p>A configuration file <code>client.properties</code> is created in directory <code>/home/ssm-user/snowpipe-streaming/scripts</code></p>
<h3 is-upgraded>9. Create a streaming topic called &#34;streaming&#34; in the MSK cluster</h3>
<p>Now we can run the following commands to create a Kafka topic on the MSK cluster to stream our data.</p>
<pre><code language="language-commandline" class="language-commandline">$HOME/snowpipe-streaming/kafka_2.12-2.8.1/bin/kafka-topics.sh --bootstrap-server $BS --command-config $HOME/snowpipe-streaming/scripts/client.properties --create --topic streaming --partitions 2 --replication-factor 2
</code></pre>
<p>You should see the response <code>Created topic streaming</code> if it is successful.</p>
<p>To describe the topic, run the following commands:</p>
<pre><code language="language-commandline" class="language-commandline">$HOME/snowpipe-streaming/kafka_2.12-2.8.1/bin/kafka-topics.sh --bootstrap-server $BS --command-config $HOME/snowpipe-streaming/scripts/client.properties --describe --topic streaming
</code></pre>
<p>You should see there are two partitions with a replication factor of 2 in the <code>streaming</code> topic. See below example screenshot: <img src="img/f98216dd6f0c5ee9.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Prepare the Snowflake account for streaming" duration="10">
        <h3 is-upgraded>1. Creating user, role, and database</h3>
<p>First login to your Snowflake account as a power user with ACCOUNTADMIN role. Then run the following SQL commands in a worksheet to create a user, database and the role that we will use in the lab.</p>
<pre><code>-- Set default value for multiple variables
-- For purpose of this workshop, it is recommended to use these defaults during the exercise to avoid errors
-- You should change them after the workshop
SET PWD = &#39;Test1234567&#39;;
SET USER = &#39;STREAMING_USER&#39;;
SET DB = &#39;MSK_STREAMING_DB&#39;;
SET WH = &#39;MSK_STREAMING_WH&#39;;
SET ROLE = &#39;MSK_STREAMING_RL&#39;;

USE ROLE ACCOUNTADMIN;

-- CREATE USERS
CREATE USER IF NOT EXISTS IDENTIFIER($USER) PASSWORD=$PWD  COMMENT=&#39;STREAMING USER&#39;;

-- CREATE ROLES
CREATE OR REPLACE ROLE IDENTIFIER($ROLE);

-- CREATE DATABASE AND WAREHOUSE
CREATE DATABASE IF NOT EXISTS IDENTIFIER($DB);
USE IDENTIFIER($DB);
CREATE OR REPLACE WAREHOUSE IDENTIFIER($WH) WITH WAREHOUSE_SIZE = &#39;SMALL&#39;;

-- GRANTS
GRANT CREATE WAREHOUSE ON ACCOUNT TO ROLE IDENTIFIER($ROLE);
GRANT ROLE IDENTIFIER($ROLE) TO USER IDENTIFIER($USER);
GRANT OWNERSHIP ON DATABASE IDENTIFIER($DB) TO ROLE IDENTIFIER($ROLE);
GRANT USAGE ON WAREHOUSE IDENTIFIER($WH) TO ROLE IDENTIFIER($ROLE);

-- SET DEFAULTS
ALTER USER IDENTIFIER($USER) SET DEFAULT_ROLE=$ROLE;
ALTER USER IDENTIFIER($USER) SET DEFAULT_WAREHOUSE=$WH;


-- RUN FOLLOWING COMMANDS TO FIND YOUR ACCOUNT IDENTIFIER, COPY IT DOWN FOR USE LATER
-- IT WILL BE SOMETHING LIKE &lt;organization_name&gt;-&lt;account_name&gt;
-- e.g. ykmxgak-wyb52636

WITH HOSTLIST AS 
(SELECT * FROM TABLE(FLATTEN(INPUT =&gt; PARSE_JSON(SYSTEM$allowlist()))))
SELECT REPLACE(VALUE:host,&#39;.snowflakecomputing.com&#39;,&#39;&#39;) AS ACCOUNT_IDENTIFIER
FROM HOSTLIST
WHERE VALUE:type = &#39;SNOWFLAKE_DEPLOYMENT_REGIONLESS&#39;;

</code></pre>
<p>Please write down the Account Identifier, we will need it later. <img src="img/f5a3bbf6be8a8907.png"></p>
<p>Next we need to configure the public key for the streaming user to access Snowflake programmatically.</p>
<p>First, in the Snowflake worksheet, replace &lt; pubKey &gt; with the content of the file <code>/home/ssm-user/pub.Key</code> (see <code>step 4</code> by clicking on <code>section #2 Create a provisioned Kafka cluster and a Linux jumphost in AWS</code> in the left pane) in the following SQL command and execute.</p>
<pre><code language="language-commandline" class="language-commandline">use role accountadmin;
alter user streaming_user set rsa_public_key=&#39;&lt; pubKey &gt;&#39;;
</code></pre>
<p>See below example screenshot:</p>
<p class="image-container"><img src="img/c089a36fa3050821.png"></p>
<p>Now logout of Snowflake, sign back in as the default user <code>streaming_user</code> we just created with the associated password (default: Test1234567). Run the following SQL commands in a worksheet to create a schema (e.g. <code>MSK_STREAMING_SCHEMA</code>) in the default database (e.g. <code>MSK_STREAMING_DB</code>):</p>
<pre><code language="language-commandline" class="language-commandline">SET DB = &#39;MSK_STREAMING_DB&#39;;
SET SCHEMA = &#39;MSK_STREAMING_SCHEMA&#39;;

USE IDENTIFIER($DB);
CREATE OR REPLACE SCHEMA IDENTIFIER($SCHEMA);
</code></pre>
<h3 is-upgraded>2. Install SnowSQL (optional but highly recommended)</h3>
<p>This step is optional for this workshop but is highly recommended if you prefer to use the CLI to interact with Snowflake later instead of the web console.</p>
<p><a href="https://docs.snowflake.com/en/user-guide/snowsql.html" target="_blank">SnowSQL</a> is the command line client for connecting to Snowflake to execute SQL queries and perform all DDL and DML operations, including loading data into and unloading data out of database tables.</p>
<p>To install SnowSQL. Execute the following commands on the Linux Session Manager console:</p>
<pre><code language="language-commandline" class="language-commandline">curl https://sfc-repo.snowflakecomputing.com/snowsql/bootstrap/1.2/linux_x86_64/snowsql-1.2.24-linux_x86_64.bash -o /tmp/snowsql-1.2.24-linux_x86_64.bash
echo -e &#34;~/bin \n y&#34; &gt; /tmp/ans
bash /tmp/snowsql-1.2.24-linux_x86_64.bash &lt; /tmp/ans

</code></pre>
<p>See below example screenshot:</p>
<p class="image-container"><img src="img/52a8e9b66a7d335d.png"></p>
<p>Next set the environment variable for Snowflake Private Key Phrase:</p>
<pre><code language="language-commandline" class="language-commandline">export SNOWSQL_PRIVATE_KEY_PASSPHRASE=&lt;key phrase you set up when running openssl previously&gt;
</code></pre>
<p>Note that you should add the command above in the ~/.bashrc file to preserve this environment variable across sessions.</p>
<pre><code language="language-commandline" class="language-commandline">echo &#34;export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWSQL_PRIVATE_KEY_PASSPHRASE&#34; &gt;&gt; ~/.bashrc
</code></pre>
<p>Now you can execute this command to interact with Snowflake:</p>
<pre><code language="language-commandline" class="language-commandline">$HOME/bin/snowsql -a &lt;Snowflake Account Identifier&gt; -u streaming_user --private-key-path $HOME/rsa_key.p8 -d msk_streaming_db -s msk_streaming_schema
</code></pre>
<p>See below example screenshot:</p>
<p class="image-container"><img src="img/e5b4a4bf20541606.png"></p>
<p>Type <code>Ctrl-D</code> to get out of SnowSQL session.</p>
<p>You can edit the <a href="https://docs.snowflake.com/en/user-guide/snowsql-config.html#snowsql-config-file" target="_blank"><code>~/.snowsql/config</code></a> file to set default parameters and eliminate the need to specify them every time you run snowsql.</p>
<p>At this point, the Snowflake setup is complete.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Configure Kafka connector for Snowpipe Streaming" duration="10">
        <h3 is-upgraded>1. Run the following commands to collect various connection parameters for the Kafka connector</h3>
<pre><code language="language-commandline" class="language-commandline">cd $HOME
outf=/tmp/params
cat &lt;&lt; EOF &gt; /tmp/get_params
a=&#39;&#39;
until [ ! -z \$a ]
do
 read -p &#34;Input Snowflake account identifier: e.g. ylmxgak-wyb53646 ==&gt; &#34; a
done

echo export clstr_url=\$a.snowflakecomputing.com &gt; $outf
export clstr_url=\$a.snowflakecomputing.com

read -p &#34;Snowflake cluster user name: default: streaming_user ==&gt; &#34; user
if [[ \$user == &#34;&#34; ]]
then
   user=&#34;streaming_user&#34;
fi

echo export user=\$user &gt;&gt; $outf
export user=\$user

pass=&#39;&#39;
until [ ! -z \$pass ]
do
  read -p &#34;Private key passphrase ==&gt; &#34; pass
done

echo export key_pass=\$pass &gt;&gt; $outf
export key_pass=\$pass

read -p &#34;Full path to your Snowflake private key file, default: /home/ssm-user/rsa_key.p8 ==&gt; &#34; p8
if [[ \$p8 == &#34;&#34; ]]
then
   p8=&#34;/home/ssm-user/rsa_key.p8&#34;
fi

priv_key=\`cat \$p8 | grep -v PRIVATE | tr -d &#39;\n&#39;\`
echo export priv_key=\$priv_key  &gt;&gt; $outf
export priv_key=\$priv_key
cat $outf &gt;&gt; $HOME/.bashrc
EOF
. /tmp/get_params

</code></pre>
<p>See below example screen capture.</p>
<p class="image-container"><img src="img/3e67c9d82a3c03a7.png"></p>
<h3 is-upgraded>2. Run the following commands to create a Snowflake Kafka connect property configuration file:</h3>
<pre><code language="language-commandline" class="language-commandline">dir=/home/ssm-user/snowpipe-streaming/scripts
cat &lt;&lt; EOF &gt; $dir/snowflakeconnectorMSK.properties
name=snowpipeStreaming
connector.class=com.snowflake.kafka.connector.SnowflakeSinkConnector
tasks.max=4
topics=streaming
snowflake.private.key.passphrase=$key_pass
snowflake.database.name=MSK_STREAMING_DB
snowflake.schema.name=MSK_STREAMING_SCHEMA
snowflake.topic2table.map=streaming:MSK_STREAMING_TBL
buffer.count.records=10000
buffer.flush.time=5
buffer.size.bytes=20000000
snowflake.url.name=$clstr_url
snowflake.user.name=$user
snowflake.private.key=$priv_key
snowflake.role.name=MSK_STREAMING_RL
snowflake.ingestion.method=snowpipe_streaming
value.converter.schemas.enable=false
jmx=true
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
errors.tolerance=all
EOF
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Putting it all together" duration="10">
        <p>Finally, we are ready to start ingesting data into the Snowflake table.</p>
<h3 is-upgraded>1. Start the Kafka Connector for Snowpipe streaming</h3>
<p>Go back to the Linux console and execute the following commands to start the Kafka connector.</p>
<pre><code language="language-commandline" class="language-commandline">$HOME/snowpipe-streaming/kafka_2.12-2.8.1/bin/connect-standalone.sh $HOME/snowpipe-streaming/scripts/connect-standalone.properties $HOME/snowpipe-streaming/scripts/snowflakeconnectorMSK.properties
</code></pre>
<p>If everything goes well, you should see something similar to screen capture below: <img src="img/ea540861ddaa4c82.png"></p>
<h3 is-upgraded>2. Start the producer that will ingest real-time data to the MSK cluster</h3>
<p>Start a new Linux session in <code>step 3</code> by clicking on <code>section #2 Create a provisioned Kafka cluster and a Linux jumphost in AWS</code> in the left pane.</p>
<pre><code language="language-commandline" class="language-commandline">curl --connect-timeout 5 http://ecs-alb-1504531980.us-west-2.elb.amazonaws.com:8502/opensky | $HOME/snowpipe-streaming/kafka_2.12-2.8.1/bin/kafka-console-producer.sh --broker-list $BS --producer.config $HOME/snowpipe-streaming/scripts/client.properties --topic streaming
</code></pre>
<p>You should see response similar to screen capture below if everything works well.</p>
<p class="image-container"><img src="img/e8cf698fe65b78bb.png"></p>
<p>Note that in the script above, the producer queries a <a href="http://ecs-alb-1504531980.us-west-2.elb.amazonaws.com:8502/opensky" target="_blank">Rest API</a> that provides real-time flight data over the San Francisco Bay Area in JSON format. The data includes information such as timestamps, <a href="https://icao.usmission.gov/mission/icao/#:~:text=Home%20%7C%20About%20the%20Mission%20%7C%20U.S.,civil%20aviation%20around%20the%20world." target="_blank">icao</a> numbers, flight IDs, destination airport, longitude, latitude, and altitude of the aircraft, etc. The data is ingested into the <code>streaming</code> topic on the MSK cluster and then picked up by the Snowpipe streaming Kafka connector, which delivers it directly into a Snowflake table <code>msk_streaming_db.msk_streaming_schema.msk_streaming_tbl</code>.</p>
<p class="image-container"><img src="img/f3df9aec8be69f20.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Query ingested data in Snowflake" duration="10">
        <p>Now, switch back to the Snowflake console and make sure that you signed in as the default user <code>streaming_user</code>. The data should have been streamed into a table, ready for further processing.</p>
<h3 is-upgraded>1. Query the raw data</h3>
<p>To verify that data has been streamed into Snowflake, execute the following SQL commands.</p>
<pre><code language="language-sh" class="language-sh">use msk_streaming_db;
use schema msk_streaming_schema;
show channels in table msk_streaming_tbl;
</code></pre>
<p>You should see that there are two channels, corresponding to the two partitions created earlier in the topic. <img src="img/c1aaa40fc8cf78a1.png"></p>
<p>Note that, unlike the screen capture above, at this point, you should only see one row in the table, as we have only ingested data once. We will see new rows being added later as we continue to ingest more data.</p>
<p>Now run the following query on the table.</p>
<pre><code>select * from msk_streaming_tbl;
</code></pre>
<p>You should see there are two columns in the table: <code>RECORD_METADATA</code> and <code>RECORD_CONTENT</code> as shown in the screen capture below.</p>
<p><img src="img/ba1b0b327e197196.png"> The <code>RECORD_CONTENT</code> column is an JSON array that needs to be flattened.</p>
<h3 is-upgraded>2. Flatten the raw JSON data</h3>
<p>Now execute the following SQL commands to flatten the raw JSONs and create a materialized view with multiple columns based on the key names.</p>
<pre><code language="language-sh" class="language-sh">create or replace view flights_vw
  as select
    f.value:utc::timestamp_ntz ts_utc,
    CONVERT_TIMEZONE(&#39;UTC&#39;,&#39;America/Los_Angeles&#39;,ts_utc::timestamp_ntz) as ts_pt,
    f.value:alt::integer alt,
    f.value:dest::string dest,
    f.value:orig::string orig,
    f.value:id::string id,
    f.value:icao::string icao,
    f.value:lat::float lat,
    f.value:lon::float lon,
    st_geohash(to_geography(ST_MAKEPOINT(lon, lat)),12) geohash,
    year(ts_pt) yr,
    month(ts_pt) mo,
    day(ts_pt) dd,
    hour(ts_pt) hr
FROM   msk_streaming_tbl,
       Table(Flatten(msk_streaming_tbl.record_content)) f;
</code></pre>
<p>The SQL commands create a view, convert timestamps to different time zones, and use Snowflake&#39;s <a href="https://docs.snowflake.com/en/sql-reference/functions/st_geohash.html" target="_blank">Geohash function</a>  to generate geohashes that can be used in time-series visualization tools like Grafana</p>
<p>Let&#39;s query the view <code>flights_vw</code> now.</p>
<pre><code language="language-sh" class="language-sh">select * from flights_vw;
</code></pre>
<p>As a result, you will see a nicely structured output with columns derived from the JSONs <img src="img/cca628ea87b6caae.png"></p>
<h3 is-upgraded>3. Stream real-time flight data continuously to Snowflake</h3>
<p>We can now write a loop to stream the flight data continuously into Snowflake.</p>
<p>Go back to the Linux session and run the following script.</p>
<pre><code language="language-sh" class="language-sh">while true
do
  curl --connect-timeout 5 -k http://ecs-alb-1504531980.us-west-2.elb.amazonaws.com:8502/opensky | $HOME/snowpipe-streaming/kafka_2.12-2.8.1/bin/kafka-console-producer.sh --broker-list $BS --producer.config $HOME/snowpipe-streaming/scripts/client.properties --topic streaming
  sleep 10
done

</code></pre>
<p>You can now go back to the Snowflake worksheet to run a <code>select count(1) from flights_vw</code> query every 10 seconds to verify that the row counts is indeed increasing.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Configuring Kafka Connector with Apache Iceberg tables - New" duration="20">
        <p>Iceberg table ingestion requires version 3.0.0 or later of the Kafka connector and is only supported with Snowpipe Streaming. To configure the Kafka connector for Iceberg table ingestion, follow the regular setup steps for a Snowpipe Streaming-based connector mentioned in the Section 4 in the left pane of this quickstart with a few differences noted in the following sections.</p>
<h3 is-upgraded>1. Create an External Volume in Snowflake</h3>
<p>Before you create an Iceberg table, you must have an external volume. An external volume is a Snowflake object that stores information about your cloud storage locations and identity and access management (IAM) entities (for example, IAM roles). Snowflake uses an external volume to establish a connection with your cloud storage in order to access Iceberg metadata and Parquet data.</p>
<p>To create an external volume in S3, follow the steps mentioned in this <a href="https://docs.snowflake.com/en/user-guide/tables-iceberg-configure-external-volume-s3?_fsi=jKSUJNzH&_fsi=jKSUJNzH" target="_blank">document</a>.</p>
<p>Below is an example to create External Volume below: When creating the external volume, be sure to use the ACCOUNTADMIN role.</p>
<pre><code language="language-sql" class="language-sql">use role accountadmin;
use schema msk_streaming_schema;
CREATE OR REPLACE EXTERNAL VOLUME iceberg_lab_vol
   STORAGE_LOCATIONS =
      (
         (
            NAME = &#39;my-s3-us-west-2&#39;
            STORAGE_PROVIDER = &#39;S3&#39;
            STORAGE_BASE_URL = &#39;s3://&lt;my s3 bucket name&gt;/&#39;
            STORAGE_AWS_ROLE_ARN = &#39;&lt;arn:aws:iam::123456789012:role//myrole&gt;&#39;
            STORAGE_AWS_EXTERNAL_ID = &#39;iceberg_table_external_id&#39;
         )
      );
</code></pre>
<p>After the external volume is created, use the ACCOUNTADMIN role to grant usage to the role.</p>
<pre><code language="language-sql" class="language-sql">GRANT ALL ON EXTERNAL VOLUME iceberg_lab_vol TO ROLE IDENTIFIER($ROLE) WITH GRANT OPTION;
 
</code></pre>
<h3 is-upgraded>2.Pre-create a Snowflake-managed Iceberg Table for Streaming</h3>
<p>Note that if you are using a regular table to store the streaming data, Snowpipe streaming will automatically create it for you if it doesn&#39;t exist already. But for Iceberg table, you will need to pre-create an Iceberg table referencing the external volume you just created. You can specify BASE_LOCATION to instruct Snowflake where to write table data and metadata, or leave empty to write data and metadata to the location specified in the external volume definition.</p>
<p>Sharing an example of the SQL to create External volumne below:</p>
<pre><code language="language-sql" class="language-sql">CREATE OR REPLACE ICEBERG TABLE MSK_STREAMING_ICEBERG_TBL (
    record_metadata OBJECT()
  )
  EXTERNAL_VOLUME = &#39;iceberg_lab_vol&#39;
  CATALOG = &#39;SNOWFLAKE&#39;
  BASE_LOCATION = &#39;MSK_STREAMING_ICEBERG_TBL&#39;;

</code></pre>
<p>Since the BASE_LOCATION is specified, it will create a folder based on the value in the STORAGE_BASE_URL of the External Volume.</p>
<p class="image-container"><img src="img/f3b8a9aeb465e431.png"></p>
<p>Since the Scehmatization is enabled in the Kafka Connector, followig privileges are set properly on the table.</p>
<pre><code language="language-sql" class="language-sql">GRANT EVOLVE SCHEMA ON TABLE msk_streaming_db.msk_streaming_schema.MSK_STREAMING_ICEBERG_TBL TO ROLE MSK_STREAMING_RL; 
 
ALTER ICEBERG TABLE msk_streaming_db.msk_streaming_schema.MSK_STREAMING_ICEBERG_TBL set enable_schema_evolution=true;

</code></pre>
<h3 is-upgraded>3. Modify the Snowpipe streaming properties file</h3>
<p>Modify the two lines in <code>$HOME/snowpipe-streaming/scripts/snowflakeconnectorMSK.properties</code></p>
<pre><code language="language-commandline" class="language-commandline">topics=streaming-iceberg
snowflake.topic2table.map=streaming-iceberg:MSK_STREAMING_ICEBERG_TBL

# Add the property which specifies connector to ingest data into an Iceberg table.
snowflake.streaming.iceberg.enabled = true

#Also verify schematiaztion is enabled
snowflake.enable.schematization=true
</code></pre>
<p>Save the properties file</p>
<p>Note, Iceberg table ingestion is not supported when snowflake.streaming.enable.single.buffer is set to false.</p>
<h3 is-upgraded>4. Create a new topic in MSK</h3>
<p>We now need to create a new topic <code>streaming-iceberg</code> in MSK cluster by running the following command:</p>
<pre><code language="language-commandline" class="language-commandline">$HOME/snowpipe-streaming/kafka_2.12-2.8.1/bin/kafka-topics.sh --bootstrap-server $BS --command-config $HOME/snowpipe-streaming/scripts/client.properties --create --topic streaming-iceberg --partitions 2 --replication-factor 2

</code></pre>
<h3 is-upgraded>5. Restart the consumer</h3>
<p>Restart the consumer by issuing the following shell command in a new Session Manager console.</p>
<pre><code language="language-commandline" class="language-commandline">kill -9 `ps -ef | grep java | grep -v grep | awk &#39;{print $2}&#39;`
$HOME/snowpipe-streaming/kafka_2.12-2.8.1/bin/connect-standalone.sh $HOME/snowpipe-streaming/scripts/connect-standalone.properties $HOME/snowpipe-streaming/scripts/snowflakeconnectorMSK.properties
</code></pre>
<h3 is-upgraded>6. Ingest data</h3>
<p>Now ingest some data into the newly created topic by running the following command in a new Session Manager console.</p>
<pre><code language="language-commandline" class="language-commandline">curl --connect-timeout 5 http://ecs-alb-1504531980.us-west-2.elb.amazonaws.com:8502/opensky | \
jq -c &#39;.[]&#39; | \
while read i ; \
do \
echo $i | \
$HOME/snowpipe-streaming/kafka_2.12-2.8.1/bin/kafka-console-producer.sh --broker-list $BS --producer.config $HOME/snowpipe-streaming/scripts/client.properties --topic streaming-iceberg ; \
echo $i ; \
done
</code></pre>
<h3 is-upgraded>7. Query ingested data in Snowflake</h3>
<p>Now, switch back to the Snowflake console and make sure that you signed in as the default user <code>streaming_user</code>. The data should have been streamed into a table, ready for further processing.</p>
<p>To verify that data has been streamed into Snowflake, execute the following SQL commands.</p>
<pre><code>select * from MSK_STREAMING_DB.MSK_STREAMING_SCHEMA.MSK_STREAMING_ICEBERG_TBL;
</code></pre>
<p>You should see that the table contains the keys in json records as column names with values and RECORD_METADATA columns are populated.</p>
<p class="image-container"><img src="img/e802d00385b114c5.png"></p>


      </google-codelab-step>
    
      <google-codelab-step label="Use MSK Connect (MSKC) - Optional" duration="15">
        <p>So far we have been hosting the Kafka connector for Snowpipe Streaming on the EC2 instance. You can also use <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank">Amazon MSK Connect</a> to manage the connector.</p>
<p>*Note that in order to use MSKC, it is suggested to run your MSK cluster in a private subnet with egress to the internet via the <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html" target="_blank">NAT gateway</a> for it to work. See this <a href="https://docs.aws.amazon.com/msk/latest/developerguide/msk-connect-internet-access.html" target="_blank">AWS documentation</a> for more information.</p>
<h3 is-upgraded>1. Create a S3 bucket to store the custom plugins</h3>
<p>Follow this <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.html" target="_blank">AWS documentation</a> to create a S3 bucket.</p>
<h3 is-upgraded>2. Upload the libraries to the S3 bucket</h3>
<p>On your EC2 session, run the following commands to compress the libraries into a zipped file.</p>
<pre><code language="language-commandline" class="language-commandline">cd $HOME/snowpipe-streaming/kafka_2.12-2.8.1/libs
zip -9 /tmp/snowpipeStreaming-mskc-plugins.zip *
aws s3 cp /tmp/snowpipeStreaming-mskc-plugins.zip s3://&lt;your s3 bucket name&gt;/snowpipeStreaming-mskc-plugins.zip
</code></pre>
<h3 is-upgraded>3. Create a custom plugin in MSK</h3>
<p>Go to the <a href="https://console.aws.amazon.com/msk/home" target="_blank">MSK console</a>, click <code>Custom plugins</code> on the left pane. Click <code>Create Custom plugin</code>. <img src="img/cfd90394fc7ef6fa.png"></p>
<p>Fill in the s3 path to your uploaded zip file, e.g. <code>s3://my-mskc-bucket/snowpipeStreaming-mskc-plugins.zip</code> Give the custom plugin a name, e.g. <code>my-mskc-plugin</code>, click <code>Create custom plugin</code>.</p>
<p class="image-container"><img src="img/8d0487a4869a2b13.png"></p>
<h3 is-upgraded>4. Create a connector</h3>
<p>Click <code>Connectors</code> on the left pane, then click <code>Create connector</code>. <img src="img/391ff0fb6d7741ad.png"></p>
<p>Check the <code>Use existing custom plugin</code> button. Select the custom plugin you just created, click <code>Next</code>. <img src="img/bdcda041c7f04e73.png"></p>
<p>Give the connector a name, e.g. <code>snowpipeStreaming</code> in the <code>Basic information</code> section. <img src="img/f982ab5e60ad0fd6.png"></p>
<p>Select the MSK cluster you want to associate this connector with. Scroll down to <code>Configuration settings</code>, copy and paste the content from the configuration file: <code>$HOME/snowpipe-streaming/scripts/snowflakeconnectorMSK.properties</code> in the EC2 instance. <img src="img/5bc7f28e1bc1a856.png"></p>
<p>Leave all other settings as default, further scroll down to <code>Access permissions</code>. In the <code>Choose service role</code> drop-down menu, select the role created by the Cloudformation template in the beginning of this quickstarts. The role name should look something like this <code><CFT stack name>-MSKConnectRole-<random characters></code>. Click <code>Next</code>.</p>
<p class="image-container"><img src="img/d432731df8798ac4.png"></p>
<p>In the <code>Security</code> page, leave everything as default, click <code>Next</code>. Skip the <code>Logs</code> page as it is optional, click <code>Next</code>. Review the configurations and click <code>Create connector</code>. The connector will be created in about 5-10 minutes. <img src="img/a30497bd74f6fbfc.png"></p>
<p>At this point, the Kafka connector for Snowpipestreaming has been configured, it is running and managed by MSKC, all you need to do is to run the source connector to ingest live data continuously as shown in Step 3 of Section 6.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Schema detection - Optional" duration="10">
        <p>Previously we ingested raw jsons into the table <code>MSK_STREAMING_TBL</code> and did a DDL to create a nicely formulated view with the column names mapped to the keys in the jsons. You can now skip the DDL step with <a href="https://docs.snowflake.com/en/user-guide/data-load-snowpipe-streaming-kafka-schema-detection" target="_blank">schema detection</a> enabled to detect the schema of the streaming data and load data into tables that automatically match any user-defined schema.</p>
<h3 is-upgraded>1. Modify the Snowpipe streaming properties file</h3>
<p>Modify the two lines in <code>$HOME/snowpipe-streaming/scripts/snowflakeconnectorMSK.properties</code></p>
<p>from</p>
<pre><code language="language-commandline" class="language-commandline">topics=streaming
snowflake.topic2table.map=streaming:MSK_STREAMING_TBL
</code></pre>
<p>to</p>
<pre><code language="language-commandline" class="language-commandline">topics=streaming,streaming-schematization
snowflake.topic2table.map=streaming:MSK_STREAMING_TBL,streaming-schematization:MSK_STREAMING_SCHEMATIZATION_TBL

#Also enable schematiaztion by adding
snowflake.enable.schematization=true
</code></pre>
<p>Save the properties file</p>
<h3 is-upgraded>2. Create a new topic in MSK</h3>
<p>We now need to create a new topic <code>streaming-schematization</code> in MSK cluster by running the following command:</p>
<pre><code>$HOME/snowpipe-streaming/kafka_2.12-2.8.1/bin/kafka-topics.sh --bootstrap-server $BS --command-config $HOME/snowpipe-streaming/scripts/client.properties --create --topic streaming-schematization --partitions 2 --replication-factor 2
</code></pre>
<h3 is-upgraded>3. Restart the consumer</h3>
<p>Restart the consumer by issuing the following shell command in a new Session Manager console.</p>
<pre><code language="language-commandline" class="language-commandline">kill -9 `ps -ef | grep java | grep -v grep | awk &#39;{print $2}&#39;`
$HOME/snowpipe-streaming/kafka_2.12-2.8.1/bin/connect-standalone.sh $HOME/snowpipe-streaming/scripts/connect-standalone.properties $HOME/snowpipe-streaming/scripts/snowflakeconnectorMSK.properties
</code></pre>
<h3 is-upgraded>4. Ingest data</h3>
<p>Now ingest some data into the newly created topic by running the following command in a new Session Manager console.</p>
<pre><code language="language-commandline" class="language-commandline">curl --connect-timeout 5 http://ecs-alb-1504531980.us-west-2.elb.amazonaws.com:8502/opensky | \
jq -c &#39;.[]&#39; | \
while read i ; \
do \
echo $i | \
$HOME/snowpipe-streaming/kafka_2.12-2.8.1/bin/kafka-console-producer.sh --broker-list $BS --producer.config $HOME/snowpipe-streaming/scripts/client.properties --topic streaming-schematization ; \
done

</code></pre>
<p>You should see the producer using <a href="https://jqlang.github.io/jq/" target="_blank">jq</a> to break up the json array and stream in the records one by one.</p>
<h3 is-upgraded>5. Verify schema detection is working</h3>
<p>Now head over to the Snowflake UI, and issue the following SQL command:</p>
<pre><code language="language-commandline" class="language-commandline">select * from msk_streaming_schematization_tbl;
</code></pre>
<p>You should see the table already contains the keys in json records as column names with values populated. There is no need to do DDL as before.</p>
<p class="image-container"><img src="img/99de3e08030dc79.png"></p>
<p>Note that using the shell script <code>connect-standalone.sh</code> that comes with the Kafka distribution is not the most efficient way of ingesting data into Snowflake as it opens and closes the topic every single time it calls the Snowpipe streaming SDK as you can see in the script in Step 4 above. We are doing this for the purpose of a quick demonstration.</p>
<p>Other programing languages like Python or Java are highly recommended as they keep the topic open throughout the ingesting process.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Cleanup" duration="0">
        <p>When you are done with the demo, to tear down the AWS resources, simply go to the <a href="https://console.aws.amazon.com/cloudformation/home?stacks" target="_blank">Cloudformation</a> console. Select the Cloudformation template you used to deploy the MSK cluster at the start of the demo, then click the <code>Delete</code> tab. All the resources that were deployed previously, such as EC2 instances, MSK clusters, roles, etc., will be cleaned up.</p>
<p>See example screen capture below.</p>
<p class="image-container"><img src="img/2291f2d8c284fa7a.png"></p>
<p>After the deletion of the MSK cluster, you will also need to delete the Cloudformation template for VPC if you created your own at the very beginning of the lab.</p>
<p>For Snowflake cleanup, execute the following SQL commands.</p>
<pre><code language="language-commandline" class="language-commandline">USE ROLE ACCOUNTADMIN;

DROP DATABASE MSK_STREAMING_DB;
DROP WAREHOUSE MSK_STREAMING_WH;
DROP ROLE MSK_STREAMING_RL;

-- Drop the streaming user
DROP USER IF EXISTS STREAMING_USER;
</code></pre>


      </google-codelab-step>
    
      <google-codelab-step label="Conclusion and Resources" duration="5">
        <p>In this lab, we built a demo to show how to ingest time-series data using Snowpipe streaming and Kafka with low latency. We demonstrated this using a self-managed Kafka connector on an EC2 instance. However, for a production environment, we recommend using <a href="https://aws.amazon.com/msk/features/msk-connect/" target="_blank">Amazon MSK Connect</a>, which offers scalability and resilience through the AWS infrastructure. Alternatively, if you have infrastructure supported by either <a href="https://aws.amazon.com/eks/" target="_blank">Amazon EKS</a> or <a href="https://aws.amazon.com/ecs/" target="_blank">Amazon ECS</a>, you can use them to host your containerized Kafka connectors as well.</p>
<p>For those of you who are interested in learning more about how to build sleek dashboards for monitoring the live flight data, please navigate to this <a href="https://quickstarts.snowflake.com/guide/getting_started_with_amg_and_streamlit_on_real-time_dashboarding/" target="_blank">quickstart</a> to continue.</p>
<p>Related Resources</p>
<ul>
<li><a href="https://medium.com/snowflake/snowpipe-streaming-demystified-e1ee385c6d9c" target="_blank">Snowpipe Streaming Demystified</a></li>
<li><a href="https://quickstarts.snowflake.com/guide/getting_started_with_amg_and_streamlit_on_real-time_dashboarding/" target="_blank">Getting Started with Amazon Managed Service for Grafana and Streamlit On Real-time Dashboarding</a></li>
<li><a href="https://quickstarts.snowflake.com/" target="_blank">Getting started with Snowflake</a></li>
<li><a href="https://aws.amazon.com/marketplace/seller-profile?id=18d60ae8-2c99-4881-a31a-e74770d70347" target="_blank">Snowflake on AWS Marketplace</a></li>
<li><a href="https://www.snowflake.com/Workloads/data-sharing/" target="_blank">Snowflake for Data Sharing</a></li>
<li><a href="https://www.snowflake.com/en/data-cloud/marketplace/" target="_blank">Snowflake Marketplace</a></li>
<li><a href="https://aws.amazon.com/msk/" target="_blank">Amazon Managed Streaming for Apache Kafka (MSK)</a></li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/native-shim.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/custom-elements.min.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/prettify.js"></script>
  <script src="https://quickstarts.snowflake.com/elements/codelab-elements/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>
  <script type="text/javascript">
    window.heap=window.heap||[],heap.load=function(e,t){window.heap.appid=e,window.heap.config=t=t||{};var r=t.forceSSL||"https:"===document.location.protocol,a=document.createElement("script");a.type="text/javascript",a.async=!0,a.src=(r?"https:":"http:")+"//cdn.heapanalytics.com/js/heap-"+e+".js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(a,n);for(var o=function(e){return function(){heap.push([e].concat(Array.prototype.slice.call(arguments,0)))}},p=["addEventProperties","addUserProperties","clearEventProperties","identify","removeEventProperty","setEventProperties","track","unsetEventProperty"],c=0;c<p.length;c++)heap[p[c]]=o(p[c])};
    heap.load("2025084205");
  </script>
</body>
</html>
